\Week{10}{Fooling Simple Computations : Small Biased Distributions and Sets}

We have seen limited independence in the previous lecture as a method for randomness efficient error reduction and sometimes complete derandomization of algorithms. Both of these can be thought of us means of studying limited independent bits $y \in \{0,1\}^m$ which can be generated by small number of bits as opposed to $m$ independent random bits. 

One of the views that we developed is that this can also be equivalently be viewed as, the limited independence is able to "fool" the amplification process. In this week, we will first explore this in a fundamental way by asking the following question - what kind of computation can limited independence fool? To formulate this further, let us recall the randomized algorithm set up once again. \\

\begin{minipage}{0.5\linewidth}
Recall that we have a radomized algorithm $\calA$ which takes in the input as $x \in \zon$ and random string is represented by $y \in \zo^m$.
Our derandomization problem was equivalently stated as, given $\calA(x)$ - with $x$ fixed, as an algoithm which takes $y$ as the input, with the guarantee that for most $y$'s the algorithm outputs the correct answer ($0/1$), can we algorithmically (efficiently) find out the correct answer given by the algorithm $\calA$. For the purpose of understanding the idea of fooling restricted $\calA$, we can imagine $\calA$ as computing a Boolean function which transforms $y \in \{0,1\}^m$ bits to $\{0,1\}$. But notation, we will think of it as $f : \zo^m \to \zo$.
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_g)[below=of q_r,yshift=-2cm]{${\cal G}(y')$};
\node[state,rectangle,minimum width=2.5cm,minimum height=5.25cm](q_b)[below=of q_r,yshift=-0.5cm]{};
\node[](q_l)[above=of q_r,yshift=0.85cm]{${\cal B}(x,y')$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3.5cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3.5cm,yshift=0mm]{};
\node[coordinate](q_3)[below=of q_g,xshift=0cm,yshift=-2cm]{};
\draw[->] ([yshift=1mm]q_r.east) -- ([yshift=1mm]q_0)node[midway] {\sc Yes/No};
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y$};
\draw[->] ([yshift=-2mm]q_3) -- ([yshift=0mm]q_g.south)node[midway,swap] {$y' \in \{0,1\}^{m'}$};

\end{tikzpicture}
\end{minipage}

\vspace{2mm}

With the above view, we can define the follwing - a distribution $D$ of $m$ bits, $y \in \{0,1\}^m$, is said to be $(s,\epsilon)$-pseudo-random if for every function $f$ (which abstracts out the algorithm $\calA$) which can be computed resource bounds\footnote{We keep this term to be resource bounds when considering computation of Boolean functions. A natural model of computation here is that of circuits, which we describe later when it is needed.} of $s$, we have that 

$$\left| \Pr_{Y \leftarrow U} [f(Y) = 1] - \Pr_{Y \leftarrow D} [f(Y) = 1] \right| \le \epsilon$$

\noindent We start with the task of fooling very simple Boolean functions $f$. Indeed, the task is not interesting if $f$ is not even depending on all input bits. Say if it depends only on the first bit $y_1$ of $y$, then we can fool the computation by just keeping $y_1$ as a pure random bit and the rest of the bits dependent on it. Indeed, arguably the first Boolean function that we can think of would be the conjuction and disjunction and another one would be parity function, majority function etc. We take them up in the next two sections.

\section{Fooling the Conjuction ($\land_n$) using $k$-wise Independece}

Now we show that the good old $k$-wise independent distributions that we have constructed in the earlier lectures actually fools the conjuction with an exponentially decaying $\epsilon$ with respect to $k$. Formally,

\begin{theorem}
\label{thm:k-wise-indep-fools-and}
Let $D$ be a $k$-wise independent distribution on $\zo^m$. Then, there is a constant $c$ such that :
$$\left| \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \right| \le \frac{1}{2^{ck}}$$
\end{theorem}
\begin{proof}
The main technique in the proof is the inclusion exclusion principle and some clever approximations. The original published proof that we give here is due to 

To start with, we will translate the statement about intersection to unions by applying De-Morgan's laws.

The following discussion will hold true for any distribution $D$ on $\zo^m$.
\begin{eqnarray*}
\Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] & = & 1 - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 0 \right] \\
& = & 1 - \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \textrm{ where $E_i$ is the event $Y_i = 0$. }
\end{eqnarray*}
Using this, we can rewrite what we want to estimate:
$$\left| \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \right| = 
\left| \Pr_{Y \leftarrow U} \left[ \bigcup_{i=1}^m E_i \right] - \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \right|$$

Thus, our task reduces to estimating $\Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right]$. This s where naturally the principle of exclusion can be applied. Recalling the principle : the starting point is the union bound - $\Pr \left[ \bigcup_{i=1}^m E_i \right] \le \Pr[E_1]+\Pr[E_2] + \ldots + \Pr[E_m]$ which forms an upper bound. In an attempt to achieve equality, we subtract the double-counted terms, and this leads to an over-subtraction, which we add again and so on. More formally, we write it in terms of the index sets:

\begin{eqnarray*}
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \le & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] \\
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \ge & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] - \sum_{\substack{S \subseteq [m]\\ |S| = 2}} \Pr \left[ \bigcap_{i \in S} E_i \right] \\
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \ge & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] - \sum_{\substack{S \subseteq [m]\\ |S| = 2}} \Pr \left[ \bigcap_{i \in S} E_i \right] + \sum_{\substack{S \subseteq [m]\\ |S| = 3}} \Pr \left[ \bigcap_{i \in S} E_i \right] \\
\end{eqnarray*}
And in general -  if we define : $T_i = \sum_{\substack{S \subseteq [m]\\ |S| = i}} \Pr \left[ \bigcap_{j \in S} E_j \right]$ and the partial sums $S_\ell = \sum_{i=1}^\ell (-1)^{i+1} T_i$. Using this, we conclude:

$$\forall 1 \le \ell \le m, ~~~\Pr \left[ \bigcup_{i=1}^m E_i \right] ~~~\textrm{ is }~~ \begin{cases}
\ge S_\ell \textrm{ when $\ell$ is odd.}\\
\le S_\ell \textrm{ when $\ell$ is even.}\\
\end{cases}
$$

Whatever we discussed so far is true for any distribution $D$. Let us create notation for the two quantities that we want to estimate:
$$ \Gamma = \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \hspace{2cm} \Delta = \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] $$
and we want to estimate an upper bound for $|\Gamma - \Delta|$.

Now we apply the fact that the distribution $D$ is $k$-wise independent. As per the definition, for any subset $S \subset [m]$, 
$$\Pr_{D \leftarrow Y} \left[\bigcap_{i \in S} E_i = 0 \right] = \prod_{i\in S} \Pr_{D \leftarrow Y} \left[E_i\right] $$
and this is true for both the $k$-wise independent distribution $D$ and the uniform distribution $U$ (which is $n$-wise independent. Interpreting in terms of the above notation, this implies that $\forall \ell \le k$, the value of $S_i$ remains the same whether we work with the distribution $D$ or $U$.

This in particular, implies that : 
$$\forall \ell : 1 \le \ell \le k-1, \textrm{ where $\ell$ is even, } S_{\ell} \le \Gamma \le S_{\ell+1} 
\textrm{ and } S_{\ell} \le \Delta \le S_{\ell+1} 
$$
This gives that 
$$\left| \Gamma - \Delta \right| \le  |S_{\ell+1} - S_{\ell}| \le T_\ell$$
Noticing that the gap between $S_\ell$ and $S_{\ell+1}$ keeps on decreasing with increasing $\ell$, we have that $\left| \Gamma - \Delta \right| \le T_k$. Now we need to estimate an upper bound for $T_k$.
\end{proof}

\paragraph{Estimating an upper bound for $T_k$ :}
Now we estimate the value of $T_\ell$ which involves a useful generalization of the arithemetic mean - geometric mean inequality.
\begin{lemma}
Let $q_1, q_2, \ldots q_m$ be non-negative real numbers. If we choose $S \subset [m]$:
$$\E_{\substack{S \subseteq [m] \\ |S|=1}} \left[ \prod_{j \in S} q_j \right] \ge \E_{\substack{S \subseteq [m] \\ |S|=2}} \left[ \left(\prod_{j \in S} q_j \right)^{1/2} \right] \ge \ldots \le \E_{\substack{S \subseteq [m] \\ |S|=k}} \left[ \left( \prod_{j \in S} q_j \right)^{1/k} \right] \ge \ldots \ge  \E_{\substack{S \subseteq [m] \\ |S|=m}} \left[ \left( \prod_{j \in S} q_j \right)^{1/m}\right]$$
\end{lemma}
\noindent The first term on the LHS is nothing but $\frac{1}{m} \sum_{i=1}^m q_i$ and the last term in the rightmost end is $\left( \prod_{j \in S} q_j \right)^{1/m}$ and hence the above is a generalization of AM-GM inequality (Exercise : Prove it precisely).

To apply this the lemma to bound $T_k$, we set up:
(denote the $P_i = \Pr[E_i]$)
$$T_k = \sum_{\substack{S \subseteq [m]\\ |S| = k}} \Pr_{Y \leftarrow D} \left[ \bigcap_{j \in S} E_j \right] = \sum_{\substack{S \subseteq [m]\\ |S| = k}} \left[ \prod_{i \in S} P_i \right] \le {n \choose k} \E_{\substack{S \subseteq [m]\\ |S| = k}} \left[ \prod_{i \in S} P_i \right] $$
Now we can apply the Lemma to the RHS expression, and get the following :

$$T_k \le {n \choose k} \bigsum_{i=1}^m \left(\frac{1}{m}\bigsum_{i=1}^m P_i \right)^k \le \left( \frac{em}{k} \right)^k \left( \frac{\sum P_i}{m} \right)^k = \left( \frac{e \sum P_i}{k} \right)^k $$

Now we have an easy case. Suppose $\sum P_i \le \frac{2e}{k}$, then we are done. That is, if the individual bits are $0$ with very low probability, then we are done. But this is hardly an interesting case for us.

\newpage

It remains to handle the case when $\sum_i P_i > \frac{k}{2e}$. Let $m'$ be the largest such that $\sum_{i=1}^{m'} P_i \le \frac{k}{2e}$. We can apply the previous argument for the first $m'$ bits.

Now let us define the following quantities:
$$ V_D = \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \hspace{2cm} V_U = \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] $$
$$ V_D' = \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] \hspace{2cm} V_U' = \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] $$

What we need to upper bound is the expression $|V_D - V_U|$.
By definition, $V_U \le V_U'$, $V_D \le V_D'$ since the event $\bigwedge_{i=1}^m Y_i = 1$ implies $\bigwedge_{i=1}^{m'} Y_i = 1$. Due to the choice of $m'$, by applying the above easy case, we have that $|V_D'-V_U'| \le 2^{-k}$.

Thus it suffices to prove that $V_U' \le 2^{-\Omega(k)}$. We do this below as the last step of the proof.

\begin{eqnarray*}
V_U' & = & \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] = \prod_{i=1}^{n'} \Pr_{Y \leftarrow U} \left[ Y_i = 1 \right] = \prod_{i=1}^{n'} \left(1-P_i\right) \\
& \le & \left(\frac{1}{m'} \sum_{i=1}^{m'} (1-P_i) \right)^{m'} \textrm {(by applying AM-GM inequality)} \\
& \le & \left(\frac{1}{m'} \left(m'-\sum_{i=1}^{m'} P_i\right) \right)^{m'} \le \left(\frac{1}{m'} \left(m'-\frac{k}{2e} \right) \right)^{m'} \textrm{ (since $\sum P_i > \frac{k}{2e}$) }\\
& \le & \left(1-\frac{k}{2em'}\right)^{m'} \le e^{-\frac{k}{2e}} \textrm{ by definition of $e^x$ } \\
& \le & e^{-\Omega(k)} \le 2^{-\Omega(k)}
\end{eqnarray*}

\section{Fooling the PARITY ($\oplus_n$) : Small Biased Sets}

Now we come to fooling {\sc PARITY} of $m$ bits:

We start the following claim that we have used before. For a $w \in \zo^m$, where $w \ne 0$, the probability that a randomly chosen $a \in \zo^m$ satisfies $\langle
w,a \rangle = 0$ is exactly $\half$. How close is this probability to half can be used as a measure of how pure the $m$ bits are. The following definition formalizes this.
\begin{definition}[{\bf Small Biased Distribution}]
Let $\epsilon > 0$. A distribution $Y = (Y_1,Y_2, \ldots, Y_m)$ over $\{0,1\}^m$ is said to be an \textit{$\epsilon$-biased distribution }if $\forall w \in \{0,1\}^m$:
$$\frac{1-\epsilon}{2} \le \Pr_{y \sim Y}\left[ \langle w,y \rangle = 0 \right] \le \frac{1+\epsilon}{2}$$
\end{definition}
\noindent An equivalent definition is in terms of the psuedorandom generators that we discussed in the initial lectures. The algorithm $\calA$ that we attempt to ``fool" is rather simplistic, it is just the parity of a set of $y$ bits (specified by the subset $I \subseteq [k]$). 

\hspace{-6mm}\begin{minipage}{0.55\linewidth}
By our formal definition of "fooling", this is exactly, for any $I \subseteq [m]$:
$$\card{ \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right) - \Pr_{y \sim U} \left( \bigoplus_{i \in I} y_i = 0 \right)} \le \frac{\epsilon}{2}$$
Noting that the second term is exactly $\half$, this is equivalent definition to : for $I \subseteq [m]$, 
$$\frac{1-\epsilon}{2} \le \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right) \le \frac{1+\epsilon}{2}$$
\end{minipage}
\begin{minipage}{0.01\linewidth}
~
\end{minipage}
\begin{minipage}{0.35\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$\\$\bigoplus_{i \in I} [y_i]$}; 
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_g)[below=of q_r,yshift=-2cm]{${\cal G}(y')$};
\node[state,rectangle,minimum width=2.5cm,minimum height=5.25cm](q_b)[below=of q_r,yshift=-0.75cm]{};
\node[](q_l)[above=of q_r,yshift=0.85cm]{${\cal B}(x,y')$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3.5cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3.5cm,yshift=0mm]{};
\node[coordinate](q_3)[below=of q_g,xshift=0cm,yshift=-2cm]{};
\draw[->] ([yshift=1mm]q_r.east) -- ([yshift=1mm]q_0)node[midway] {\sc Yes/No};
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {\tiny $y \in \zo^m$};
\draw[->] ([yshift=-5mm]q_3) -- ([yshift=0mm]q_g.south)node[midway,swap] {$y' \in \{0,1\}^{m'}$};

\end{tikzpicture}
\end{minipage}

\noindent The small biased distributions get their name by the following equivalent definition.

\begin{definition}[{\bf Bias of a distribution}]
A distribution $Y \subseteq \zo^m$ is said to have a \textit{bias} of $\epsilon$ if for any $I \subseteq [m]$:
$$\card{ \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right) - \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 1 \right)} \le \epsilon$$
\end{definition}


%\noindent The equivalence follows by observing that:
%$$\Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 1 \right) = 1 - \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right)$$

\noindent For a distribution $Y$, the support of the distribution $\mathsf{supp}(Y)$ are the elements of the sample space which has a non-zero probability assigned to them. For the $\epsilon$-biased distributions, we would like smaller support. Moreover, a special case is when we have a multiset of small size over which the distribution is uniform. This motivates the following definition.

\begin{definition}[{\bf Small Biased Sets}]
Let $\epsilon > 0$.
A sub(multi)set $S \subseteq \zo^m$ is said to be an \textit{$\epsilon$-biased set} if the distribution $Y$ on $\zo^m$ defined as:
\[
Y(w) = \left\{
\begin{array}{ll}
1/|S| & \textrm{ if $w \in S$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]
is an $\epsilon$-biased distribution on $\zo^m$.
\end{definition}

\noindent The size of small biased set is an important parameter. Imagine that we have $S \subseteq \zo^m$ such that $|S| = \poly(m)$ to be small biased set, and that the set $S$ is explicitly described an indexed by $\alpha \in \zo^\ell$ where $\ell = \log(|S|)$. Then, by choosing $O(\log m)$ bits uniformly at random, we have an $\epsilon$-biased distribution  which in certain situations will be as good as uniform distribution on $\zo^m$ (which requires $m$ bits).

We will quickly remark that $\epsilon$-biased distributions if efficiently and explcitly constructed will lead to newer constructions of expanders and newer $t$-wise independent distributions. We will present these later in this lecture.

\noindent We quickly remark on what is known:

\begin{itemize}
\item By probablistic method, we can show that there exists $\epsilon$-biased spaces of size $O(m/\epsilon^2)$.
\item The first explicit construction was by \cite{NN90,NN93}. The space was of size $O(m/\epsilon^3)$.
\item Incomparable bounds by \cite{AGHP92}. The space constructed is of size $O(m^2/\epsilon^2)$.
\item Improved bounds by \cite{BT09}. The space constructed is of size $O\left((m/\epsilon^2)^{5/4}\right)$.
\item Almost optimal bounds by \cite{Tas17}. The space constructed is of size $O\left(\frac{m}{\epsilon^{2+o(1)}} \right)$.
\end{itemize}

While it may look like a hard fight for optimizing the power for $\epsilon$ in the above expression, there are applications where that decides the boundary of efficiency.

\section{Expanders from Small Biased Sets}
\label{sec:expanders-from-small-biased-sets}
We now describe a connection between small biased sets and expanders. Suppose that $S \subseteq \F_2^m$ is an $\epsilon$-biased set. We claim that this set naturally defines an expander. Viewing $\F_2^m$ as a group, a standard graph associated with it is the Cayley graph, defined as follows:

\begin{definition}[{\bf Cayley Graph of $\F_2^m$ with respect to $\S$}]
For $S \subseteq \F_2^n$, let $G(V,E)$ be the graph defined as follows. $V = \F_2^n$. Define the edges as:
$$E = \left\{ (a,a+w) \mid a \in \F_2^m \textrm{ and } w \in S\right\}$$
\end{definition}

The graph is undirected since $w$ is its own additive inverse. The number of vertices is $2^m$, the degree is exactly $|S|$, assuming multiedges where elements repeat. We prove the following lemma.

\begin{lemma}
\label{lem:expanders-from-small-biased-sets}
The graph $G$ is $(2^m,|S|,\epsilon)$ spectral expander.
\end{lemma}
\begin{proof}
Let $A$ be the normalized adjacency matrix which is of the order $2^m \times 2^m$. We explicitly write down all the linearly independent eigen vectors and then bound the second largest eigen value.

For $y \in \F_2^n$, define a function $\Gamma_y : \F_2^n \to \mathbb{R}$ as : 
$$\forall w \in \F_2^n \textrm{ define } \Gamma_y(w) = (-1)^{\langle y,w \rangle}$$

This function has some nice properties. For any $y \in \F_2^m$, $\Gamma_y(w+w') = \Gamma_y(w)\Gamma_y(w')$. Note that, given a $y \in \F_2^m$, we can consider $\Gamma_y$ as a vector in $\mathbb{R}^{2^m}$. We claim:

\begin{claim}
$\Gamma_y$ are eigen vectors of $A$.
\end{claim}
\begin{proof}
We prove this directly by checking what $A\Gamma_y$ vector will be. Indeed:
\begin{eqnarray*}
\forall a \in \zo^m:~~~ \left(A\Gamma_y\right)[a] 
& = & \sum_{(a,b)\in E} \left(A_{ab}\right) \left(\Gamma_y\right)[b] \\
& = & \frac{1}{|S|} \sum_{(a,b)\in E} \left(\Gamma_y\right)[b] = \frac{1}{|S|} \sum_{(a,b) \in E} \Gamma_y(b) \\
& = & \frac{1}{|S|} \sum_{w \in S} \Gamma_y(a+w) = \Gamma_y(a) \left( \sum_{w \in S} \frac{1}{|S|}\Gamma_y(w) \right) \\
& = & \lambda_y (\Gamma_y[a]) \\
A\Gamma_y & = & \lambda_y \Gamma_y
\end{eqnarray*}
And hence $\Gamma_y$ is an eigen vector of $A$ with eigen value $\bigsum_{w \in S} \frac{1}{|S|}\Gamma_y(w)$.
\end{proof}

Now we turn to bounding the eigen values $\lambda_y$. Notice that $\lambda_{0^m} = 1$. Indeed, when $y=0$, the vector $\Gamma_y \in \mathbb{R}^{2^m}$ is the all $1$s vector and hence is an eigen vector for the eigen value $1$. We claim that when $y \ne 0$, $\lambda_y \le \epsilon$. Let $D$ be the distribution on $\zo^m$ with support as $S$ and uniformly distributed over $S$. Since the set $S$ is $\epsilon$-biased, the distribution $D$ has bias at most $\epsilon$.
\begin{eqnarray*}
\lambda_y = \sum_{w \in S} \frac{1}{|S|} \Gamma_y(w) & = &  \frac{1}{|S|} \sum_{w \in S} (-1)^{\langle y, w \rangle} = \frac{1}{|S|} \left(\sum_{\substack{w \in S \\\langle y, w \rangle = 0}} (1) \right) + \frac{1}{|S|} \left(\sum_{\substack{w \in S \\\langle y, w \rangle = 1}} (-1)\right)\\[4mm]
& = & \Pr_{w \sim D} \left[ \langle y, w \rangle = 0 \right] - \Pr_{w \sim D} \left[ \langle y, w \rangle = 1 \right] \le \epsilon \textrm{\hspace{7mm} since bias of $D$ is at most $\epsilon$.}
\end{eqnarray*}

\end{proof}

\section{Existence of $\epsilon$-Biased Sets}

We now quickly show the existence of $\epsilon$ biased set of size $\frac{m}{\epsilon^2}$. The argument is through probabilistic method. We state the technical theorem first.

\begin{theorem}
For every $\epsilon$, there exists $S \subseteq \zo^m$ of size $O(\frac{m}{\epsilon^2})$ such that $S$ is an $\epsilon$-biased set.
\end{theorem}
\begin{proof}
We choose $z_1, z_2 \ldots z_\ell$ uniformly at random from the set $S \subseteq \zo^m$. We imagine that $S = \{z_1, z_2, \ldots z_\ell\}$. We ask the question. What does it mean for $S$ to be $\epsilon$ biased?  By definition, if $Y$ is the distribution over $\zo^m$ with support as $S$ (and distributed uniformly), we need that:
$\forall w \in \{0,1\}^n \setminus \{0^m\}$:
$$\frac{1-\epsilon}{2} \le \Pr_{y \sim Y}\left[ \langle w,y \rangle = 0 \right] \le \frac{1+\epsilon}{2}$$
$$\textrm{\hspace{-2cm} Equivalently, } \frac{(1-\epsilon)\ell}{2} \le \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} \le \frac{(1+\epsilon)\ell}{2}$$
$$\textrm{\hspace{-2cm} Equivalently, } \left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| \le \frac{\epsilon\ell}{2}$$

To show the existence of the set $S$ of the required size, we need to show that for $\ell$ chosen as required, we should prove :
$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\begin{array}{cc}
 \forall w \in \zo^m, w \ne 0 \\
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| \le \frac{\epsilon\ell}{2}
\end{array}
\right]
 > 0
$$
It suffices to show an upper bound on the complementary event.
$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\begin{array}{cc}
 \exists w \in \zo^m, w \ne 0 \\
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| > \frac{\epsilon\ell}{2}
\end{array}
\right]
< 1
$$
We plan to apply union bound to handle $\exists w \in \zo^m$ part of the statement. Hence, we fix a $w \in \zo^m$, such that $w \ne 0$ and want to derive an upper bound for:

$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| > \frac{\epsilon\ell}{2}
\right]
$$
To model this, define a random variable $X_i$ which takes value $1$ when $z_i$ satisfies $\langle w,z_i \rangle = 0$ and $0$ otherwise. Since $z_i$ is chosen uniformly at random and $w \ne 0$, $\E[X_i] = \frac{1}{2}$. Defining $X = \sum_{i=1}^\ell$ gives us $\E[X] = \frac{\ell}{2}$. We are asking for the probability that $\Pr_{z_1,z_2, \ldots z_\ell} \left[ \card{\aphantom X - \E[X]} \ge \frac{\epsilon\ell}{2} \right]$. By Chernoff's bound\footnote{If $X = \sum_{i=1}^n{X_i}$ then $\Pr \left[ \card{\aphantom X - \E[X]} \ge A \right] \le e^{-A^2/2n}$}
$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ \card{\aphantom X - \E[X]} \ge \frac{\epsilon\ell}{2} \right] \le 2^{-\Omega\left(\frac{\epsilon^2\ell}{2}\right)}$$
With the union bound applied:
\begin{eqnarray*}
\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\begin{array}{cc}
 \exists w \in \zo^m, w \ne 0 \\
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| > \frac{\epsilon\ell}{2}
\end{array}
\right]
& \le & 2^m \times 2^{-\Omega\left(\frac{\epsilon^2\ell}{2}\right)}
\end{eqnarray*}
For this to be less than $1$, we just need to choose $\ell$ such that $m < \frac{\epsilon^2\ell}{2}$. Thus, choice of $\ell = O\left(\frac{m}{\epsilon^2}\right)$ works. Hence, by probabilistic method, there exists a set of size $O\left(\frac{m}{\epsilon^2}\right)$ which is an $\epsilon$-biased set. This completes the proof of existence.
\end{proof}

\section{Explicit Construction of $\epsilon$-biased Sets}

\section{Lowerbounds for the size of $\epsilon$-biased Sets}

\section{Bias Amplification}