\Week{10}{Fooling the Conjunction using $k$-wise Independence}

We have seen limited independence in the previous lecture as a method for randomness efficient error reduction and sometimes complete derandomization of algorithms. Both of these can be thought of us means of studying limited independent bits $y \in \{0,1\}^m$ which can be generated by small number of bits as opposed to $m$ independent random bits. 

One of the views that we developed is that this can also be equivalently be viewed as, the limited independence is able to "fool" the amplification process. In this week, we will first explore this in a fundamental way by asking the following question - what kind of computation can limited independence fool? To formulate this further, let us recall the randomized algorithm set up once again. \\

\begin{minipage}{0.5\linewidth}
Recall that we have a radomized algorithm $\calA$ which takes in the input as $x \in \zon$ and random string is represented by $y \in \zo^m$.
Our derandomization problem was equivalently stated as, given $\calA(x)$ - with $x$ fixed, as an algoithm which takes $y$ as the input, with the guarantee that for most $y$'s the algorithm outputs the correct answer ($0/1$), can we algorithmically (efficiently) find out the correct answer given by the algorithm $\calA$. For the purpose of understanding the idea of fooling restricted $\calA$, we can imagine $\calA$ as computing a Boolean function which transforms $y \in \{0,1\}^m$ bits to $\{0,1\}$. But notation, we will think of it as $f : \zo^m \to \zo$.
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_g)[below=of q_r,yshift=-2cm]{${\cal G}(y')$};
\node[state,rectangle,minimum width=2.5cm,minimum height=5.25cm](q_b)[below=of q_r,yshift=-0.5cm]{};
\node[](q_l)[above=of q_r,yshift=0.85cm]{${\cal B}(x,y')$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3.5cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3.5cm,yshift=0mm]{};
\node[coordinate](q_3)[below=of q_g,xshift=0cm,yshift=-2cm]{};
\draw[->] ([yshift=1mm]q_r.east) -- ([yshift=1mm]q_0)node[midway] {\sc Yes/No};
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y$};
\draw[->] ([yshift=-2mm]q_3) -- ([yshift=0mm]q_g.south)node[midway,swap] {$y' \in \{0,1\}^{m'}$};

\end{tikzpicture}
\end{minipage}

\vspace{2mm}

With the above view, we can define the follwing - a distribution $D$ of $m$ bits, $y \in \{0,1\}^m$, is said to be $(s,\epsilon)$-pseudo-random if for every function $f$ (which abstracts out the algorithm $\calA$) which can be computed resource bounds\footnote{We keep this term to be resource bounds when considering computation of Boolean functions. A natural model of computation here is that of circuits, which we describe later when it is needed.} of $s$, we have that 

$$\left| \Pr_{Y \leftarrow U} [f(Y) = 1] - \Pr_{Y \leftarrow D} [f(Y) = 1] \right| \le \epsilon$$

\noindent We start with the task of fooling very simple Boolean functions $f$. Indeed, the task is not interesting if $f$ is not even depending on all input bits. Say if it depends only on the first bit $y_1$ of $y$, then we can fool the computation by just keeping $y_1$ as a pure random bit and the rest of the bits dependent on it. Indeed, arguably the first Boolean function that we can think of would be the conjuction and disjunction and another one would be parity function, majority function etc. We take them up in the next two sections.

\section{Fooling the Conjuction ($\land_n$) using $k$-wise Independece}

Now we show that the good old $k$-wise independent distributions that we have constructed in the earlier lectures actually fools the conjuction with an exponentially decaying $\epsilon$ with respect to $k$. Formally,

\begin{theorem}
\label{thm:k-wise-indep-fools-and}
Let $D$ be a $k$-wise independent distribution on $\zo^m$. Then, there is a constant $c$ such that :
$$\left| \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \right| \le \frac{1}{2^{ck}}$$
\end{theorem}
\begin{proof}
The main technique in the proof is the inclusion exclusion principle and some clever approximations. The original published proof that we give here is due to 

To start with, we will translate the statement about intersection to unions by applying De-Morgan's laws.

The following discussion will hold true for any distribution $D$ on $\zo^m$.
\begin{eqnarray*}
\Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] & = & 1 - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 0 \right] \\
& = & 1 - \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \textrm{ where $E_i$ is the event $Y_i = 0$. }
\end{eqnarray*}
Using this, we can rewrite what we want to estimate:
$$\left| \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \right| = 
\left| \Pr_{Y \leftarrow U} \left[ \bigcup_{i=1}^m E_i \right] - \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \right|$$

Thus, our task reduces to estimating $\Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right]$. This s where naturally the principle of exclusion can be applied. Recalling the principle : the starting point is the union bound - $\Pr \left[ \bigcup_{i=1}^m E_i \right] \le \Pr[E_1]+\Pr[E_2] + \ldots + \Pr[E_m]$ which forms an upper bound. In an attempt to achieve equality, we subtract the double-counted terms, and this leads to an over-subtraction, which we add again and so on. More formally, we write it in terms of the index sets:

\begin{eqnarray*}
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \le & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] \\
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \ge & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] - \sum_{\substack{S \subseteq [m]\\ |S| = 2}} \Pr \left[ \bigcap_{i \in S} E_i \right] \\
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \ge & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] - \sum_{\substack{S \subseteq [m]\\ |S| = 2}} \Pr \left[ \bigcap_{i \in S} E_i \right] + \sum_{\substack{S \subseteq [m]\\ |S| = 3}} \Pr \left[ \bigcap_{i \in S} E_i \right] \\
\end{eqnarray*}
And in general -  if we define : $T_i = \sum_{\substack{S \subseteq [m]\\ |S| = i}} \Pr \left[ \bigcap_{j \in S} E_j \right]$ and the partial sums $S_\ell = \sum_{i=1}^\ell (-1)^{i+1} T_i$. Using this, we conclude:

$$\forall 1 \le \ell \le m, ~~~\Pr \left[ \bigcup_{i=1}^m E_i \right] ~~~\textrm{ is }~~ \begin{cases}
\ge S_\ell \textrm{ when $\ell$ is odd.}\\
\le S_\ell \textrm{ when $\ell$ is even.}\\
\end{cases}
$$

Whatever we discussed so far is true for any distribution $D$. Let us create notation for the two quantities that we want to estimate:
$$ \Gamma = \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \hspace{2cm} \Delta = \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] $$
and we want to estimate an upper bound for $|\Gamma - \Delta|$.

Now we apply the fact that the distribution $D$ is $k$-wise independent. As per the definition, for any subset $S \subset [m]$, 
$$\Pr_{D \leftarrow Y} \left[\bigcap_{i \in S} E_i = 0 \right] = \prod_{i\in S} \Pr_{D \leftarrow Y} \left[E_i\right] $$
and this is true for both the $k$-wise independent distribution $D$ and the uniform distribution $U$ (which is $n$-wise independent. Interpreting in terms of the above notation, this implies that $\forall \ell \le k$, the value of $S_i$ remains the same whether we work with the distribution $D$ or $U$.

This in particular, implies that : 
$$\forall \ell : 1 \le \ell \le k-1, \textrm{ where $\ell$ is even, } S_{\ell} \le \Gamma \le S_{\ell+1} 
\textrm{ and } S_{\ell} \le \Delta \le S_{\ell+1} 
$$
This gives that 
$$\left| \Gamma - \Delta \right| \le  |S_{\ell+1} - S_{\ell}| \le T_\ell$$
Noticing that the gap between $S_\ell$ and $S_{\ell+1}$ keeps on decreasing with increasing $\ell$, we have that $\left| \Gamma - \Delta \right| \le T_k$. Now we need to estimate an upper bound for $T_k$.
\end{proof}

\paragraph{Estimating an upper bound for $T_k$ :}
Now we estimate the value of $T_\ell$ which involves a useful generalization of the arithemetic mean - geometric mean inequality.
\begin{lemma}
Let $q_1, q_2, \ldots q_m$ be non-negative real numbers. If we choose $S \subset [m]$:
$$\E_{\substack{S \subseteq [m] \\ |S|=1}} \left[ \prod_{j \in S} q_j \right] \ge \E_{\substack{S \subseteq [m] \\ |S|=2}} \left[ \left(\prod_{j \in S} q_j \right)^{1/2} \right] \ge \ldots \le \E_{\substack{S \subseteq [m] \\ |S|=k}} \left[ \left( \prod_{j \in S} q_j \right)^{1/k} \right] \ge \ldots \ge  \E_{\substack{S \subseteq [m] \\ |S|=m}} \left[ \left( \prod_{j \in S} q_j \right)^{1/m}\right]$$
\end{lemma}
\noindent The first term on the LHS is nothing but $\frac{1}{m} \sum_{i=1}^m q_i$ and the last term in the rightmost end is $\left( \prod_{j \in S} q_j \right)^{1/m}$ and hence the above is a generalization of AM-GM inequality (Exercise : Prove it precisely).

To apply this the lemma to bound $T_k$, we set up:
(denote the $P_i = \Pr[E_i]$)
$$T_k = \sum_{\substack{S \subseteq [m]\\ |S| = k}} \Pr_{Y \leftarrow D} \left[ \bigcap_{j \in S} E_j \right] = \sum_{\substack{S \subseteq [m]\\ |S| = k}} \left[ \prod_{i \in S} P_i \right] \le {n \choose k} \E_{\substack{S \subseteq [m]\\ |S| = k}} \left[ \prod_{i \in S} P_i \right] $$
Now we can apply the Lemma to the RHS expression, and get the following :

$$T_k \le {n \choose k} \bigsum_{i=1}^m \left(\frac{1}{m}\bigsum_{i=1}^m P_i \right)^k \le \left( \frac{em}{k} \right)^k \left( \frac{\sum P_i}{m} \right)^k = \left( \frac{e \sum P_i}{k} \right)^k $$

Now we have an easy case. Suppose $\sum P_i \le \frac{2e}{k}$, then we are done. That is, if the individual bits are $0$ with very low probability, then we are done. But this is hardly an interesting case for us.

It remains to handle the case when $\sum_i P_i > \frac{k}{2e}$. Let $m'$ be the largest such that $\sum_{i=1}^{m'} P_i \le \frac{k}{2e}$. We can apply the previous argument for the first $m'$ bits.

Now let us define the following quantities:
$$ V_D = \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \hspace{2cm} V_U = \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] $$
$$ V_D' = \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] \hspace{2cm} V_U' = \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] $$

What we need to upper bound is the expression $|V_D - V_U|$.
By definition, $V_U \le V_U'$, $V_D \le V_D'$ since the event $\bigwedge_{i=1}^m Y_i = 1$ implies $\bigwedge_{i=1}^{m'} Y_i = 1$. Due to the choice of $m'$, by applying the above easy case, we have that $|V_D'-V_U'| \le 2^{-k}$.

Thus it suffices to prove that $V_U' \le 2^{-\Omega(k)}$. We do this below as the last step of the proof.

\begin{eqnarray*}
V_U' & = & \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] = \prod_{i=1}^{n'} \Pr_{Y \leftarrow U} \left[ Y_i = 1 \right] = \prod_{i=1}^{n'} \left(1-P_i\right) \\
& \le & \left(\frac{1}{m'} \sum_{i=1}^{m'} (1-P_i) \right)^{m'} \textrm {(by applying AM-GM inequality)} \\
& \le & \left(\frac{1}{m'} \left(m'-\sum_{i=1}^{m'} P_i\right) \right)^{m'} \le \left(\frac{1}{m'} \left(m'-\frac{k}{2e} \right) \right)^{m'} \textrm{ (since $\sum P_i > \frac{k}{2e}$) }\\
& \le & \left(1-\frac{k}{2em'}\right)^{m'} \le e^{-\frac{k}{2e}} \textrm{ by definition of $e^x$ } \\
& \le & e^{-\Omega(k)} \le 2^{-\Omega(k)}
\end{eqnarray*}

\begin{exercise-prob}[See Problem Set 4~(Problem~\ref{disp-pairwiseindep})]
\begin{show-ps4}{disp-pairwiseindep}
This question discusses a construction of an explicit $\frac{1}{2}$-hitting disperser from an explicit pairwise independent hash family. Given parameters $V = \{1,2,\ldots m\}$ and $\epsilon > 0$, let $D$ be a set of size more than $\lceil \frac{1}{\epsilon} \rceil$. Let $H$ be a pairwise independent hash family that has functions from $D$ to $V$ such that the size of $H$ is polynomial in $|D|/|V|$. Define the following disperser $(U,V,E)$, with $U = H$ and let $$E = \{(h,h(x)) \mid h \in U, x \in D\}$$ That is, each vertex of $U$ corresponds to a hash function, and each outgoing edge from such a vertex corresponds to applying this hash function to a particular value in $D$. Show that what we constructed is a $\frac{1}{2}$-hitting disperser with $|U|$ polynomially bounded in $|V|/\epsilon$ with degree $O(\frac{1}{\epsilon})$ and threshold $\epsilon |U|$.

{\em Hint : It suffices to show that if we take any subset $V'$ of $V$ of size at least $\frac{|V|}{2}$ and a random vertex $h \in U$, the probability that there is an edge between $h$ and $V'$ is more than $1-\epsilon$. Show that this sufficient and then prove the same. In the probability calculation, Chebychev inequality and part (c) of a question from midsem will come handy, which you can use without proof for the purpose of this question.}
\end{show-ps4}
\end{exercise-prob}
