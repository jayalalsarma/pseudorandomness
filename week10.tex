\Week{10}{Fooling Simple Computations : Small Biased Distributions and Sets}

We have seen limited independence in the previous lecture as a method for randomness efficient error reduction and sometimes complete derandomization of algorithms. Both of these can be thought of us means of studying limited independent bits $y \in \{0,1\}^m$ which can be generated by small number of bits as opposed to $m$ independent random bits. 

One of the views that we developed is that this can also be equivalently be viewed as, the limited independence is able to "fool" the amplification process. In this week, we will first explore this in a fundamental way by asking the following question - what kind of computation can limited independence fool? To formulate this further, let us recall the randomized algorithm set up once again. \\

\begin{minipage}{0.5\linewidth}
Recall that we have a radomized algorithm $\calA$ which takes in the input as $x \in \zon$ and random string is represented by $y \in \zo^m$.
Our derandomization problem was equivalently stated as, given $\calA(x)$ - with $x$ fixed, as an algoithm which takes $y$ as the input, with the guarantee that for most $y$'s the algorithm outputs the correct answer ($0/1$), can we algorithmically (efficiently) find out the correct answer given by the algorithm $\calA$. For the purpose of understanding the idea of fooling restricted $\calA$, we can imagine $\calA$ as computing a Boolean function which transforms $y \in \{0,1\}^m$ bits to $\{0,1\}$. But notation, we will think of it as $f : \zo^m \to \zo$.
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_g)[below=of q_r,yshift=-2cm]{${\cal G}(y')$};
\node[state,rectangle,minimum width=2.5cm,minimum height=5.25cm](q_b)[below=of q_r,yshift=-0.5cm]{};
\node[](q_l)[above=of q_r,yshift=0.85cm]{${\cal B}(x,y')$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3.5cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3.5cm,yshift=0mm]{};
\node[coordinate](q_3)[below=of q_g,xshift=0cm,yshift=-2cm]{};
\draw[->] ([yshift=1mm]q_r.east) -- ([yshift=1mm]q_0)node[midway] {\sc Yes/No};
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y$};
\draw[->] ([yshift=-2mm]q_3) -- ([yshift=0mm]q_g.south)node[midway,swap] {$y' \in \{0,1\}^{m'}$};

\end{tikzpicture}
\end{minipage}

\vspace{2mm}

With the above view, we can define the follwing - a distribution $D$ of $m$ bits, $y \in \{0,1\}^m$, is said to be $(s,\epsilon)$-pseudo-random if for every function $f$ (which abstracts out the algorithm $\calA$) which can be computed resource bounds\footnote{We keep this term to be resource bounds when considering computation of Boolean functions. A natural model of computation here is that of circuits, which we describe later when it is needed.} of $s$, we have that 

$$\left| \Pr_{Y \leftarrow U} [f(Y) = 1] - \Pr_{Y \leftarrow D} [f(Y) = 1] \right| \le \epsilon$$

\noindent We start with the task of fooling very simple Boolean functions $f$. Indeed, the task is not interesting if $f$ is not even depending on all input bits. Say if it depends only on the first bit $y_1$ of $y$, then we can fool the computation by just keeping $y_1$ as a pure random bit and the rest of the bits dependent on it. Indeed, arguably the first Boolean function that we can think of would be the conjuction and disjunction and another one would be parity function, majority function etc. We take them up in the next two sections.

\section{Fooling the Conjuction ($\land_n$) using $k$-wise Independece}

Now we show that the good old $k$-wise independent distributions that we have constructed in the earlier lectures actually fools the conjuction with an exponentially decaying $\epsilon$ with respect to $k$. Formally,

\begin{theorem}
\label{thm:k-wise-indep-fools-and}
Let $D$ be a $k$-wise independent distribution on $\zo^m$. Then, there is a constant $c$ such that :
$$\left| \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \right| \le \frac{1}{2^{ck}}$$
\end{theorem}
\begin{proof}
The main technique in the proof is the inclusion exclusion principle and some clever approximations. The original published proof that we give here is due to 

To start with, we will translate the statement about intersection to unions by applying De-Morgan's laws.

The following discussion will hold true for any distribution $D$ on $\zo^m$.
\begin{eqnarray*}
\Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] & = & 1 - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 0 \right] \\
& = & 1 - \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \textrm{ where $E_i$ is the event $Y_i = 0$. }
\end{eqnarray*}
Using this, we can rewrite what we want to estimate:
$$\left| \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] - \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \right| = 
\left| \Pr_{Y \leftarrow U} \left[ \bigcup_{i=1}^m E_i \right] - \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \right|$$

Thus, our task reduces to estimating $\Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right]$. This s where naturally the principle of exclusion can be applied. Recalling the principle : the starting point is the union bound - $\Pr \left[ \bigcup_{i=1}^m E_i \right] \le \Pr[E_1]+\Pr[E_2] + \ldots + \Pr[E_m]$ which forms an upper bound. In an attempt to achieve equality, we subtract the double-counted terms, and this leads to an over-subtraction, which we add again and so on. More formally, we write it in terms of the index sets:

\begin{eqnarray*}
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \le & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] \\
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \ge & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] - \sum_{\substack{S \subseteq [m]\\ |S| = 2}} \Pr \left[ \bigcap_{i \in S} E_i \right] \\
\Pr \left[ \bigcup_{i=1}^m E_i \right] & \ge & \sum_{\substack{S \subseteq [m]\\ |S| = 1}} \Pr\left[ \bigcap_{i \in S} E_i \right] - \sum_{\substack{S \subseteq [m]\\ |S| = 2}} \Pr \left[ \bigcap_{i \in S} E_i \right] + \sum_{\substack{S \subseteq [m]\\ |S| = 3}} \Pr \left[ \bigcap_{i \in S} E_i \right] \\
\end{eqnarray*}
And in general -  if we define : $T_i = \sum_{\substack{S \subseteq [m]\\ |S| = i}} \Pr \left[ \bigcap_{j \in S} E_j \right]$ and the partial sums $S_\ell = \sum_{i=1}^\ell (-1)^{i+1} T_i$. Using this, we conclude:

$$\forall 1 \le \ell \le m, ~~~\Pr \left[ \bigcup_{i=1}^m E_i \right] ~~~\textrm{ is }~~ \begin{cases}
\ge S_\ell \textrm{ when $\ell$ is odd.}\\
\le S_\ell \textrm{ when $\ell$ is even.}\\
\end{cases}
$$

Whatever we discussed so far is true for any distribution $D$. Let us create notation for the two quantities that we want to estimate:
$$ \Gamma = \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] \hspace{2cm} \Delta = \Pr_{Y \leftarrow D} \left[ \bigcup_{i=1}^m E_i \right] $$
and we want to estimate an upper bound for $|\Gamma - \Delta|$.

Now we apply the fact that the distribution $D$ is $k$-wise independent. As per the definition, for any subset $S \subset [m]$, 
$$\Pr_{D \leftarrow Y} \left[\bigcap_{i \in S} E_i = 0 \right] = \prod_{i\in S} \Pr_{D \leftarrow Y} \left[E_i\right] $$
and this is true for both the $k$-wise independent distribution $D$ and the uniform distribution $U$ (which is $n$-wise independent. Interpreting in terms of the above notation, this implies that $\forall \ell \le k$, the value of $S_i$ remains the same whether we work with the distribution $D$ or $U$.

This in particular, implies that : 
$$\forall \ell : 1 \le \ell \le k-1, \textrm{ where $\ell$ is even, } S_{\ell} \le \Gamma \le S_{\ell+1} 
\textrm{ and } S_{\ell} \le \Delta \le S_{\ell+1} 
$$
This gives that 
$$\left| \Gamma - \Delta \right| \le  |S_{\ell+1} - S_{\ell}| \le T_\ell$$
Noticing that the gap between $S_\ell$ and $S_{\ell+1}$ keeps on decreasing with increasing $\ell$, we have that $\left| \Gamma - \Delta \right| \le T_k$. Now we need to estimate an upper bound for $T_k$.
\end{proof}

\paragraph{Estimating an upper bound for $T_k$ :}
Now we estimate the value of $T_\ell$ which involves a useful generalization of the arithemetic mean - geometric mean inequality.
\begin{lemma}
Let $q_1, q_2, \ldots q_m$ be non-negative real numbers. If we choose $S \subset [m]$:
$$\E_{\substack{S \subseteq [m] \\ |S|=1}} \left[ \prod_{j \in S} q_j \right] \ge \E_{\substack{S \subseteq [m] \\ |S|=2}} \left[ \left(\prod_{j \in S} q_j \right)^{1/2} \right] \ge \ldots \le \E_{\substack{S \subseteq [m] \\ |S|=k}} \left[ \left( \prod_{j \in S} q_j \right)^{1/k} \right] \ge \ldots \ge  \E_{\substack{S \subseteq [m] \\ |S|=m}} \left[ \left( \prod_{j \in S} q_j \right)^{1/m}\right]$$
\end{lemma}
\noindent The first term on the LHS is nothing but $\frac{1}{m} \sum_{i=1}^m q_i$ and the last term in the rightmost end is $\left( \prod_{j \in S} q_j \right)^{1/m}$ and hence the above is a generalization of AM-GM inequality (Exercise : Prove it precisely).

To apply this the lemma to bound $T_k$, we set up:
(denote the $P_i = \Pr[E_i]$)
$$T_k = \sum_{\substack{S \subseteq [m]\\ |S| = k}} \Pr_{Y \leftarrow D} \left[ \bigcap_{j \in S} E_j \right] = \sum_{\substack{S \subseteq [m]\\ |S| = k}} \left[ \prod_{i \in S} P_i \right] \le {n \choose k} \E_{\substack{S \subseteq [m]\\ |S| = k}} \left[ \prod_{i \in S} P_i \right] $$
Now we can apply the Lemma to the RHS expression, and get the following :

$$T_k \le {n \choose k} \bigsum_{i=1}^m \left(\frac{1}{m}\bigsum_{i=1}^m P_i \right)^k \le \left( \frac{em}{k} \right)^k \left( \frac{\sum P_i}{m} \right)^k = \left( \frac{e \sum P_i}{k} \right)^k $$

Now we have an easy case. Suppose $\sum P_i \le \frac{2e}{k}$, then we are done. That is, if the individual bits are $0$ with very low probability, then we are done. But this is hardly an interesting case for us.

It remains to handle the case when $\sum_i P_i > \frac{k}{2e}$. Let $m'$ be the largest such that $\sum_{i=1}^{m'} P_i \le \frac{k}{2e}$. We can apply the previous argument for the first $m'$ bits.

Now let us define the following quantities:
$$ V_D = \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] \hspace{2cm} V_U = \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^m Y_i = 1 \right] $$
$$ V_D' = \Pr_{Y \leftarrow D} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] \hspace{2cm} V_U' = \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] $$

What we need to upper bound is the expression $|V_D - V_U|$.
By definition, $V_U \le V_U'$, $V_D \le V_D'$ since the event $\bigwedge_{i=1}^m Y_i = 1$ implies $\bigwedge_{i=1}^{m'} Y_i = 1$. Due to the choice of $m'$, by applying the above easy case, we have that $|V_D'-V_U'| \le 2^{-k}$.

Thus it suffices to prove that $V_U' \le 2^{-\Omega(k)}$. We do this below as the last step of the proof.

\begin{eqnarray*}
V_U' & = & \Pr_{Y \leftarrow U} \left[ \bigwedge_{i=1}^{m'} Y_i = 1 \right] = \prod_{i=1}^{n'} \Pr_{Y \leftarrow U} \left[ Y_i = 1 \right] = \prod_{i=1}^{n'} \left(1-P_i\right) \\
& \le & \left(\frac{1}{m'} \sum_{i=1}^{m'} (1-P_i) \right)^{m'} \textrm {(by applying AM-GM inequality)} \\
& \le & \left(\frac{1}{m'} \left(m'-\sum_{i=1}^{m'} P_i\right) \right)^{m'} \le \left(\frac{1}{m'} \left(m'-\frac{k}{2e} \right) \right)^{m'} \textrm{ (since $\sum P_i > \frac{k}{2e}$) }\\
& \le & \left(1-\frac{k}{2em'}\right)^{m'} \le e^{-\frac{k}{2e}} \textrm{ by definition of $e^x$ } \\
& \le & e^{-\Omega(k)} \le 2^{-\Omega(k)}
\end{eqnarray*}

\section{Fooling the PARITY ($\oplus_n$) : Small Biased Sets}

Now we come to fooling {\sc PARITY} of $m$ bits. We start with the following notion of "almost uniform" distributions. Consider the following claim that we have seen before. For a $w \in \zo^m$, where $w \ne 0$, the probability that a randomly chosen $a \in \zo^m$ satisfies $\langle
w,a \rangle = 0$ is exactly $\half$. How close is this probability to half can be used as a measure of how pure the $m$ bits are. The following definition formalizes this.
\begin{definition}[{\bf Small Biased Distribution}]
Let $\epsilon > 0$. A distribution $Y = (Y_1,Y_2, \ldots, Y_m)$ over $\{0,1\}^m$ is said to be an \textit{$\epsilon$-biased distribution }if $\forall w \in \{0,1\}^m$:
$$\frac{1-\epsilon}{2} \le \Pr_{y \sim Y}\left[ \langle w,y \rangle = 0 \right] \le \frac{1+\epsilon}{2}$$
\end{definition}
\noindent An equivalent definition is in terms of the psuedorandom generators that we discussed in the initial lectures. The algorithm $\calA$ that we attempt to ``fool" is rather simplistic, it is just the parity of a set of $y$ bits (specified by the subset $I \subseteq [k]$). 

\hspace{-6mm}\begin{minipage}{0.55\linewidth}
By our formal definition of "fooling", this is exactly, for any $I \subseteq [m]$:
$$\card{ \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right) - \Pr_{y \sim U} \left( \bigoplus_{i \in I} y_i = 0 \right)} \le \frac{\epsilon}{2}$$
Noting that the second term is exactly $\half$, this is equivalent definition to : for $I \subseteq [m]$, 
$$\frac{1-\epsilon}{2} \le \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right) \le \frac{1+\epsilon}{2}$$
\end{minipage}
\begin{minipage}{0.01\linewidth}
~
\end{minipage}
\begin{minipage}{0.35\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$\\$\bigoplus_{i \in I} [y_i]$}; 
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_g)[below=of q_r,yshift=-2cm]{${\cal G}(y')$};
\node[state,rectangle,minimum width=2.5cm,minimum height=5.25cm](q_b)[below=of q_r,yshift=-0.75cm]{};
\node[](q_l)[above=of q_r,yshift=0.85cm]{${\cal B}(x,y')$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3.5cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3.5cm,yshift=0mm]{};
\node[coordinate](q_3)[below=of q_g,xshift=0cm,yshift=-2cm]{};
\draw[->] ([yshift=1mm]q_r.east) -- ([yshift=1mm]q_0)node[midway] {\sc Yes/No};
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {\tiny $y \in \zo^m$};
\draw[->] ([yshift=-5mm]q_3) -- ([yshift=0mm]q_g.south)node[midway,swap] {$y' \in \{0,1\}^{m'}$};

\end{tikzpicture}
\end{minipage}

\noindent The small biased distributions get their name by the following equivalent definition.

\begin{definition}[{\bf Bias of a distribution}]
A distribution $Y \subseteq \zo^m$ is said to have a \textit{bias} of $\epsilon$ if for any $I \subseteq [m]$:
$$\card{ \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right) - \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 1 \right)} \le \epsilon$$
\end{definition}

It follows from the above that $\epsilon$-biased distributions is exactly same as distributions whose bias is $\epsilon$. The following characterization is also useful. Let $D$ be a distribution over $\zo^m$.
\begin{proposition}
\label{prop:epsilon-biasd-expectation}
$D$ is $\epsilon$-biased $\iff$ $\forall w \in \zo^m \setminus \{0^m\}$, $\E_{y \to D} \left[ (-1)^{\langle w,y \rangle} \right] \le \epsilon$.
\end{proposition}
In the discussion below, for showing that some distributions are $\epsilon$-biased, we show the RHS.

%\noindent The equivalence follows by observing that:
%$$\Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 1 \right) = 1 - \Pr_{y \sim D}\left( \bigoplus_{i \in I} y_i = 0 \right)$$

\paragraph{Small Biased Sets:} For a distribution $Y$, the support of the distribution $\mathsf{supp}(Y)$ are the elements of the sample space which has a non-zero probability assigned to them. For the $\epsilon$-biased distributions, we would like smaller support. Moreover, a special case is when we have a multiset of small size over which the distribution is uniform. This motivates the following definition.

\begin{definition}[{\bf Small Biased Sets}]
Let $\epsilon > 0$.
A sub(multi)set $S \subseteq \zo^m$ is said to be an \textit{$\epsilon$-biased set} if the distribution $Y$ on $\zo^m$ defined as:
\[
Y(w) = \left\{
\begin{array}{ll}
1/|S| & \textrm{ if $w \in S$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]
is an $\epsilon$-biased distribution on $\zo^m$.
\end{definition}

\noindent The size of small biased set is an important parameter. Imagine that we have $S \subseteq \zo^m$ such that $|S| = \poly(m)$ to be small biased set, and that the set $S$ is explicitly described an indexed by $\alpha \in \zo^\ell$ where $\ell = \log(|S|)$. Then, by choosing $O(\log m)$ bits uniformly at random, we have an $\epsilon$-biased distribution  which in certain situations will be as good as uniform distribution on $\zo^m$ (which requires $m$ bits).

We will quickly remark that $\epsilon$-biased distributions if efficiently and explcitly constructed will lead to newer constructions of expanders and newer $t$-wise independent distributions. We will present these later in this lecture.

\noindent We quickly remark on what is known:

\begin{itemize}
\item By probablistic method, we can show that there exists $\epsilon$-biased spaces of size $O(m/\epsilon^2)$.
\item The first explicit construction was by \cite{NN90,NN93}. The space was of size $O(m/\epsilon^3)$.
\item Incomparable bounds by \cite{AGHP92}. The space constructed is of size $O(m^2/\epsilon^2)$.
\item Improved bounds by \cite{BT09}. The space constructed is of size $O\left((m/\epsilon^2)^{5/4}\right)$.
\item Almost optimal bounds by \cite{Tas17}. The space constructed is of size $O\left(\frac{m}{\epsilon^{2+o(1)}} \right)$.
\end{itemize}

While it may look like a hard fight for optimizing the power for $\epsilon$ in the above expression, there are applications where that decides the boundary of efficiency.

\section{Expanders from Small Biased Sets}
\label{sec:expanders-from-small-biased-sets}
We now describe a connection between small biased sets and expanders. Suppose that $S \subseteq \F_2^m$ is an $\epsilon$-biased set. We claim that this set naturally defines an expander. Viewing $\F_2^m$ as a group, a standard graph associated with it is the Cayley graph, defined as follows:

\begin{definition}[{\bf Cayley Graph of $\F_2^m$ with respect to $\S$}]
For $S \subseteq \F_2^n$, let $G(V,E)$ be the graph defined as follows. $V = \F_2^n$. Define the edges as:
$$E = \left\{ (a,a+w) \mid a \in \F_2^m \textrm{ and } w \in S\right\}$$
\end{definition}

The graph is undirected since $w$ is its own additive inverse. The number of vertices is $2^m$, the degree is exactly $|S|$, assuming multiedges where elements repeat. We prove the following lemma.

\begin{lemma}
\label{lem:expanders-from-small-biased-sets}
The graph $G$ is $(2^m,|S|,\epsilon)$ spectral expander.
\end{lemma}
\begin{proof}
Let $A$ be the normalized adjacency matrix which is of the order $2^m \times 2^m$. We explicitly write down all the linearly independent eigen vectors and then bound the second largest eigen value.

For $y \in \F_2^n$, define a function $\Gamma_y : \F_2^n \to \mathbb{R}$ as : 
$$\forall w \in \F_2^n \textrm{ define } \Gamma_y(w) = (-1)^{\langle y,w \rangle}$$

This function has some nice properties. For any $y \in \F_2^m$, $\Gamma_y(w+w') = \Gamma_y(w)\Gamma_y(w')$. Note that, given a $y \in \F_2^m$, we can consider $\Gamma_y$ as a vector in $\mathbb{R}^{2^m}$. We claim:

\begin{claim}
$\Gamma_y$ are eigen vectors of $A$.
\end{claim}
\begin{proof}
We prove this directly by checking what $A\Gamma_y$ vector will be. Indeed:
\begin{eqnarray*}
\forall a \in \zo^m:~~~ \left(A\Gamma_y\right)[a] 
& = & \sum_{(a,b)\in E} \left(A_{ab}\right) \left(\Gamma_y\right)[b] \\
& = & \frac{1}{|S|} \sum_{(a,b)\in E} \left(\Gamma_y\right)[b] = \frac{1}{|S|} \sum_{(a,b) \in E} \Gamma_y(b) \\
& = & \frac{1}{|S|} \sum_{w \in S} \Gamma_y(a+w) = \Gamma_y(a) \left( \sum_{w \in S} \frac{1}{|S|}\Gamma_y(w) \right) \\
& = & \lambda_y (\Gamma_y[a]) \\
A\Gamma_y & = & \lambda_y \Gamma_y
\end{eqnarray*}
And hence $\Gamma_y$ is an eigen vector of $A$ with eigen value $\bigsum_{w \in S} \frac{1}{|S|}\Gamma_y(w)$.
\end{proof}

Now we turn to bounding the eigen values $\lambda_y$. Notice that $\lambda_{0^m} = 1$. Indeed, when $y=0$, the vector $\Gamma_y \in \mathbb{R}^{2^m}$ is the all $1$s vector and hence is an eigen vector for the eigen value $1$. We claim that when $y \ne 0$, $\lambda_y \le \epsilon$. Let $D$ be the distribution on $\zo^m$ with support as $S$ and uniformly distributed over $S$. Since the set $S$ is $\epsilon$-biased, the distribution $D$ has bias at most $\epsilon$.
\begin{eqnarray*}
\lambda_y = \sum_{w \in S} \frac{1}{|S|} \Gamma_y(w) & = &  \frac{1}{|S|} \sum_{w \in S} (-1)^{\langle y, w \rangle} = \frac{1}{|S|} \left(\sum_{\substack{w \in S \\\langle y, w \rangle = 0}} (1) \right) + \frac{1}{|S|} \left(\sum_{\substack{w \in S \\\langle y, w \rangle = 1}} (-1)\right)\\[4mm]
& = & \Pr_{w \sim D} \left[ \langle y, w \rangle = 0 \right] - \Pr_{w \sim D} \left[ \langle y, w \rangle = 1 \right] \le \epsilon \textrm{\hspace{7mm} since bias of $D$ is at most $\epsilon$.}
\end{eqnarray*}

\end{proof}

\section{Existence of $\epsilon$-Biased Sets}

We now quickly show the existence of $\epsilon$ biased set of size $\frac{m}{\epsilon^2}$. The argument is through probabilistic method. We state the technical theorem first.

\begin{theorem}
For every $\epsilon$, there exists $S \subseteq \zo^m$ of size $O(\frac{m}{\epsilon^2})$ such that $S$ is an $\epsilon$-biased set.
\end{theorem}
\begin{proof}
We choose $z_1, z_2 \ldots z_\ell$ uniformly at random from the set $S \subseteq \zo^m$. We imagine that $S = \{z_1, z_2, \ldots z_\ell\}$. We ask the question. What does it mean for $S$ to be $\epsilon$ biased?  By definition, if $Y$ is the distribution over $\zo^m$ with support as $S$ (and distributed uniformly), we need that:
$\forall w \in \{0,1\}^n \setminus \{0^m\}$:
$$\frac{1-\epsilon}{2} \le \Pr_{y \sim Y}\left[ \langle w,y \rangle = 0 \right] \le \frac{1+\epsilon}{2}$$
$$\textrm{\hspace{-2cm} Equivalently, } \frac{(1-\epsilon)\ell}{2} \le \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} \le \frac{(1+\epsilon)\ell}{2}$$
$$\textrm{\hspace{-2cm} Equivalently, } \left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| \le \frac{\epsilon\ell}{2}$$

To show the existence of the set $S$ of the required size, we need to show that for $\ell$ chosen as required, we should prove :
$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\begin{array}{cc}
 \forall w \in \zo^m, w \ne 0 \\
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| \le \frac{\epsilon\ell}{2}
\end{array}
\right]
 > 0
$$
It suffices to show an upper bound on the complementary event.
$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\begin{array}{cc}
 \exists w \in \zo^m, w \ne 0 \\
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| > \frac{\epsilon\ell}{2}
\end{array}
\right]
< 1
$$
We plan to apply union bound to handle $\exists w \in \zo^m$ part of the statement. Hence, we fix a $w \in \zo^m$, such that $w \ne 0$ and want to derive an upper bound for:

$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| > \frac{\epsilon\ell}{2}
\right]
$$
To model this, define a random variable $X_i$ which takes value $1$ when $z_i$ satisfies $\langle w,z_i \rangle = 0$ and $0$ otherwise. Since $z_i$ is chosen uniformly at random and $w \ne 0$, $\E[X_i] = \frac{1}{2}$. Defining $X = \sum_{i=1}^\ell$ gives us $\E[X] = \frac{\ell}{2}$. We are asking for the probability that $\Pr_{z_1,z_2, \ldots z_\ell} \left[ \card{\aphantom X - \E[X]} \ge \frac{\epsilon\ell}{2} \right]$. By Chernoff's bound\footnote{If $X = \sum_{i=1}^n{X_i}$ then $\Pr \left[ \card{\aphantom X - \E[X]} \ge A \right] \le e^{-A^2/2n}$}
$$\Pr_{z_1,z_2, \ldots z_\ell} \left[ \card{\aphantom X - \E[X]} \ge \frac{\epsilon\ell}{2} \right] \le 2^{-\Omega\left(\frac{\epsilon^2\ell}{2}\right)}$$
With the union bound applied:
\begin{eqnarray*}
\Pr_{z_1,z_2, \ldots z_\ell} \left[ 
\begin{array}{cc}
 \exists w \in \zo^m, w \ne 0 \\
\left| \vphantom{2^{2^{2^2}}} \card{\vphantom{2^{2^{2}}}\{i \in [\ell] \mid \langle w,z_i \rangle = 0\}} - \frac{\ell}{2} \right| > \frac{\epsilon\ell}{2}
\end{array}
\right]
& \le & 2^m \times 2^{-\Omega\left(\frac{\epsilon^2\ell}{2}\right)}
\end{eqnarray*}
For this to be less than $1$, we just need to choose $\ell$ such that $m < \frac{\epsilon^2\ell}{2}$. Thus, choice of $\ell = O\left(\frac{m}{\epsilon^2}\right)$ works. Hence, by probabilistic method, there exists a set of size $O\left(\frac{m}{\epsilon^2}\right()$ which is an $\epsilon$-biased set. This completes the proof of existence.
\end{proof}


\section{Explicit Construction of $\epsilon$-biased Sets}

We first digress a little bit and understand the connection between two finite field $\F_q^m$ and $\F_{q^m}$. Let $r(x)$ be an irreducible polynomial\footnote{A polynomial is said to be irreducible if it cannot be written as the product of two other polynomials of smaller degree.} of degree $m$ in $\F_q[x]$ - which are polynomials whose coefficients are from $\F_q$. We claim that the set:
$$\F \defn \left\{ p(x)\mod r(x) : p(x) \in \F_q[x] \right\}$$
forms a finite field where the addition and multiplication is modulo the polynomial $r(x)$. By notation, $\F \equiv \F/\langle
f\rangle$. Notice that, since the set is finite and there is no zero divisor in the set ($a,b \ne 0$ such that $ab = 0$), every non-zero element in $\F$ has an inverse\footnote{Consider $a \ne 0$, and suppose $a$ does not have an inverse, then mutliplying $a$ with all the non-zero elements in $\F$ should not give repeated elements (otherwise it will give zero divisors) and should not contain $0$ and $1$. Hence a contradiction.}. Hence this set is a field. Thus, each distinct element in $\F$ can also be viewed as a tuple of coefficients $(c_0, c_1, c_2, \ldots c_{m-1}) \in (\F_q)^m$ and vice versa. Thus it is a vector space over $\F_q$ of dimension $m$.

For the remaining discussion, $r(x) = a_0 + a_1x+ \ldots a_mx^m$. Let $\alpha$ be a root of $r(x)$. Clearly, since $r(x)$ is irreducible, $\alpha$ is not in $\F_q$ (hence, consider $\alpha$ as an abstract symbol for a root of $r(d)$). Let us "adjoin" $\alpha$ to $\F_q$ and then try to take closure to make it a field. To start with, what is $\alpha^2$?, that becomes abstract another element outside $\F_q$, this way, we introduce $m-1$ new elements, $\alpha, \alpha^2, \ldots \alpha^{m-1}$. At the $m^{th}$ step, what is $\alpha^{m}$? That is not a new element since $r(\alpha) = 0$ and $a_m \ne 0$, hence $\alpha^m = \frac{1}{a_n}(-a_0-a_1\alpha-a_2\alpha^2-\ldots-a_{n-1}\alpha^{n-1})$. But then, all the distinct combinations of the above powers of $\alpha$ with coefficients from $\F_q$ are all distinct elements since $\alpha$ does not introduce any non-trivial relationship among them. This gives rise to a set of size $q^m$ which is a field by construction (again, inverse exists because it is a finite ring without zero divisors). We will call this set to be $\F_{q^m}$.

The two sets that we described above $\F_{q^m}$ and $(\F_q)^m$ have a natural bijection among them. The map from $(\F_q)^m$ to $\F_{q^m}$ is easy to describe : given $(c_0, c_1, c_2, \ldots c_{m-1}) \in (\F_q)^m$, which denotes the polynomial $p(x) = c_0+c_1x+c_2x^2+\ldots c_{m-1}x^{m-1}$. The image in $\F_{q^m}$ is the evaluation $p(\alpha)$. This map is a bijection and also respects the addition and multiplication. It is an isomorphism between the two fields. Thus, we have an isomorphism, $\phi : \F_{q^m} \to \F_q^m$. We will use this map for the construction.

\paragraph{Construction of the $\epsilon$-biased set:} We directly define the set:

$$ S = \left\{ y \in \{0,1\}^m ~\bigger{\bigger{\bigger{|}}}
\begin{array}{c}
y = y_1y_2 \ldots y_m \textrm{ where }\\
y_i = \langle \phi(\alpha^i), z \rangle, \textrm{ for } \alpha,z \in \F_2^\ell 
\end{array}
\right\}
$$

As per the above formulation, the size of the set $S$ is at most $2^{2\ell}$. We will choose the parameter $\ell$ later (to be $\log\left(\frac{m}{\epsilon}\right)$) and this will meet our size requirement. We now prove that for the above value of $\ell$, the set $S$ is $\epsilon$-biased.

\begin{lemma}
\label{lemma:epsilon-biased}
For $\ell = \log\left(\frac{m}{\epsilon}\right)$), the set $S$ is $\epsilon$-biased.
\end{lemma}
\begin{proof}
Consider the distribution $Y$ on $\zo^m$ defined as:
\[
Y(y) = \left\{
\begin{array}{ll}
1/|S| & \textrm{ if $y \in S$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]
For a given $w \in \zo^m$, $w \ne 0$, we need to estimate the probability $\Pr_{y \sim Y} \left[ \langle y, w \rangle = 0 \right]$. This is same as:
\begin{eqnarray*}
Pr_{y \in S} \left[ \langle y, w \rangle = 0 \right] 
& = & \Pr_{y \in S} \left( \sum_{i=1}^m y_iw_i = 0 \right) 
= \Pr_{y \in S} \left[ \sum_{i=1}^m \langle \phi(\alpha^i), z \rangle w_i = 0 \right] \\
& = & \Pr_{y \in S} \left[ \sum_{i=1}^m \langle \phi(\alpha^i)w_i, z \rangle = 0 \right] 
= \Pr_{y \in S} \left[ \sum_{i=1}^m \langle \phi(\alpha^iw_i), z \rangle = 0 \right] \\
& = & \Pr_{y \in S} \left[ \left\langle \sum_{i=1}^m \phi(\alpha^iw_i), z \right\rangle = 0 \right]
= \Pr_{y \in S} \left[ \left\langle \phi \left( \sum_{i=1}^m\alpha^iw_i \right) , z \right\rangle  = 0 \right] \\
& = & \Pr_{y \in S} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \right] \\
&& \textrm{ where $p_w(x) = \sum_{i=1}^mw_ix^i$ is a polynomial defined by $w$ of degree $m$.}
\end{eqnarray*}
Notice that choosing $y \in S$ unformly at random is equivalent to choosing $\alpha$ and $z$ uniformly at random from $\F_2^\ell$. Since $\phi$ takes only $0$ to $0^m$, $\phi(p_w(\alpha)) = 0$ if and only if $p_w(\alpha) = 0$. Hence, we can estimate the above expression by conditioning on the event $p_w(\alpha) = 0$. 
\newpage
\begin{eqnarray*}
\Pr_{\alpha, z \in \F_2^\ell} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \right] 
& = & 
\Pr_{\alpha, z \in \F_2^\ell} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \mid p_w(\alpha) = 0 \right] \Pr_{\alpha \in \F_2^\ell} [ p_w(\alpha) = 0 ] \\
& & + \Pr_{\alpha, z \in \F_2^\ell} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \mid p_w(\alpha) \ne 0 \right] \Pr_{\alpha \in \F_2^\ell} [ p_w(\alpha) \ne 0 ] \\
\end{eqnarray*}

$\Pr_{\alpha \in \F_2^\ell} [ p_w(\alpha) = 0 ] \le \frac{m}{2^\ell}$ since $p_w(x)$ can have at most $m$ roots in $\F_2^\ell$. Denote $p = \frac{m}{2^\ell}$.
Conditioned on $p_w(\alpha) = 0$, we know that $\phi(p_w(\alpha)) = 0$ and,
$\Pr_{\alpha,z \in \F_2^\ell} \langle \phi(p_w(\alpha)), z \rangle  = 0] = 1$.
Conditioned on $p_w(\alpha) \ne 0$, we know that $\phi(p_w(\alpha)) \ne 0$ and,
$\Pr_{\alpha,z \in \F_2^\ell} \langle \phi(p_w(\alpha)), z \rangle  = 0] = \half$. Thus,
$$
\Pr_{\alpha, z \in \F_2^\ell} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \right] 
\le 1 \times p + \half \times (1) \le \half+p 
$$
To see a lower bound, 
\begin{eqnarray*}
\Pr_{\alpha, z \in \F_2^\ell} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \right] 
& \ge & 
\Pr_{\alpha, z \in \F_2^\ell} \left[\left\langle \phi(p_w(\alpha)), z \right\rangle  = 0 \mid p_w(\alpha) \ne 0 \right] \Pr_{\alpha \in \F_2^\ell} [ p_w(\alpha) \ne 0 ]  \\
& \ge & \half(1-p) \ge \half - \frac{p}{2} \ge \half - p
\end{eqnarray*}
Hence, 
$$\textrm{Hence, } \half - p \le \Pr_{y \sim Y} \left[ \langle y, w \rangle = 0 \right] \le \half+p $$

Thus, we just need to ensure that, $p \le \frac{\epsilon}{2}$. We choose $\ell$ such that, $2^{\ell} = \frac{m}{2\epsilon}$. Recall that the set $S$ was of size $2^{2\ell}$. Hence $|S| = \frac{m^2}{4\epsilon^2} \le O\left(\frac{m^2}{\epsilon^2}\right)$. This completes the correctness proof of the construction.
\end{proof}

\section{Repeated Sampling to Improves Bias}

Consider an $\epsilon$-biased set $S \subseteq \zo^m$.
As earlier, the distribution $D$ defined based on the set $S$ on $\zo^m$:
\[
D(y) = \left\{
\begin{array}{ll}
1/|S| & \textrm{ if $y \in S$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]

By definition $D$  is $\epsilon$-biased. We will consider the following distribution derived from $D$. Let $y_1, y_2, \ldots y_t$ be $t$ strings in in $\F_2^m$, independently chosen from distribution $D$. Consider the distribution of their sum $y = y_1+y_2+\ldots+y_t$ where the sum is defined over $\F_2^m$ (and hence is bitwise sum  modulo 2). We will denote the distribution of $y$ over $\zo^m$ to be  $D'$. We claim that $D'$ is $\epsilon^t$ biased.

\begin{claim}
\label{lem:bias-amplification}
The distribution $\D^t$ is $\epsilon^t$-biased.
\end{claim}
\begin{proof}
Let $w \in \zo^m$ such that $w \ne 0$. By Proposition~\ref{prop:epsilon-biasd-expectation} it suffices to upper bound $\E_{y \sim D^t} (-1)^{\langle y,w \rangle}$.
\begin{eqnarray*}
\E_{y \sim D^t} (-1)^{\langle y,w \rangle} & = & \E_{y_1, y_2 \ldots y_t \sim D} \left[ (-1)^{\langle \sum_i y_i,w  \rangle} \right] \\
& = & \E_{y_1, y_2 \ldots y_t \sim D} \left[ \prod_i \left((-1)^{\langle y_i,w  \rangle} \right) \right] = \prod_i \E_{y_i \sim D} \left[ \left((-1)^{\langle y_i,w  \rangle} \right] \right) \le \epsilon^t
\end{eqnarray*}
\end{proof}

\begin{curiousity}
The above is a trivial bias amplification method and the analysis depends on independence of the sampling. One may ask the question, can we do more randomness efficient amplification of the bias? 

Given our background with expanders, it is natural to consider a random walk on expanders. A simple case is consider the following length 2 walk lemma which is attributed to Rozenman and Wigderson (unpublished).
\begin{lemma}
Let $D$ be an $\epsilon$-biased distribution over $\zo^m$ and let $G$ be an $(2^m,d,\lambda)$ spectral expander whose vertices are labeled by samples of $D$ so that the number of vertices labeled $w \in \zo^m$ is proportional to $D(w)$. Let $D'$ be the following distribution: Uniformly choose a random vertex $y_1$ and choose a random neighbor of $y_2$ to sample a random edge $(y_1,y_2)$ of $G$
and output $y=y_1+y_2$. Then $D'$ is $(\epsilon^2+ \lambda)$-biased and with support $O(d.\supp(D))$.
\end{lemma}
Notice that $\epsilon^2$ is smaller than $\epsilon$ and that amplifies the bias, and we lose it out a bit by additive $\lambda$. Thus if we choose a better spectral expander, we achieve amplification without much loss. One can also repeat this again and again, and achieve a good bias amplification for resulting distribution $D_i$ which will be $\epsilon_i$-biased and support size $s_i$ such that $\epsilon_{i+1} = \epsilon_i^2+\lambda_i$ and $s_{i+1} = O(d.s_i)$. Choosing $\lambda_i = \epsilon_i^2$ and $d_i = 1/\lambda_i^4$, this gives:
$$\epsilon_{i+1} = 2\epsilon_i^2 \textrm{ and } s_{i+1} = s_i/\epsilon_i^4$$

Instead of repeating this on different graphs, one can imagine~\cite{Tas17} taking a walk for $t$ steps: that is, choose $y_1$ from $D$ and then take a walk on the $s$-wide replacement product reduces the bias almost optimally. If the labels obtained in the walk be $y_1, y_2, y_3, \ldots y_t$ then output $y = y_1+y_2+\ldots+y_t$. Let $D^t$ be the resulting distribution whose bias we need to estimate. \cite{Tas17} analyses this as follows: let $w \in \zo^m$ and $w \ne 0$. We need to show that:
$$\Pr_{y \sim D^t} \left[ \langle y, w \rangle = 1 \right] = \Pr_{y \sim D^t} \left[ \sum_{i=1}^t \left\langle y_i, w \right\rangle = 1\right]
 \in \left[ \half-\frac{\delta}{2},\half+\frac{\delta}{2}\right] $$
Thus, if we define a bad set $B$ as follows:
$$B = \left\{ u \in \zo^m \mid \sum_{i:w_i=1} u_i = 1  \right\}$$ 
We need to estimate the probability that $\sum_{i=1}^t \left\langle y_i, w \right\rangle = 1$. This is same as the probability that an odd number of $y_i$s sampled falls into the bad set $B$. Compare this with the expander walk based error reduction method, where we wanted to analyse the probability that the majority of the samples fall into a bad set. It is surprising that a similar analysis works for the "parity" of the samples to be in $B$ also. We refer the reader to \cite{Tas17} for modeling this interesting fact algebraically and a proof. See Section 3 of \cite{Tas17}.
\end{curiousity}

\section{Lowerbounds for the size of $\epsilon$-biased Sets}

We showed the construction of $\epsilon$-biased set of size $\left(\frac{m}{\epsilon}\right)^2$ and also showed the existence of $\epsilon$-biased set of size at most $\frac{m}{\epsilon^2}$. We now show that the existence proof is tight. That is any $\epsilon$-biased set has to be that large.

\begin{theorem}
If $S \subseteq \zo^m$ is $\epsilon$-biased, then $|S| \ge \Omega\left(\frac{m}{\epsilon^2 \log{(1/\epsilon)}}\right)$.
\end{theorem}
\begin{proof}
As earlier, the distribution $D$ defined based on the set $S$ on $\zo^m$:
\[
D(y) = \left\{
\begin{array}{ll}
1/|S| & \textrm{ if $y \in S$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]
\noindent By definition $D$  is $\epsilon$-biased. We consider $D^t$ which is $\epsilon^t$-biased by Lemma~\ref{lem:bias-amplification}.\\[-2mm]

\hspace{-6mm}\begin{minipage}{0.55\linewidth}
\vspace{-5mm}
Recall that the support of a distribution $D$ ($\supp(D)$) is the elements of the sample space which has non-zero probability in the distribution. Eg: $\supp(D) = S$. We will study the size of support of $D'$. 
By definition:\\[-2mm]
$$\supp(D^t) = \left\{ y_1+y_2+\ldots+y_t :
\begin{array}{c}
\forall i \in [t]\\ 
y_i \in \supp(D) 
\end{array}
\right\} $$
%\vspace{-2mm}
For picture assumes $0^m \in S$, and is shown for representational purposes.
\end{minipage}
\begin{minipage}{0.35\linewidth}
%\begin{figure}
%\caption{Support of $D$ and $D^t$}
\begin{tikzpicture}
%\AsymCloud{coordinate}{text}{scale factor}
\draw (-3,-2) rectangle (3.5cm,2.5cm);
\AsymCloud{(1,0)}{}{1.6}
\node[] at (0,0){$\supp(D)$};
\node[] at (2,0){$\supp(D^t)$};
\node[] at (2.5,2){$\zo^m$};
\draw (0,0) circle (0.9cm);
\end{tikzpicture} 
%\end{figure}
\vspace{1mm}
\end{minipage}

\noindent We will estimate a lower bound and upper bound for the size of the $\supp(D)$ in terms of $|S|$.
\begin{description}
\item{\sf Estimating an upper bound for $|\supp(D)|$}:
We would like to estimate 
$$\card{\left\{ y_1+y_2+\ldots+y_t :
\begin{array}{c}
\forall i \in [t]\\ 
y_i \in \supp(D) 
\end{array}
\right\}}$$
Let $s = |S| = |\supp(D)|$. Thus we need to choose $y_1, y_2, \ldots y_\ell \in S$ each of which repeats $c_1, c_2, \ldots, c_\ell$ such that $c_1+c_2+\ldots+c_\ell = t$. This is at most ${s+t-1 \choose t} \le {s+t \choose t} \le \left(\frac{(s+t)e}{t}\right)^t$.
\item{\sf Estimating an lower bound for $|\supp(D)|$}:
We will use the following lemma which we leave as an exercise:
\begin{lemma}
If $D$ is an $\epsilon$-biased distribution, then as a vector $D \in \mathbb{R}^{2^m}$:
$$ \frac{1}{|\supp(D)|} \le \norm{D} \le \epsilon^2+\frac{1}{2^m}$$
\end{lemma}
\vspace{-5mm}
$$\hspace{-21mm}\textrm{ Applying this lemma to $D^t$ which is $\epsilon^t$ biased:~~~~}
\frac{1}{|\supp(D^t)|} \le \norm{D^t} \le \epsilon^{2t}+\frac{1}{2^m}$$
Choose $t$ such that $\epsilon^{2t} = 2^{-m}$, which is equivalent to $t = \frac{m}{2\log(1/\epsilon)}$.
This gives, 
$$\frac{1}{2\epsilon^{2t}} \le |\supp(D^t)| \le \left(\frac{(s+t)e}{t}\right)^t \textrm{ and hence, }
\frac{1}{2^{1/t}\epsilon^{2}} \le \frac{se}{t}+e$$
This gives : $|S| \ge \frac{m}{2^{1/t}}\left(\frac{1}{\epsilon^2 \log(1/\epsilon)}\right)$. Choose $m$ large enought to get $|S| \ge \frac{m}{\epsilon^2 \log(1/\epsilon)}$. Hence the proof.
\end{description}
\end{proof}

\begin{exercise-prob}[See Problem Set 4~(Problem~\ref{epsilon-biased})]
\begin{show-ps4}{epsilon-biased}
If $D$ is an $\epsilon$-biased distribution over $\{0,1\}^m$, then as a vector $D \in \mathbb{R}^{2^m}$:
$$ \frac{1}{|\supp(D)|} \le \norm{D} \le \epsilon^2+\frac{1}{2^m}$$
\end{show-ps4}
\end{exercise-prob}


\begin{exercise-prob}[See Problem Set 4~(Problem~\ref{k-wise-indep})]
\begin{show-ps4}{k-wise-indep}
Let $G : \zo^{k \log m} \to \zo$ be the generator that we described such that $G\left(U_{k \log m}\right)$ outputs a $k$-wise independent distribution. (See the explicit construction of $k$-wise independent distribution). If we replace the input to $G$ with a small bias distribution of $\epsilon' = \frac{\epsilon}{2^k}$, then the output of $G$ is $\epsilon$-close to being $k$-wise independent. Thus, conclude that, there is a generator for almost $k$-wise independent distributions with seed length $O\left(\log\log m+k+\log(1/\epsilon)\right)$.
\end{show-ps4}
\end{exercise-prob}
