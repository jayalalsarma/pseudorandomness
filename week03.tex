\Week{3}{Amplification : Making Algorithms Err Less}
\noindent 

In the last week, we saw algorithm specific techniques of derandomization. More precisely, the derandomization uses the peculiarity of the algorithms in the way they use the random bits and the analysis. In this week, we will go back to the abstract set up where we know nothing about the algorithm other than the resource bounds it uses.

We recall some notations first. A randomized algorithm ${\cal{A}}$ on input $x$ runs in time $t(n)$ (where $n=|x|$) and let $y \in \{0,1\}^{m(n)}$ be the concatenation of the unbiased coin toss experiement that the algorithm does during its execution. Notice that $m(n) \le t(n)$ (we drop the $n$ when it is not required explicitly). If the algorithm runs in polynomial time $t(n) \le n^c$ for a constant $c$ independent of $n$. \\

\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^m$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.5\linewidth}
\vspace{-7mm}
The guarantee we have is there is an $\epsilon \in (0,\half]$.
\vspace{-3mm}
$$\forall x \in \{0,1\}^n,~\Pr_{y \in \{0,1\}^m} [A(x,y) \textrm{ is correct.}] \ge \half+\epsilon $$
\end{minipage}
\vspace{3mm}

\noindent Imagine that we had a success probability of very close to 1. That is, $\epsilon > \half-\frac{1}{2^{m}}$. That is, $\forall x \in \{0,1\}^n$ : 
$\Pr_{y \in \{0,1\}^m} [A(x,y) \textrm{ is correct.}] > 1-\frac{1}{2^m}$. Notice that, now the algorithm does not need to use randomness and can fix $y$ to be any string in $\{0,1\}^m$ and run the algorithm ${\cal{A}}$ and the answer is guaranteed to be correct. (This is because, if there exists at least one $y \in \{0,1\}^m$ for which the algorithm errs then the success probability would have been $\le 1-\frac{1}{2^m}$.) Thus we would have derandomized the algortithm efficiently.

But how do we achieve such high success probability? Viewing a randomized algorithm as an experiment that we do in physics lab to compute a value, we will repeat the experiment and take the most frequent value in order to reduce the error. However, this also increases the number of random bits which goes against the above plan. Let us formally review this amplifcation method nevertheless.

\section{Success Probability Amplifcation by Repetition}

We first write down the algorithm which follows the simple idea of repetition with independent random bits.

\begin{algorithm}
\label{alg:trivial-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State {\em count} $\gets 0$.
\State Choose $k$ independent random strings $y_1, y_2, \ldots y_k \in \{0,1\}^m$. \Comment{Uses $O(kn)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.
\end{algorithmic}
\end{algorithm}

Why would this improve the success probability? and if so, how does it depend on $k$?. The following lemma answers these. Fix the input $x$. Let ${\cal{E}}$ represent the event that ${\cal{A}}$ accept on the random string $y$.

\begin{lemma}
If $\mathcal{E}$ is an event that $Pr(\mathcal{E}) \geq \frac{1}{2} + \epsilon $, then the probability the $\mathcal{E}$ occurs atleast $\frac {k}{2}$ times on $k$ independent trials is at least 
$1-\frac{1}{2}(1-4\epsilon^2)^\frac{k}{2}$
\end{lemma}
\begin{proof}
Let $q$ denote the probability the $\mathcal{E}$ occurs atleast $\frac {k}{2}$ times on $k$ independent trials.
Let $q_i$ = Pr($\mathcal{E}$ occurs exatly $i$ times in $k$ trials), $0 \leq i \leq k$. Thus,
$q = 1 - \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}$ $q_i$. We will analyse the complementary event:
Pr($\mathcal{E}$ occurs atmost $\frac{k}{2}$ times) = $\sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}$ $q_i$. \\ 
We show an upper bound on each $q_i$ and thus show an lower bound on $q$.
\begin{eqnarray*}
q_i & = & {k \choose i} (\frac{1}{2} + \epsilon)^{i} (\frac{1}{2} - \epsilon)^ {k-i} \\
& \leq & {k\choose i} \left(\frac{1}{2} + \epsilon\right)^{i} \left(\frac{1}{2} - \epsilon\right)^ {k-i} \left(\frac{\frac{1}{2} + \epsilon}{\frac{1}{2} - \epsilon}\right)^{\frac{k}{2} - i}  (\textrm{because }\epsilon \le \frac{1}{2} ) \\
& = & {k\choose i}\left(\frac{1}{2} + \epsilon\right)^{\frac{k}{2}}\left(\frac{1}{2} - \epsilon\right)^{\frac{k}{2}} \\
&= & {k\choose i} \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}}
\end{eqnarray*}
Now we analyse the sum:
\begin{eqnarray*}
\sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}q_i & \leq & \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}{k\choose i} \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}} \\
q = 1 - \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}q_i & \geq & \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}{k\choose i} \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}} \\
& = & 1 - \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}} 2^{k-1} \\
& = & 1 - \frac{1}{2} \left(1 - 4\epsilon^2\right)^{\frac{k}{2}} \\
\textrm {Thus, } q & \ge & 1 - \frac{1}{2} \left(1 - 4\epsilon^2\right)^{\frac{k}{2}}
\end{eqnarray*}
\end{proof}

\noindent Thus, if we had an algorithm ${\cal{A}}$ with $\epsilon = \frac{1}{3}$ (that is success probability is at least $\frac{2}{3}$, and we want an algorithm with ${\cal{A'}}$ with $\epsilon = \frac{1}{4}$ (that is, success probability is at least $\frac{3}{4}$). Then, the number of times the iteration that needs to be done can be back calculated as the $k$ that satisfies:
$$1-\half\left(1-\frac{4}{9}\right)^{\frac{k}{2}} \ge \frac{3}{4}$$
which will be a constant. Quite interestingly, we can do this even when the required error probability is exponentially small. That is, suppose we require the success probability to be $1-\frac{1}{2^{q(n)}}$ which is quite close to 1. Then the value of $k$ should be :

$$1-\half\left(1-\frac{4}{9}\right)^{\frac{k}{2}} \ge 1-\frac{1}{2^{q(n)}} \Longrightarrow
\left(\frac{9}{5}\right)^{\frac{k}{2}}\le 2^{q(n)-1}$$
which in turn would imply $k = p(n)$ to be a polynomial value in terms of $n$.
Thus we have the following lemma:

\begin{lemma}[{\bf Amplification Lemma}]
\label{lem:amplification}
Let ${\cal{A}}$ be a randomized algorithm running in time $\poly(n)$ which has $\half+\epsilon$ as the probability of success. Then for any $q(n)$, we have a randomized algorithm ${\cal{A'}}$ that runs in time $\poly(n)$ with the following success probability. For every input $x \in \{0,1\}^{n}$.
\[ \Pr_{y} \left[ {\cal{A'}}(x,y) \text{ is {\em correct} } \right] \geq 1-2^{-q(n)} \]
\end{lemma}

Define $\Gamma_x = \{ y \in \{0,1\}^{p(n)}\mid {\cal{A}}(x,y) \textrm{ is correct }\}$.\\[-2mm]

\begin{minipage}{0.6\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A'}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^{p(n)}$};
\end{tikzpicture}

\vspace{2mm}
The guarantee we have is : 
$\forall x,~|\Gamma_x| \geq (1-2^{-q(n)})2^{p(n)}$
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.3\linewidth}
\vspace{-1cm}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\draw (2,0) ellipse (2cm and 2.5cm) node[midway,yshift=-1cm, xshift=1cm]{$\{0,1\}^{p(n)}$};
\draw (2,1) ellipse (1cm and 1cm)
node{$\Gamma_x$};;
\end{tikzpicture}
\end{minipage}

%
%\begin{exercise-prob}[See Problem Set 1(Problem~\ref{amplification-one-sided})]
%\begin{show-ps1}{amplification-one-sided}
%We saw in class the amplification of success probability by repetition. The analysis involved binomial coefficients simple manipulations involving them. The algorithm and the analysis gets much simpler when the algorithm given is one-sided error. Suppose we are given a randomized algorithm which is one-sided error with error probability, at most $\epsilon \in (0,1]$, modify the algorithm to say {\sc Yes} if at least one of the trials says {\sc Yes}. Derive a relation between $\epsilon$, $k$ (the number of repetitions) and the target bound of error probability (say $\epsilon'$).
%\end{show-ps1}
%\end{exercise-prob}

\begin{exercise-prob}[See Problem Set 1(Problem~\ref{amplification-by-tailbound})]
\begin{show-ps1}{amplification-by-tailbound}
A decision problem $L$ is in a class called $\BPP$ if there exists a randomized polynomial-time algorithm $A$ such that for every $x \in L$ it holds that $\Pr[A(x,y) = 1] \ge \frac{2}{3}$, and for every $x \notin L$ it holds that $\Pr[A(x) = 0] \ge \frac{2}{3}$. For $\epsilon : \mathbb{N} \to [0,1]$, let $\BPP_{\epsilon}$ denote the class of decision problems that can be solved in probabilistic polynomial time with error probability upper-bounded by $\epsilon$. Prove the following two claims:
\begin{enumerate}[(a)]
\item 
For every positive polynomial $p$ and $\epsilon(n) = \frac{1}{2}-\frac{1}{p(n)}$, 
the class $\BPP_{\epsilon}$ equals $\BPP$.
\item For every positive polynomial $p$ and $\epsilon(n) = 2^{-p(n)}$, 
the class $\BPP_\epsilon$ equals $\BPP$.
\end{enumerate}
We already proved something similar in class (See Amplication Lemma). This exercise asks you to prove the same using tail bounds. Given an algorithm $A$, consider an algorithm $A'$ that on input $x$ invokes $A$ on $x$ for $t(|x|)$ times, and decided based on majority as we did in class. For Part (a) $t(n) = O(p(n)^2)$ and apply Chebyshev’s Inequality. For Part 2 set $t(n) = O(p(n))$ and apply the Chernoff Bound.
\end{show-ps1}
\end{exercise-prob}


\section{Sipser's Argument}

From the previous section, we concluded that, for every $x \in \{0,1\}^{p(n)}$, there is a large set $\Gamma_x$ of random strings which are "good" for $x$. However, we do not know how to find even an element $y \in \Gamma_x$ in time $\poly(n)$. If we do, then we have a derandomization for the algorithm ${\cal{A'}}$ (which derandomizes ${\cal{A}}$ too).

It is intuitive to think that since a large fraction of $y \in \{0,1\}^{p(n)}$ are good for each $x \in \{0,1\}^n$, could there be a single $y$ which is good for all $x$? We show that this is indeed the case. This, in fact, is a simple consequence of the amplification lemma in the previous section.

We first apply Lemma~$\ref{lem:amplification}$ with $q(n) = 2n$. Thus we have a randomized polynomial time algorithm with error bound $2^{-2n}$ (i.e. at most $2^{-2n}$ fraction of random strings are ``bad'') using $p(n)$ randomness. More precisely, we have that:
\begin{align}
  \textrm{Answer is {\sc Yes} for input $x$} \implies \Pr_{y} \left[ {\cal{A}}(x,y) \text{ accepts } \right] & \geq
   1-2^{-2n} \\
  \textrm{Answer is {\sc No} for input $x$} \implies \Pr_{y}\left[ {\cal{A}}(x,y) \text{ accepts } \right] & \geq
   2^{-2n} 
\end{align}
That is in such a machine the number of random strings $y$ which lead
the machine to output a wrong answer is bounded by $2^{-2n}$. Now let
us consider a matrix $M$ whose rows are indexed by inputs of length
$n$ and columns are indexed by random strings of length $p(n)$, and
the $(x,y)$th entry is $1$ if on fixing the random bits to be $y$ the
machine $M$ on input $x$ outputs correctly and it is $0$
otherwise. That is $M[x,y]=1$ if and only if ${\cal{A'}}(x,y)$ is correct for the input $x$. By the amplification we
are guaranteed that for a given input $x$ at most $2^{-2n}$ fraction
of the random strings can have $M[x,y]=0$. Hence the total number of
zeros in the $A$ matrix is at most the number of rows times the
maximum number of zeros in a row, which is equal to 
\begin{eqnarray*}
  \text{\# 0's in matrix }M & \leq 2^n \times 2^{-2n} \times 2^{p(n)}
  \\
                            & \leq 2^{p(n)-n}
\end{eqnarray*}

But the total number of zeros, $2^{p(n)-n}$ is strictly less than the
number of columns in the matrix $M$. Hence there must be at least one
column with no zeros in it. If a column in the $M$ matrix has no zeros
then by the definition of $M$ matrix, the random string $w$ represented by
this column when fed as random bits to machine ${\cal{A}'}(x,w)$ would output
the correct answer for every $x\in \{0,1\}^{n}$. Thus we have the follwing lemma.

\begin{lemma}
For every $n$, there is a $y \in \{0,1\}^{p(n)}$ such that $y \in \Gamma_x$ for every $x \in \{0,1\}^n$.
\end{lemma}

Thus there is a function $h : \mathbb{N} \to \Sigma^*$ such that the answer to $x$ is {\sc Yes} if we run the algorithm $\calA$ on input $x$ and random string $h(|x|)$ it is guaranteed not to err. This will give a complete deranomization. However, we need the function $h$ to be computable in polynomial time and that is a challege.

\section{Amplification with Dependent Trials using Expanders}
\label{sec:intro-expanders}

We study another seemingly unrelated graph theoretic object which are called expanders. They have been extensively studied and used in many areas of science and engineering as the set of sparse graph families which has high connectivity connectivity properties. These are particularly desirable in network design such that the network is highly fault tolerant - even the failure of a few links (edges) will not affect the connectivity of the network. The fact that this can be achieved without having too many edges (complete graph is a trivial way to do this) is what makes expander graphs more applicable in this setting.

Expanders have been studied in the theoretical side as well for past few decades and have found numerous applications. Informally, the graph is such that every subset of vertices fetches a large set of vertices in its immediate neighborhood. For a set of vertices in a graph $G$, define $N(S)$ to be the neighbors of $V$. We define the graph class formally now.

\begin{definition}{(\bf Expander Graphs)}
A graph $G(V,E)$ is said to be a $(\alpha,\beta)$-expander if every $S \subseteq V$ such that $|S| \le \alpha|V|$, the number of new neighbors $|N(S)\setminus S| \ge \beta|S|$.
\end{definition}

A trivial (but unfortunately useless) example of an expander graph is the complete graph. Indeed, in a complete graph on $n$ vertices, if we choose any $S \subseteq V$, such that $|S| \le \frac{n}{2}$, we have that $N(S)\setminus S = V \setminus S$. Hence, $|N(S)\setminus S| \ge |S|$. This gives that it is a $(\half,1)$-expander as per the above definition. However, it is going to be useless as we will see in our application. In fact, ideally, we would like expander graphs where the degree $d$ and $\alpha, \beta$ are all constants independent of $n$.

Thus, we are looking for graphs with constant degree (if possible, even regular). It implies that the graph must be sparse\footnote{a graph is said to be sparse if $|E| = O(|V|)$, or else it is called dense.}
Let us also record a non-example of an expander, which is even a sparse graph. Consider an $n \times n$ complete grid graph, which has $n^2$ vertices and all possible {\em grid edges} present. Consider a set $S$ which is of size $\sqrt{n}$, which has $n$ vertices and hence $|S| \le \alpha |V|$ for any constant $\alpha$. However, the number of new neighbors for the set in the grid graph will be $O(\sqrt{n})$ which is not at most $\beta|S|$ for any constant $\beta$. Hence such a graph is not an expander for any constant $\alpha$ and $\beta$.

To ensure that the definition of expander is practiced enough, we suggest the following exericse which derives a combinatorial consequence of the expansion property.

\begin{exercise-prob}[See Problem Set 1(Problem~\ref{expander-diameter})]
\begin{show-ps1}{expander-diameter}
Let $G(V,E)$ be an undirected graph $n$ vertices which is $(\frac{1}{2},2)$-expander. Show that the diameter of the graph is at most $O(\log n)$. The diameter of a graph is at most $k$ if and only if between any two vertices in the graph $G$, there is a path of length at most $k$ in the graph $G$.
\end{show-ps1}
\end{exercise-prob}


\paragraph{Random Walks on Expanders and Our Application:} Although the above definition explains the name expander graphs, the application in our context comes from the fact that if we choose a vertex in the expander at random and then choose the next vertices uniformly at random, the distribution of the $\ell^{th}$ vertex that you get to (as $\ell$ increases) is very close to uniform over the entire connected component of the graph where your starting vertex was lying. Intuitively, this is termed as the {\em rapid mixing} of random walks on expander walks.

We will prove the technical details of this idea at a later point in the course where we introduce expander graphs. Informally, to apply the expander graphs in the context of the success probability amplification, we consider the graph on $G$ as a graph on $2^{p(n)}$ vertices indexed by $\{0,1\}^{p(n)}$. That is, each vertex in the graph can be interpreted as a $y \in \{0,1\}^{p(n)}$. Based on this, the following algorithm gives a good amplification of success probability.


\begin{algorithm}
\label{alg:trivial-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State {\em count} $\gets 0$. 
\State Let $G(V,E)$ be an expander graph on $2^{p(n)}$ vertices.
\State Choose a vertex $y_1 \in V$ uniformly at random. \Comment{Uses $O(p(n))$ random bits}
\State Starting at $v$ perform a random walk for $k$ steps in $G$. Let $y_1, y_2, \ldots y_k \in \{0,1\}^m$ be the vertices representing the walk. \Comment{Uses $O(k \log d)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.
\end{algorithmic}
\end{algorithm}

The idea of the above algorithm is based on the rapid mixing of random walks. Without the details, the strings $y_1, y_2, \ldots y_k$ are ``almost" as good as randomly chosen $y_i$'s since the random walk mixes fast and hence the amplification (although with weaker parameters) can be derived. This will be done in upcoming lectures.

Notice that the number of random bits used by the algorithm is $O(k \log d)$. This explains why the complete graph (whose degree in this case would have been $2^{p(n)}-1$) would not have been good enough for our purpose because it leads to an exponential running time for our algorithm.

\section{Amplification for One-sided Error Algorithms using Dispersers}

Now we handle the case of one-sided error algorithms. These algorithms have the following peculiarity. For a one-sided error algorithm ${\cal{A}}$:
\begin{align}
  \textrm{Answer is {\sc Yes} for input $x$} \implies \Pr_{y} \left[ {\cal{A}}(x,y) \text{ accepts } \right] & \geq
   \half+\epsilon \\
  \textrm{Answer is {\sc No} for input $x$} \implies \Pr_{y}\left[ {\cal{A}}(x,y) \text{ accepts } \right] & =
   0 
\end{align}
The success probability amplification of such algorithms is easier.

\begin{algorithm}
\label{alg:onesided-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State Choose $k$ independent random strings $y_1, y_2, \ldots y_k \in \{0,1\}^m$. \Comment{Uses $O(km)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, {\sc Accept}.
\EndFor
\State {\sc Reject}.
\end{algorithmic}
\end{algorithm}

\begin{lemma}
If ${\cal{A}}$ has success probability at least $1-\epsilon$ then ${\cal{A}'}$ has success probability at least $1-\epsilon^k$.
\end{lemma}
\begin{proof}
The argument is rather straightforward. Suppose the answer is {\sc No}, then we know that no matter what $y_i$ we choose, ${\cal{A}}(x,y_i)$ rejects and hence the algorithm ${\cal{A}'}$ rejects. Suppose the answer is {\sc Yes}, then ${\cal{A}'}$ rejects (which is an error), if none of the $y_i$ chosen makes ${\cal{A}}(x,y_i)$ accept in step 3. Since each $y_i$ is chosen uniformly at random:
$$Pr[ \forall i, {\cal{A}}(x,y_i) \textrm{ rejects }
 ] = \prod_{i} \left( Pr[ {\cal{A}}(x,y_i) \textrm{ rejects }] \right) \le \epsilon^k $$
Hence, the probability that  ${\cal{A}'}$ accepts is at least $1-\epsilon^k$.
\end{proof}

Again, we can use the ideas from the previous section on expanders in order to do more randomness efficient amplification of success probability. However, since the algorithm is one-sides, we have other methods too. 
We now define a new combinatorial object called \textit{disperser} which is useful for amplifying the success probability of an algorithm which has one-sided error, using much less number of random bits.

\begin{definition}[{\bf Dispersers}]
A bipartite graph $G=(U \cup V, E)$ is called an $(1- \epsilon)$-disperser with threshold $T$ if, for
all $S \subseteq U$ of size $|S|\ge T$, we have that the size of the set of neighbors of $S$ in $V$,$|\Gamma(S)|$, is more than $\epsilon|V|$.
\end{definition}

Notice the difference between the definition of expanders and dispersers. Here a large set should have large fraction on the right side as the neighbors. Intuitively, disperser will ``disperse" from a large to set to a good fraction of vertices on the right side.

We will now use a disperser to do randomness efficient amplification for one sided error algorithms. Notice that the algorithm ${\cal{A}}$ is using $r$ random bits. 

\begin{theorem}
Suppose we have an explicit $\half$-disperser with threshold $T$, degree $d$, $U = \{0,1\}^R$ and $V = \{0,1\}^m$. Then a one-sided error algorithm running in time $t$, using $r$ random bits to achieve error probability $\half$ can be converted to a one-sided error algorithm, running in time $(R+d+t)^{O(1)}$ using $R$ random bits to achieve error probability $\frac{T}{2^R}$.
\end{theorem}
\begin{proof}
We write down a formal proof of the above theorem. Suppose the algorithm that we start with is ${\cal{A}}$ with success probability $\half$. The amplification algorithm is as follows. Suppose we have an explicit disperser with the above parameters. For a vertex $y \in U$, denote $N(y)$ to be the neighbors of $y$. Indeed, $N(y) \subseteq V$.

\begin{algorithm}
\label{alg:onesided-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State Choose $y \in U=\{0,1\}^R$ uniformly at random. \Comment{Uses $R$ random bits.}
\State Let $N(y)$ be $\{y_1, y_2, \ldots y_d\}$.
\For{\texttt{each $i \in [d]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, {\sc Accept}.
\EndFor
\State {\sc Reject}.
\end{algorithmic}
\end{algorithm}

\noindent We analyse the probability of error in the {\sc Yes} case. For the algorithm ${\cal{A}}$ consider the following set of good random strings.
$$\Gamma_x = \{ y \in \{0,1\}^m \mid {\cal{A}}(x,y) \textrm{ says {\sc Yes} }\}$$
From the fact that the success probability of ${\cal{A}}$ is at least $\half$, we have $|\Gamma_x| \ge \half|U|$.
Notice that for any $y \in \{0,1\}^R$, $y$ is bad for ${\cal{A}'}$ if none of $N(y) = \{y_1, y_2, \ldots y_d\}$ is in $\Gamma_x$. Define $S \subseteq U$ as follows.
$$S = \{ y \in \{0,1\}^R \mid N(y) \cap \Gamma_x = \phi \}$$

\noindent Suppose the error probability is more than $\frac{T}{2^R}$. This implies that $|S| > T$. However, $N(S) \cap \Gamma_x \ne \phi$. This set $S$ contradicts the definition of disperser. Hence the proof.
\end{proof}

\section{Deranomization of One-sided Error Algorithms using Hitting Set Generators}

We now give a method of completely derandomizing One-sided error algorithms using the combinatorial objects called hitting sets.

\begin{definition}[{\bf Hitting Sets}]
Let ${\cal{C}}$ be a collection of subsets over a finite universe $U$. A $(1-\epsilon)$-hitting set for ${\cal{C}}$ is a set $H \subseteq U$ such that for every $S \in {\cal{C}}$ of size at least $(1-\epsilon)|U|$, we have:
$ S \cap H \ne \phi$
\end{definition}

What is the connection to deranomization. Define $\Gamma_x = \{ y \in \{0,1\}^{p(n)}\mid {\cal{A}}(x,y) \textrm{ is correct }\}$. 
If the success probability of ${\cal{A}}$ is at least $1-\epsilon$, we have $|\Gamma_x| \ge (1-\epsilon)|U|$.

\begin{minipage}{0.6\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A'}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^{p(n)}$};
\end{tikzpicture}

\noindent Hence a hitting set in the above definition is usefu
l, if the collection of sets is the set of possible $\Gamma_x$ for every $x$ of length $n$ and for every algorithm ${\cal{A}}$ which runs in polynomial time.
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.25\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\draw (2,0) ellipse (2cm and 2.5cm) node[midway,yshift=-1cm, xshift=1cm]{$\{0,1\}^{p(n)}$};
\draw (2,1) ellipse (1cm and 1cm)
node{$\Gamma_x$};;
\end{tikzpicture}
\end{minipage}

\noindent This motivates the following definition.
\begin{definition}[{\bf Hitting Set Generator}]
Let ${\cal{C}}$ be a collection of subsets over a finite universe $U$. For ${\cal{C}}$, a $(1-\epsilon)$-hitting set generator is a deterministic algorithm which on input $r$, the set $H_r \subseteq \{0,1\}^r$ which is a $(1-\epsilon)$-hitting set for ${\cal{C}}$.
\end{definition}

If we consider ${\cal{C}}$ to be the set of possible $\Gamma_x \subseteq \{0,1\}^r$ for every $x$ of length $n$ and for every algorithm ${\cal{A}}$ which runs in polynomial time. From the definitions itself the following theorem follows.
\begin{theorem}
If there is a $\half$-hitting set generator that runs in time $\poly(n)$ for the above set system, then any one-sided error randomized algorithm that has success probability at least $\half$ has an equivalent deterministic polynomial time algorithm.
\end{theorem}

\begin{proof}
The deterministic algorithm is as follows:
\begin{algorithm}
\label{alg:hittingset-algo}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State Using the generator, compute $H_r = \{y_1, y_2, \ldots y_\ell\}$ \Comment{Runs in $\poly(n)$, hence $\ell=\poly(n)$}
\For{\texttt{each $i \in [\ell]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, {\sc Accept}.
\EndFor
\State {\sc Reject}.
\end{algorithmic}
\end{algorithm}

Indeed, if the answer if {\sc No}, then $\Gamma_x=0$, so the algorithm reaches step 5 and rejects. If the answer is {\sc Yes}, then $|\Gamma_x| \ge \half$ and since $H_r$ is a $\half$-hitting set, we have that $H_r \cap \Gamma_x \ne \phi$. Let us say $y_i \in \Gamma_x \cap H_r$. Hence, ${\cal{A}}(x,y_i)$ accepts. Thus, ${\cal{A}'}(x)$ accepts. The algorithm is deterministic and runs in polynial time.
\end{proof}

\section{Connections between various objects}

In the past few lectures, we informally introduced several mathematical objects in relation to our main task - derandomizing randomized algorithms. We also stated that they all have the following common feature.
{\em By non-constructive arugments, we can prove that these objects exists for the range of paramters we want, but the question that remains is whether they can be constructed explicitly or not.}

It turns out that there are lots of interconnections between these objects (although sometimes for weaker set of parameters). The remaining part of the course also will demonstrate these connections, in addition to detailing out the applications of these objects in connection to the derandomization task.\\

\begin{tikzpicture}
[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\draw (2,3) node (hs) { Hitting Set Generators};
\draw (2,1) node (prg) {Pseudorandom Generators};
\draw (8,1) node (ext){Randomness Extractors};
\draw (2,-2) node (code){Linear Codes};
\draw (8,-2) node (exp){Expander Graphs};
\draw (14,-2) node (disp){Dispersers};
\draw[<->,line width=1mm](code.east)--(exp.west);
\draw[->,line width=1mm](prg.east)--(ext.west);
\draw[<->,line width=1mm]([yshift=2mm,xshift=8mm]code.north)--([yshift=-3mm,xshift=-3mm]ext.south);
\draw[<->,line width=1mm]([yshift=-3mm,xshift=3mm]ext.south)--([yshift=2mm,xshift=-8mm]disp.north);
\draw[->,line width=1mm](exp.east)--(disp.west);
\draw[->,line width=1mm](exp.north)--([yshift=-3mm]ext.south);
\draw[<->,line width=1mm](hs.south)--(prg.north);
\end{tikzpicture}

\vspace{5mm}
In the next lecture, we will first introduce the tools to show non-constructive existence and demonstrate them with various examples.
