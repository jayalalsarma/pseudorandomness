\Week{3}{Amplification : Making Algorithms Err Less}
\noindent 

In the last week, we saw algorithm specific techniques of derandomization. More precisely, the derandomization uses the peculiarity of the algorithms in the way they use the random bits and the analysis. In this week, we will go back to the abstract set up where we know nothing about the algorithm other than the resource bounds it uses.

We recall some notations first. A randomized algorithm ${\cal{A}}$ on input $x$ runs in time $t(n)$ (where $n=|x|$) and let $y \in \{0,1\}^{m(n)}$ be the concatenation of the unbiased coin toss experiement that the algorithm does during its execution. Notice that $m(n) \le t(n)$ (we drop the $n$ when it is not required explicitly). If the algorithm runs in polynomial time $t(n) \le n^c$ for a constant $c$ independent of $n$. \\

\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^m$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.5\linewidth}
\vspace{-7mm}
The guarantee we have is there is an $\epsilon \in (0,\half]$.
\vspace{-3mm}
$$\forall x \in \{0,1\}^n,~\Pr_{y \in \{0,1\}^r} [A(x,y) \textrm{ is correct.}] \ge \half+\epsilon $$
\end{minipage}
\vspace{3mm}

\noindent Imagine that we had a success probability of very close to 1. That is, $\epsilon > \half-\frac{1}{2^{m}}$. That is, $\forall x \in \{0,1\}^n$ : 
$\Pr_{y \in \{0,1\}^m} [A(x,y) \textrm{ is correct.}] > 1-\frac{1}{2^m}$. Notice that, now the algorithm does not need to use randomness and can fix $y$ to be any string in $\{0,1\}^r$ and run the algorithm ${\cal{A}}$ and the answer is guaranteed to be correct. (This is because, if there exists at least one $y \in \{0,1\}^m$ for which the algorithm errs then the success probability would have been $\le 1-\frac{1}{2^m}$.) Thus we would have derandomized the algortithm efficiently.

But how do we achieve such high success probability? Viewing a randomized algorithm as an experiment that we do in physics lab to compute a value, we will repeat the experiment and take the most frequent value in order to reduce the error. However, this also increases the number of random bits which goes against the above plan. Let us formally review this amplifcation method nevertheless.

\section{Success Probability Amplifcation by Repetition}

We first write down the algorithm which follows the simple idea of repetition with independent random bits.

\begin{algorithm}
\label{alg:trivial-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State {\em count} $\gets 0$.
\State Choose $k$ independent random strings $y_1, y_2, \ldots y_k \in \{0,1\}^r$. \Comment{Uses $O(kn)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.
\end{algorithmic}
\end{algorithm}

Why would this improve the success probability? and if so, how does it depend on $k$?. The following lemma answers these. Fix the input $x$. Let ${\cal{E}}$ represent the event that ${\cal{A}}$ accept on the random string $y$.

\begin{lemma}
If $\mathcal{E}$ is an event that $Pr(\mathcal{E}) \geq \frac{1}{2} + \epsilon $, then the probability the $\mathcal{E}$ occurs atleast $\frac {k}{2}$ times on $k$ independent trials is at least 
$1-\frac{1}{2}(1-4\epsilon^2)^\frac{k}{2}$
\end{lemma}
\begin{proof}
Let $q$ denote the probability the $\mathcal{E}$ occurs atleast $\frac {k}{2}$ times on $k$ independent trials.
Let $q_i$ = Pr($\mathcal{E}$ occurs exatly $i$ times in $k$ trials), $0 \leq i \leq k$. Thus,
$q = 1 - \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}$ $q_i$. We will analyse the complementary event:
Pr($\mathcal{E}$ occurs atmost $\frac{k}{2}$ times) = $\sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}$ $q_i$. \\ 
We show an upper bound on each $q_i$ and thus show an lower bound on $q$.
\begin{eqnarray*}
q_i & = & {k \choose i} (\frac{1}{2} + \epsilon)^{i} (\frac{1}{2} - \epsilon)^ {k-i} \\
& \leq & {k\choose i} \left(\frac{1}{2} + \epsilon\right)^{i} \left(\frac{1}{2} - \epsilon\right)^ {k-i} \left(\frac{\frac{1}{2} + \epsilon}{\frac{1}{2} - \epsilon}\right)^{\frac{k}{2} - i}  (\textrm{because }\epsilon \le \frac{1}{2} ) \\
& = & {k\choose i}\left(\frac{1}{2} + \epsilon\right)^{\frac{k}{2}}\left(\frac{1}{2} - \epsilon\right)^{\frac{k}{2}} \\
&= & {k\choose i} \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}}
\end{eqnarray*}
Now we analyse the sum:
\begin{eqnarray*}
\sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}q_i & \leq & \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}{k\choose i} \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}} \\
q = 1 - \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}q_i & \geq & \sum_{i=0}^{\lfloor\frac{k}{2}\rfloor}{k\choose i} \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}} \\
& = & 1 - \left(\frac{1}{4} - \epsilon^2\right)^{\frac{k}{2}} 2^{k-1} \\
& = & 1 - \frac{1}{2} \left(1 - 4\epsilon^2\right)^{\frac{k}{2}} \\
\textrm {Thus, } q & \ge & 1 - \frac{1}{2} \left(1 - 4\epsilon^2\right)^{\frac{k}{2}}
\end{eqnarray*}
\end{proof}

\noindent Thus, if we had an algorithm ${\cal{A}}$ with $\epsilon = \frac{1}{3}$ (that is success probability is at least $\frac{2}{3}$, and we want an algorithm with ${\cal{A'}}$ with $\epsilon = \frac{1}{4}$ (that is, success probability is at least $\frac{3}{4}$). Then, the number of times the iteration that needs to be done can be back calculated as the $k$ that satisfies:
$$1-\half\left(1-\frac{4}{9}\right)^{\frac{k}{2}} \ge \frac{3}{4}$$
which will be a constant. Quite interestingly, we can do this even when the required error probability is exponentially small. That is, suppose we require the success probability to be $1-\frac{1}{2^{q(n)}}$ which is quite close to 1. Then the value of $k$ should be :

$$1-\half\left(1-\frac{4}{9}\right)^{\frac{k}{2}} \ge 1-\frac{1}{2^{q(n)}} \Longrightarrow
\left(\frac{9}{5}\right)^{\frac{k}{2}}\le 2^{q(n)-1}$$
which in turn would imply $k = p(n)$ to be a polynomial value in terms of $n$.
Thus we have the following lemma:

\begin{lemma}[{\bf Amplification Lemma}]
\label{lem:amplification}
Let ${\cal{A}}$ be a randomized algorithm running in time $\poly(n)$ which has $\half+\epsilon$ as the probability of success. Then for any $q(n)$, we have a randomized algorithm ${\cal{A'}}$ that runs in time $\poly(n)$ with the following success probability. For every input $x \in \{0,1\}^{n}$.
\[ \Pr_{y} \left[ {\cal{A'}}(x,y) \text{ is {\em correct} } \right] \geq 1-2^{-q(n)} \]
\end{lemma}

Define $\Gamma_x = \{ y \in \{0,1\}^{p(n)}\mid {\cal{A}}(x,y) \textrm{ is correct }\}$.\\[-2mm]

\begin{minipage}{0.6\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A'}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^{p(n)}$};
\end{tikzpicture}

\vspace{2mm}
The guarantee we have is : 
$\forall x,~|\Gamma_x| \geq (1-2^{-q(n)})2^{p(n)}$
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.3\linewidth}
\vspace{-1cm}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\draw (2,0) ellipse (2cm and 2.5cm) node[midway,yshift=-1cm, xshift=1cm]{$\{0,1\}^{p(n)}$};
\draw (2,1) ellipse (1cm and 1cm)
node{$\Gamma_x$};;
\end{tikzpicture}
\end{minipage}

%
%\begin{exercise-prob}[See Problem Set 1(Problem~\ref{amplification-one-sided})]
%\begin{show-ps1}{amplification-one-sided}
%We saw in class the amplification of success probability by repetition. The analysis involved binomial coefficients simple manipulations involving them. The algorithm and the analysis gets much simpler when the algorithm given is one-sided error. Suppose we are given a randomized algorithm which is one-sided error with error probability, at most $\epsilon \in (0,1]$, modify the algorithm to say {\sc Yes} if at least one of the trials says {\sc Yes}. Derive a relation between $\epsilon$, $k$ (the number of repetitions) and the target bound of error probability (say $\epsilon'$).
%\end{show-ps1}
%\end{exercise-prob}

\begin{exercise-prob}[See Problem Set 1(Problem~\ref{amplification-by-tailbound})]
\begin{show-ps1}{amplification-by-tailbound}
A decision problem $L$ is in a class called $\BPP$ if there exists a randomized polynomial-time algorithm $A$ such that for every $x \in L$ it holds that $\Pr[A(x,y) = 1] \ge \frac{2}{3}$, and for every $x \notin L$ it holds that $\Pr[A(x) = 0] \ge \frac{2}{3}$. For $\epsilon : \mathbb{N} \to [0,1]$, let $\BPP_{\epsilon}$ denote the class of decision problems that can be solved in probabilistic polynomial time with error probability upper-bounded by $\epsilon$. Prove the following two claims:
\begin{enumerate}[(a)]
\item 
For every positive polynomial $p$ and $\epsilon(n) = \frac{1}{2}-\frac{1}{p(n)}$, 
the class $\BPP_{\epsilon}$ equals $\BPP$.
\item For every positive polynomial $p$ and $\epsilon(n) = 2^{-p(n)}$, 
the class $\BPP_\epsilon$ equals $\BPP$.
\end{enumerate}
We already proved something similar in class (See Amplication Lemma). This exercise asks you to prove the same using tail bounds. Given an algorithm $A$, consider an algorithm $A'$ that on input $x$ invokes $A$ on $x$ for $t(|x|)$ times, and decided based on majority as we did in class. For Part (a) $t(n) = O(p(n)^2)$ and apply Chebyshev’s Inequality. For Part 2 set $t(n) = O(p(n))$ and apply the Chernoff Bound.
\end{show-ps1}
\end{exercise-prob}


\section{Sipser's Argument}

From the previous section, we concluded that, for every $x \in \{0,1\}^{p(n)}$, there is a large set $\Gamma_x$ of random strings which are "good" for $x$. However, we do not know how to find even an element $y \in \Gamma_x$ in time $\poly(n)$. If we do, then we have a derandomization for the algorithm ${\cal{A'}}$ (which derandomizes ${\cal{A}}$ too).

It is intuitive to think that since a large fraction of $y \in \{0,1\}^{p(n)}$ are good for each $x \in \{0,1\}^n$, could there be a single $y$ which is good for all $x$? We show that this is indeed the case. This, in fact, is a simple consequence of the amplification lemma in the previous section.

We first apply Lemma~$\ref{lem:amplification}$ with $q(n) = 2n$. Thus we have a randomized polynomial time algorithm with error bound $2^{-2n}$ (i.e. at most $2^{-2n}$ fraction of random strings are ``bad'') using $p(n)$ randomness. More precisely, we have that:
\begin{align}
  \textrm{Answer is {\sc Yes} for input $x$} \implies \Pr_{y} \left[ {\cal{A}}(x,y) \text{ accepts } \right] & \geq
   1-2^{-2n} \\
  \textrm{Answer is {\sc No} for input $x$} \implies \Pr_{y}\left[ {\cal{A}}(x,y) \text{ accepts } \right] & \geq
   2^{-2n} 
\end{align}
That is in such a machine the number of random strings $y$ which lead
the machine to output a wrong answer is bounded by $2^{-2n}$. Now let
us consider a matrix $M$ whose rows are indexed by inputs of length
$n$ and columns are indexed by random strings of length $p(n)$, and
the $(x,y)$th entry is $1$ if on fixing the random bits to be $y$ the
machine $M$ on input $x$ outputs correctly and it is $0$
otherwise. That is $M[x,y]=1$ if and only if ${\cal{A'}}(x,y)$ is correct for the input $x$. By the amplification we
are guaranteed that for a given input $x$ at most $2^{-2n}$ fraction
of the random strings can have $M[x,y]=0$. Hence the total number of
zeros in the $A$ matrix is at most the number of rows times the
maximum number of zeros in a row, which is equal to 
\begin{eqnarray*}
  \text{\# 0's in matrix }M & \leq 2^n \times 2^{-2n} \times 2^{p(n)}
  \\
                            & \leq 2^{p(n)-n}
\end{eqnarray*}

But the total number of zeros, $2^{p(n)-n}$ is strictly less than the
number of columns in the matrix $M$. Hence there must be at least one
column with no zeros in it. If a column in the $M$ matrix has no zeros
then by the definition of $M$ matrix, the random string $w$ represented by
this column when fed as random bits to machine ${\cal{A}'}(x,w)$ would output
the correct answer for every $x\in \{0,1\}^{n}$. Thus we have the follwing lemma.

\begin{lemma}
For every $n$, there is a $y \in \{0,1\}^{p(n)}$ such that $y \in \Gamma_x$ for every $x \in \{0,1\}^n$.
\end{lemma}

Thus there is a function $h : \mathbb{N} \to \Sigma^*$ such that the answer to $x$ is {\sc Yes} if we run the algorithm $\calA$ on input $x$ and random string $h(|x|)$ it is guaranteed not to err. This will give a complete deranomization. However, we need the function $h$ to be computable in polynomial time and that is a challege.

\section{Amplification with Dependent Trials using Expanders}
\label{sec:intro-expanders}

We study another seemingly unrelated graph theoretic object which are called expanders. They have been extensively studied and used in many areas of science and engineering as the set of sparse graph families which has high connectivity connectivity properties. These are particularly desirable in network design such that the network is highly fault tolerant - even the failure of a few links (edges) will not affect the connectivity of the network. The fact that this can be achieved without having too many edges (complete graph is a trivial way to do this) is what makes expander graphs more applicable in this setting.

Expanders have been studied in the theoretical side as well for past few decades and have found numerous applications. Informally, the graph is such that every subset of vertices fetches a large set of vertices in its immediate neighborhood. For a set of vertices in a graph $G$, define $N(S)$ to be the neighbors of $V$. We define the graph class formally now.

\begin{definition}{(\bf Expander Graphs)}
A graph $G(V,E)$ is said to be a $(\alpha,\beta)$-expander if every $S \subseteq V$ such that $|S| \le \alpha|V|$, the number of new neighbors $|N(S)\setminus S| \ge \beta|S|$.
\end{definition}

A trivial (but unfortunately useless) example of an expander graph is the complete graph. Indeed, in a complete graph on $n$ vertices, if we choose any $S \subseteq V$, such that $|S| \le \frac{n}{2}$, we have that $N(S)\setminus S = V \setminus S$. Hence, $|N(S)\setminus S| \ge |S|$. This gives that it is a $(\half,1)$-expander as per the above definition. However, it is going to be useless as we will see in our application. In fact, ideally, we would like expander graphs where the degree $d$ and $\alpha, \beta$ are all constants independent of $n$.

Thus, we are looking for graphs with constant degree (if possible, even regular). It implies that the graph must be sparse\footnote{a graph is said to be sparse if $|E| = O(|V|)$, or else it is called dense.}
Let us also record a non-example of an expander, which is even a sparse graph. Consider an $n \times n$ complete grid graph, which has $n^2$ vertices and all possible {\em grid edges} present. Consider a set $S$ which is of size $\sqrt{n}$, which has $n$ vertices and hence $|S| \le \alpha |V|$ for any constant $\alpha$. However, the number of new neighbors for the set in the grid graph will be $O(\sqrt{n})$ which is not at most $\beta|S|$ for any constant $\beta$. Hence such a graph is not an expander for any constant $\alpha$ and $\beta$.

To ensure that the definition of expander is practiced enough, we suggest the following exericse which derives a combinatorial consequence of the expansion property.

\begin{exercise-prob}[See Problem Set 1(Problem~\ref{expander-diameter})]
\begin{show-ps1}{expander-diameter}
Let $G(V,E)$ be an undirected graph $n$ vertices which is $(\frac{1}{2},2)$-expander. Show that the diameter of the graph is at most $O(\log n)$. The diameter of a graph is at most $k$ if and only if between any two vertices in the graph $G$, there is a path of length at most $k$ in the graph $G$.
\end{show-ps1}
\end{exercise-prob}


\paragraph{Random Walks on Expanders and Our Application:} Although the above definition explains the name expander graphs, the application in our context comes from the fact that if we choose a vertex in the expander at random and then choose the next vertices uniformly at random, the distribution of the $\ell^{th}$ vertex that you get to (as $\ell$ increases) is very close to uniform over the entire connected component of the graph where your starting vertex was lying. Intuitively, this is termed as the {\em rapid mixing} of random walks on expander walks.

We will prove the technical details of this idea at a later point in the course where we introduce expander graphs. Informally, to apply the expander graphs in the context of the success probability amplification, we consider the graph on $G$ as a graph on $2^{p(n)}$ vertices indexed by $\{0,1\}^{p(n)}$. That is, each vertex in the graph can be interpreted as a $y \in \{0,1\}^{p(n)}$. Based on this, the following algorithm gives a good amplification of success probability.


\begin{algorithm}
\label{alg:trivial-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State {\em count} $\gets 0$. 
\State Let $G(V,E)$ be an expander graph on $2^{p(n)}$ vertices.
\State Choose a vertex $y_1 \in V$ uniformly at random. \Comment{Uses $O(p(n))$ random bits}
\State Starting at $v$ perform a random walk for $k$ steps in $G$. Let $y_1, y_2, \ldots y_k \in \{0,1\}^r$ be the vertices representing the walk. \Comment{Uses $O(k \log d)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.
\end{algorithmic}
\end{algorithm}

The idea of the above algorithm is based on the rapid mixing of random walks. Without the details, the strings $y_1, y_2, \ldots y_k$ are ``almost" as good as randomly chosen $y_i$'s since the random walk mixes fast and hence the amplification (although with weaker parameters) can be derived. This will be done in upcoming lectures.

Notice that the number of random bits used by the algorithm is $O(k \log d)$. This explains why the complete graph (whose degree in this case would have been $2^{p(n)}-1$) would not have been good enough for our purpose.

