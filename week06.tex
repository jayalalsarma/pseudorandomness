\Week{6}{Expander Graphs : Existence and Algebraic Constructions}

We now use the tools that we have developed in order to construct show existance and then explict constructions of expanders. We recall the definition of expanders from Section~\ref{sec:intro-expanders} but now we make the paramters more precise.

\begin{definition}{(\bf Expander Graphs)}
A graph $G(V,E)$ is said to be a $(n,d,\alpha,\beta)$-expander if, $|V|=n$, it is $d$-regular, and every $S \subseteq V$ such that $|S| \le \alpha|V|$, the number of new neighbors $|N(S)\setminus S| \ge \beta d|S|$.
\end{definition}

\noindent {\bf Note :} We are changing the notation slightly from what we followed in the lecture to ensure that both $\alpha$ and $\beta$ are in $(0,1)$.

\noindent A weaker setting is to consider bipartite graphs where we insist the above only for subset of vertices from one side of the bipartite graph, so that $|N(S)\setminus S| = |N(S)|$ since there is no edge from a set to itself in this case.

\begin{definition}{(\bf Bipartite Expander Graphs)}
A graph $G(U \cup V,E)$ is said to be a $(n,d,\alpha,\beta)$-expander if, $|U|=n$, it is $d$-regular and every $S \subseteq U$ such that $|S| \le \alpha|U|$, the number of new neighbors $|N(S)| \ge \beta d|S|$.
\end{definition}
\section{Existence of Left regular Bipartite expanders}

What kind of parameters should we expect for expanders and what do we need. Looking at the possible application in the case of randomness efficient amplification, we would like the degree of the graph to be much much smaller than the number of vertices (ideally a constant). It is natural to keep the graph regular because then the mathematical constraints are easier to handle (as we will see). We can also allow the graph to have multiple edges between two vertices since we do not require the neighbors to be distinct.

Ideally, we would want $\beta$ to be as close to $1$ as possible. In fact, for many of the applications, it is enough to have $\beta > \half$. We will show the existance of a $(n,d,\alpha,\frac{d-2}{d})$ expander which will be good as long as $d \ge 5$. We show the following theorem (which not only shows existence but also implies abundance).

\begin{theorem}
For every constant $d$, there is an $\alpha > 0$ such that for all large enough $n$: a uniformly chosen random bipartite graph $G(U \cup V, E)$ which is left $d$-regular and $|U|=|V|=n$, is an $(n,d,\alpha,\frac{d-2}{d})$ with probability at least $\half$.
\end{theorem}
\begin{proof}
We spell out the details of the experiment first. Each vertex $u \in U$ is assigned $d$ neighbors from the right side (with replacement) chosen uniformly at random, each indepedently. 

We will compute probability that the outcome of the experiment is not an expander and show that this probability is less than $\half$.

\noindent The graph is not an expander with the required parameters if there exists a set $S \subseteq U$, $|S|\le \alpha n$ such that $|N(S)| < (d-2)|S|$. We will use union bound over all such $S$.
$$
\Pr\left[\begin{array}{l}
\exists S \subseteq U, \textrm{ with } |S|\le \alpha n \\
\textrm{ s.t. } |N(S)| < (d-2)|S|
\end{array}
\right] 
\le \bigsum_{\substack{S \subseteq U\\
|S|\le \alpha n}} Pr[|N(S)| < (d-2)|S|] 
\le \bigsum_{k=1}^{\alpha n} {n \choose k} p_k
$$
where $p_k$ is the upper bound we hope to derive for $Pr[|N(S)| < (d-2)|S|]$ for any $S \subseteq U$ with $|S|=k$.

Now, we just need to estimate $p_k$. First, we observe that the only way the number of neighbors of a vertex $u \in U$ can go below $d$ is when the random choice ends up choosing the same vertex $v \in V$ more than once in the experiment for $u$. To use this, let $M=\{\{v_1, v_2, \ldots v_{kd}\}\}$ be the multiset of $kd$ outcomes when $d$ neighbors are chosen uniformly at random for each of the vertices in $S$. If $|N(S)| < (d-2)k$, then it must be because there exists $2k$ repetitions in this sequence. The repetitions can be any of the $2k$ sub-multi-subsets out of these $kd$ neighbors. We apply union bound over all the subsets. Thus,
\begin{eqnarray*}
p_k = \Pr\left[\begin{array}{l}
\exists T \subseteq M, \textrm{ with } |T|=2k \\
\textrm{ s.t. all elements in $T$ are repeats}
\end{array}
\right] 
& = & \bigsum_{\substack{{T \subseteq M}\\|T|=2k}}
\Pr\left[\begin{array}{l}
\textrm{all elements in $T$}\\
\textrm{are repeats}
\end{array}
\right] \\
& \le & \bigsum_{\substack{{T \subseteq M}\\|T|=2k}} \left(\frac{kd}{k}\right)^{2k}
\le {kd \choose 2k}\left(\frac{kd}{n}\right)^{2k}
\end{eqnarray*}
The last inequality is by noting that the probability of any particular element repeating is at most $\frac{kd}{n}$ (the worst case is for the last element in $T$ chosen). Hence,
\begin{eqnarray*}
\Pr\left[\begin{array}{l}
\exists S \subseteq U, \textrm{ with } |S|\le \alpha n \\
\textrm{ s.t. } |N(S)| < (d-2)|S|
\end{array}
\right] 
&\le &\bigsum_{k=1}^{\alpha n} {n \choose k}  {kd \choose 2k}\left(\frac{kd}{n}\right)^{2k} \\
& \le & \bigsum_{k=1}^{\alpha n} \left(\frac{ne}{k}\right)^k \left(\frac{kde}{2k}\right)^{2k} \left(\frac{kd}{n}\right)^{2k}
\textrm{~~~~~~~~using ${n \choose k} \le \left(\frac{ne}{k}\right)^k$} \\
& \le & \bigsum_{k=1}^{\alpha n} \left(\frac{e^3d^4k}{4n}\right)^k \le \bigsum_{k=1}^{\alpha n} 4^{-k} \textrm{~~~~~~~~~~~choose $\alpha = \frac{1}{e^3d^4}$} \\
& < & \half
\end{eqnarray*}
Hence the proof.
\end{proof}

\section{Variants of Expanders}
The discussion in the beginning of the previous section motivates the definition of variants of expander definition. What we defined in the previous section is called the boundary expansion where we insisted that $|N(S)\setminus S|$ must be large. We define two variants of the same.
\begin{definition}[{\bf Vertex Exapansion}]
A graph $G(V,E)$ is said to be a $(n,d,\alpha,\beta)$-expander if, $|V|=n$, it is $d$-regular, and every $S \subseteq V$ such that $|S| \le \alpha|V|$, the number of new neighbors $$|N(S)| \ge \beta d|S|$$
\end{definition}
Notice that when we consider bipartite graphs $N(S) \cap S \ne \phi$, but still the above definition is stronger since it asks for all $S \subseteq V$ and not the subsets only in one side.
\begin{definition}[{\bf Edge Exapansion}]
A graph $G(V,E)$ is said to be a $(n,d,\alpha,\beta)$-expander if, $|V|=n$, it is $d$-regular, and every $S \subseteq V$ such that $|S| \le \alpha|V|$, the number of new neighbors
\begin{equation}
|E(S,\overline{S})| \ge \beta d|S|
\label{eqn:edge-expansion}
\end{equation}
\end{definition}

Viewing it from the graphs side, we can define the edge expansion ratio of a $d$-regular graph $G$ as,

$$h(G) = \min_{\{S \subseteq [n] :  
|S| \le \frac{n}{2}\}}\frac{|E(S,\overline{S})|}{d|S|}$$

In other words, for a given graph $G$, what is the smallest $\beta$ for which it satisfies Equation~\ref{eqn:edge-expansion}. The answer is the edge expansion ratio $h(G)$.

\begin{exercise}
Let $n,d \in \mathbb{N}$, $\alpha, \beta > 0$.
Every $(n,d,\alpha,\beta)$-boundary-expander is also a $(n,d,\alpha,\frac{\beta}{d})$-edge-expander. Conversely, every $(n,d,\alpha,\beta)$-edge-expander is also a $(n,d,\alpha,\beta)$-boundary-expander.
\end{exercise}
\begin{exercise-prob}[See Problem Set 3~(Problem~\ref{expander-defn})]
\begin{show-ps3}{expander-defn}
Let $G=(V,E)$ be a graph. For every subset $S$ of its vertices $V$ we say, that vertex $v \in V\setminus S$ is a neighbor of $S$ if $E(S,{v}) \ge 1$, an odd neighbor of $S$ if $E(S,{v})$ is odd, a unique neighbor of $S$ if $E(S,{v})=1$. Further, we denote:
\begin{eqnarray*}
B(S) & = & \{v \in V \setminus S \mid v \textrm{ is a neighbor of } S\}\\
B_{odd}(S) & = & \{v \in V \setminus S  \mid v \textrm{ is an odd neighbor of } S\}\\
B_{unique}(S) & = & \{v \in V \setminus S \mid v \textrm{ is a unique neighbor of } S\}
\end{eqnarray*}
A graph $G$ is said to be an $(n,d,\alpha,\beta)$-odd-neibhor (resp. unique-neighbor) expandor if for every $S \subseteq V$, where $|S| \le \alpha n$, the set $B_{odd}$ (resp. $B_{unique}$)  is of size at least $\beta d |S|$.
\begin{enumerate}[(a)]
\item Show that every $(n,d,\alpha,\beta)$ unique-neighbor expander is an $(n,d,\alpha,\beta)$ odd-neighbor expander.
\item Show that every $(n,d,\alpha,\beta)$ odd-neighbor expander is also an $(n,d,\alpha,\beta)$ boundary expander.
\item Show that every $(n,d,\alpha,\frac{1}{2}+\frac{\epsilon}{d})$-boundary expander is also a $(n,d,\alpha,\frac{2\epsilon}{d})$-unique-neighbor expander.
\end{enumerate}
\end{show-ps3}
\end{exercise-prob}

It is usually assumed that $\alpha = \half$. In fact, if we have an edge expander with $\alpha = \half$, then we have one for larger $\alpha$ as well. The following exercise will ascertain that.

\begin{exercise-prob}[See Problem Set 3~(Problem~\ref{expander-alpha})]
\begin{show-ps3}{expander-alpha}
Let $n \in \mathbb{N}$ and $\half < \alpha < \frac{n-1}{n}$, and $G(V,E)$ by an $(n,d,\half,\beta)$ edge-expander, then $G$ is also an $(n,d,\alpha,(1-\alpha)\beta)$ edge expander.
\end{show-ps3}
\end{exercise-prob}

We showed in the last section that, bipartite expanders exist with good parameters. Now we will show edge-expanders exists using the expectation method. The following problem is designed for that.

\begin{exercise-prob}[See Problem Set 3~(Problem~\ref{expander-exists})]
\begin{show-ps3}{expander-exists}
Through the following steps, show that there exists a family of $(n,d,\half,\beta)$ edge-expander.
\begin{enumerate}[(a)]
\item Choose $\beta$ later. Choose a random $d$-regular graph on $n$ vertices. Let $S \subseteq V$, with $s=|S| \le \frac{n}{2}$. Define the random variable $E_S = |E(S,V\setminus S)|$. Let $0 \le k \le \beta d |S|$. Prove that if $sd-k$ is an odd number, then, $\Pr[E_S = k] = 0$.
\item If $sd-k$ is even, then show that 
$$\Pr[E_S=k] \le \left(\frac{3s}{2n}\right)^{ds/4}
\textrm{ \hspace{1cm} Use ${a \choose b} \left( \frac{ae}{b}\right)^b$ and choose $\beta$ such that $\left(\frac{e}{\beta}\right)^{4\beta} \le \frac{9}{8}$} 
$$
(\textit{Hint: Suppose $sd-k$ is even, to calculate probability of $k$ edges leaving the set - select which edges are those $k$, matching them with any of the $nd-sd$ possible endpoints outside of the set $S$. Every edge with one endpoint in the subset $S$ has probability roughly $s/n$ that the other endpoint is contained in $S$ as well.}).
\item Use the previous part to show that :
$$ \Pr\left[E_S < \beta ds\right] \le \left(\frac{5s}{3n}\right)^{ds/4} \textrm{ \hspace{1cm} Assume $d \ge 140$ and approximate : $\frac{sd}{4} \le \left(\frac{10}{9}\right)^{sd/4}$}$$
\item Prove that the probability that there exists a set of size at most $\frac{n}{2}$, is $<1$. Hence conclude the theorem.
\end{enumerate}
\end{show-ps3}
\end{exercise-prob}

\section{Marguilis-Gabber-Galil Expander}

Although we showed the existence arguments only for left-regular bipartite expanders, similar arguments exists for other kinds of objects in the above lists too. Now we turn into explicit constructions of families of expander graphs.

There are two major approaches to constructing expander graphs. One regime is purely algebraic, in which the expander graphs are defined over algebraic structures and connectivity is determined by algebraic properties. They are very easy to describe (more formally, they are very explicit). However, the arguments that they are indeed expanders is more complicated (in fact, even beyond the scope of this course, in terms of background required). Hence we will not be proving that they are expanders, but we will describe them. 

The other regime of constructions is more combinatorial and are based on graph operations. It takes care to make them explicit, but the proof of expansion is somewhat more amenable for the background of this course.

\begin{description}
\item{\bf Marguilis-Gabber-Galil Expander:}
First family was studied by \cite{Mar73} where the proof of expansion was based on representation theory and did not provide any specific bound on the expansion ratio $h$. Later \cite{GG81} derived such a bound
using harmonic analysis.

We now describe the family which is on $n^2$ vertices 
is $V = \Z_n \times \Z_n$. The neighbors of the vertex $(a,b) \in V$ are :
$$N(a,b) = \left\{\begin{array}{l}
(a+b,b)\\
(a-b,b)\\
(a,b+a)\\
(a,b-a)\\
(a,b+a+1)\\
(a,b-a+1)\\
(a+b+1,b)\\
(a-b+1,b)
\end{array}
\right\}
$$
\item{\bf Expanders from Number Theory:}
These graphs can be constructed when the number of vertices expected is a prime. The family of $3$-regular contains $p$-vertex graphs for every prime $p$. Here $V = \Z_p$, and for a vertex $a \in V$: 
$$N(a) = \{a+1,a-1,a^{-1}\}$$
where all operations are in $\Z_p$.
\end{description}

Although we will not describe the proof of expansion of the above graphs, they all go through a special connection between edge expansion and the spectrum of a graph. We quickly introduce this in the next section.

\section{Spectral Expansion}
\label{sec:spectral-expansion}

We consider simple graphs for simplicity. For a $d$-regular undirected graph, define the normalized adjacency matrix as follows:
$$
A_{uv} = \left\{ \begin{array}{ll}
\frac{1}{d} & (u,v) \in E \\
0 & \textrm{ otherwise }
\end{array}
\right.
$$
Since $G$ is undrected, the matrix is symmetric and hence the eigen values of the matrix are all real numbers\footnote{We reviewed the basic related definitions about eigen values in the lecture, but we are not writing them here to avoid digression.}. Since the graph is $d$-regular, the matrix is also doubly stochastic. Hence the all $1$ vector (and any scalar multiple of it) is an eigen vector and $1$ is the eigen value corresponding to it. The largest eigen value of a doubly stochastic matrix is $1$ (Prove this as an exercise !). Notice that there are at most $n$ eigen values and some of them could be with higher multiplicity.

All eigen values of the matrix are in the interval $[-1,1]$. Folding this range, by taking absolute values, 
let the absolute values of the eigen values of the normalized adjacency matrix\footnote{We denote them as $\lambda_i(G)$.} $1=\lambda_1 \ge \lambda_2 \ge \lambda_3 \ldots \lambda_k \ge 0$ where $k \le n$. This sequence is called the {\em spectrum of the graph}. Spectrum of a graph contains surprising information about the combinatorial structure of the graph. Spectral graph theory studies the relationship between spectrum and combinatorial properties of the graph (for example, connectedness, bipartiteness, number of connected components).

The gap between the first two eigen values is called the \textbf{spectral gap} of a graph. And a $d$-regular graph is called a spectral expander if the second largest eigen value $\lambda_2(G)$ is below $1$ by a constant gap. More formally:

\begin{definition}[\textbf{Spectral Expanders}]
A $d$-regular graph is said to be $(n,d,\lambda)$ spectral expander if $\lambda_2(G) \le \lambda$. Equivalently, the spectral gap of the graph is at least $1-\lambda$.
\end{definition}

There are two questions that we will address now. First of all, why study spectral expansion? We show that spectral expanders in fact have edge expansion too. Thus to show that a graph is an edge expander, it suffices to show that it is an spectral expander. This is the approach taken for proving that the algebraic constructions the we described in the previous section are expanders. However, the way to achieve that is still through harmonic anlaysis and number theory etc.

The second question is to do the curious statement that spectral expanders are in fact edge expanders too. We try to make this precise through the following statement.

\begin{theorem}[\textbf{Spectral Expansion Implies Edge Expansion}]
If $G$ is an $(n,d,\lambda)$ spectral expander, then it is also a $(n,d,\half,\frac{1-\lambda}{2})$ edge expander.
\end{theorem}

In fact, the converse of the above theorem is also true. We show the formal proof of the above statement in the next lecture (in fact, a much stronger form too) with the converse.

In the rest of this lecture, we make an informal handwavy attempt to justify why one should expect such a connection between spectral expansion and edge expansion. The argument is through the idea of random walks on graphs. A random walk on a graph $G$ is a random experiment performed on the $G$ as follows. Starting from a vertex $v$ and choosing the next neighbor among the neighbors of the current vertex. The walk produces a sequence of vertices. The relevant question is - fix an integer $\ell \in \N$ - {\em what does the distribution of the $\ell^{th}$ vertex in the walk look like?} Of course, the answer depends on the graph. Then the question is what parameter of the graph governs the distribution of the vertex after $\ell$ steps of the walk? Are there graphs for which it will be the uniform distribution or even close to uniform distribution.

If the distribution gets closer and closer to uniform - then it is termed as {rapid mixing of the random walk}. The rate at which it gets closer is a paramter. If there was no edge expansion, then we would have had random walks initiating from the set $S$ (which did not have edge expansion) to not get distributed uniform easily. Hence we would expect random walks to not mix fast. In the contrapositive, this says that, intuitively, \textit{if random walks in a graph mixes well, then there must be edge expansion in the graph}.

Now we need to address the question, why does spectral gap imply that random walks in the graph mix well? This has a very cute linear algebraic reason. To answer this at least informally, we take an example graph. To keep track of the distribution that evolves with the length of the walk, we denote by $p_i \in \mathbb{R}^n$ the probability distribution vector after the $i$th step in the random walk.

Suppose we are starting from the topmost vertex. Let $X_i$ be the random variable denoting the vertex after the $i^{th}$ step. Define the vector $p_i$, with entries as $p_i[j] = \Pr[X_{i} = j]$.
\hspace{-6mm}\begin{minipage}{0.75\linewidth}
\begin{center}
\begin{tabular}{c|cccccccc}
& 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
$p_0$ & 1   & 0    & 0    & 0   & 0 & 0 & 0 & 0 \\
$p_1$ & 0   & 0.5  & 0.5  & 0   & 0 & 0 & 0 & 0 \\
$p_2$ & 0.5 & 0    & 0    & 0.5 & 0 & 0 & 0 & 0 \\
$p_3$ & 0   & 0.42 & 0.42 & 0   & 0.17 & 0 & 0 & 0 \\
$\vdots$ &   &  & $\vdots$ &    & & $\vdots$ & & \\
\end{tabular}
\end{center}
$$p_{i+1}[j] = \Pr[X_{i+1} = j] = \bigsum_k \Pr[X_{i} = k]\Pr[\textrm{ $(k,j)$ edge is chosen }] $$
This implies that $p_{i+1} = Ap_i$. 
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.15\linewidth}

\SetGraphUnit{1}
\GraphInit[vstyle=Classic]
\SetUpVertex[FillColor=blue!60]  
\tikzset{VertexStyle/.append style={minimum size=2pt,inner sep=1pt}}
\begin{tikzpicture}
  \Vertex[x=4,y=1,L=1]{A}
  \Vertex[x=4,y=-1,L=4]{B}
  \Vertex[x=5,y=0,L=3]{C}
  \Vertex[x=3,y=0,L=2]{D}      
  \Vertex[x=4,y=-2,L=5]{E}
  \Vertex[x=4,y=-4,L=8]{F}
  \Vertex[x=5,y=-3,L=7]{G}    
  \Vertex[x=3,y=-3,L=6]{H}
  \Edges(A,C,B,D,A)
  \Edges(E,G,F,H,E)
  \Edges(B,E)
\end{tikzpicture}
\end{minipage}

\vspace{3mm}
Now let $v_1, v_2, \ldots v_n$  be an orthonormal basis of $\mathbb{R}^n$ where each $v_i$ is an eigen vector of $\lambda_i$. By definition any vector can be written as a linear combinationof vectors in $\mathbb{R}^n$.
Let $p_i = \alpha_1v_1+\alpha_2v_2+\cdots+\alpha_nv_n$.
\begin{eqnarray*}
p_{i+1} = Ap_i & = & \alpha_1 Av_1 + \alpha_2Av_2+\cdots+\alpha_nAv_n \\
& = & \alpha_1v_1 + \alpha_2\lambda_2v_2+\cdots+\alpha_n\lambda_nv_n
\end{eqnarray*}
Notice that the component corresponding to the eigen value $1$, remains unchanged in the vector,and the other components gets multiplied by at least $\lambda_2$ and hence gets reduced. Thus the vector will go closer and closer to uniform distribution as the walk goes further and further. The rate of mixing (to uniform distribution) will depend on how small $\lambda_2(G)$ is. Thus, \textit{if there is a constant spectral gap, then the random walk mixes fast.} 

Together with the earlier statement - \textit{if random walks in a graph mixes well, then there must be edge expansion in the graph}, this implies that we should expect a connection between second largest eigen value and edge expansion ratio. This is indeed, the topic of discussion for next lecture.


\begin{curiousity}[\textbf{Gershgorin's Circle Theorem}]
This is a digression. We used the following statement (where we left the proof as an exercise).
\textit{The largest eigen value of a doubly stochastic matrix is $1$ }
For graphs without self-loops, this is a special case of a more general theorem called \textit{Gershgorin circle theorem} may be used to bound the spectrum of a square matrix. It was first published by the Gershgorin in 1931. The statement is as follows:

Let $A$ be an $n \times n$ matrix with entries from $\mathbb{C}$, 
with entries $a_{ij}$. For $i \in [n]$ let $R_{i}=\sum _{j\neq i}\left|a_{ij}\right|$ be the sum of the absolute values of the non-diagonal entries in the $i$-th row. Let $D(a_{ii},R_{i}) \subseteq \mathbb{C}$ be a closed disc centered at $a_{ii}$ with radius $R_{i}$. Such a disc is called the \textit{Gershgorin disc}.\\[-4mm]

\noindent \textbf{Gershgorin's Circle Theorem} : \textit{Every eigenvalue of $A$ lies within at least one of the Gershgorin discs.}\\[-4mm]

\noindent Indeed, in our case, for every $i$, $R_i = 1$, $a_{ii} = 0$. This immediately implies the statement we wanted to conclude.
For a diagonal matrix, the Gershgorin discs coincide with the spectrum.For a diagonal matrix, the Gershgorin discs coincide with the spectrum. Conversely, if the Gershgorin discs coincide with the spectrum, the matrix is diagonal.
\end{curiousity}

\section{Cheeger's Inequality}

We described the vague reason why we might expect that graphs with a constant spectral gap should be expected to be good edge expanders as well. Now, we make this precise.

Recall from the previous lecture about the edge expansion ratio.

$$h(G) = \min_{\{S \subseteq [n] :  
|S| \le \frac{n}{2}\}}\frac{|E(S,\overline{S})|}{d|S|}$$

Cheeger's inequality establishes a strong connection between the edge expansion ratio and the the second largest eigen value.

\begin{theorem}[{\bf Cheeger's Inequality}]
\label{thm:Cheeger}
For any undirected $d$-regular graph $G$:
$$\frac{1-\lambda_2(G)}{2} \le h(G) \le \sqrt{2(1-\lambda_2(G))}$$
\end{theorem}


The idea of the proof is to use a new parameter called {\em conductance} of the graph which is closely related to edge expansion ration and then it use it to prove the bound. We define conductance first.
\begin{equation}
\Phi(G) = \min_{\{\phi \subseteq S \subseteq [n]\}} \frac{|E(S,\overline{S}|}{d|S|\left(|\overline{S}|/|V|\right)}
\label{eqn:conductance}
\end{equation}

The way we will interpret $\Phi(G)$ is as follows. For any set $S$, if the neighbors of each vertex was chosen at random (total of $d|S|$ neighbors would have been chosen). The probability that each neighbor chosen in this way falls outside $S$, is $\left(|\overline{S}|/|V|\right)$. Hence the denominator is the expected number of crossing edges from $S$ for a random graph. Thus, the conductance, intutively, denotes how much ``random" the given graph is.

We first claim that conductance is related to edge expansion ratio.
\begin{claim}
\label{claim:hphi}
$$h(G) \le \Phi(G) \le 2h(G)$$
\end{claim}
\begin{proof}
Note that the role of $S$ and $\overline{S}$ are interchangable. Hence, without loss of generality, there is an $S$ for which $|S| \le \frac{n}{2}$ where the minimum is achieved in equation~\ref{eqn:conductance} and for that minimum, the factor $\frac{\overline{|S|}}{|V|}$ in the denominator is at least $\frac{1}{2}$ and at most $1$. Hence the minimum achieved for equation~\ref{eqn:conductance} can be at most $2h(G)$ and at least at $h(G)$.
\end{proof}

\begin{exercise}
Prove Claim~\ref{claim:hphi} for a general $\alpha$.
\end{exercise}

\section{Proof of Cheeger's Inequality: From Spectral to Combinatorial}
We now prove the LHS of the Cheeger's inequality ( (Theorem~\ref{thm:Cheeger})). Because of the above claim, it suffices to prove that $\Phi(G) \ge 1-\lambda_2(G)$. 

\noindent{\em Approach:} We prove this by a general idea which is useful in other context too and hence we present it in the general form, before applying it here. Consider a minimization question with an objective function $\psi(z_1, z_2, \ldots, z_n)$ in terms of variables $z_1, z_2, \ldots z_n \in \{0,1\}$. Let $k$ be the optimum value. Now suppose we optimize the same function without the constraint of the values being Boolean, that is $z_1, z_2, \ldots z_n \in \mathbb{R}$. Clearly the optimal value of $\psi$ can only be smaller. It is this simple trick that we will apply in the context of the above two parameters too. We will write an objective function for which $\Phi(G)$ is the solution when the variables are restricted to $\{0,1\}$ and $1-\lambda_2(G)$ is the solution when the variables are relaxed to be arbitrary real numbers.

\paragraph{\textbf{Writing $\Phi(G)$ as the result of a minimization:}} We start by writing $\Phi(G)$ as the result of an optimization problem. We encode $S \subseteq V$ as a bit string $x \in\{0,1\}^n$, and $x_i$ denote the $i^{th}$ bit of $x$.
\begin{eqnarray}
\Phi(G) & = & \min_{\{\phi \subseteq S \subseteq [n]\}} \frac{|E(S,\overline{S})||V|}{d|S||\overline{S}|}\\
& = & \min_{\substack{x \in \{0,1\}^n\setminus \{0^n,1^n\}}} \frac{\frac{n}{2}\sum_{i,j}dA_{ij}(x_i-x_j)^2}{d(\sum_{i} x_i)(n-\sum_i x_i)}\\
& = & \min_{\substack{x \in \{0,1\}^n\setminus \{0^n,1^n\}}} \frac{n\sum_{i,j}A_{ij}(x_i-x_j)^2}{2(\sum_{i} x_i)(n-\sum_i x_i)} \\
& = & \min_{\substack{x \in \{0,1\}^n\setminus \{0^n,1^n\}}} \frac{n\sum_{i,j}A_{ij}(x_i-x_j)^2}{\sum_{i,j}(x_i-x_j)^2} \\
& \ge & \min_{\substack{x \in \mathbb{R}^n\setminus \{0^n,1^n\}}} \frac{n\sum_{i,j}A_{ij}(x_i-x_j)^2}{\sum_{i,j}(x_i-x_j)^2} 
\end{eqnarray}

Now we decompose the vector $x$ as: $x = \alpha\overrightarrow{\mathbf 1}+w$ where $w \perp \overrightarrow{\mathbf 1}$. In the above expression, noting that $x_i-x_j = w_i-w_j$, we can restrict the optimization to $x \perp \overrightarrow{\mathbf 1}$. \footnote{A consequence is that $\sum_{ij} x_ix_j = \sum_i x_i (\sum_j x_j) = \sum_i x_i (\langle x, \overrightarrow{1} \rangle) = 0$.} 

\begin{eqnarray}
\Phi(G) & \ge & \min_{\substack{
x \in \mathbb{R}^n\setminus \{0^n,1^n\}\\
x \perp \mathbf{\overrightarrow{1}}}}
\frac{n\sum_{i,j}A_{ij}(x_i-x_j)^2}{\sum_{i,j}(x_i-x_j)^2}\\
& \ge & \min_{\substack{
x \in \mathbb{R}^n\setminus \{0^n,1^n\}\\
x \perp \mathbf{\overrightarrow{1}}}}
\frac{n\sum_{i,j}A_{ij}(x_i-x_j)^2}{2x^Tx-2\sum_{i,j} x_ix_j} \\
& = & \min_{\substack{
x \in \mathbb{R}^n\setminus \{0^n,1^n\}\\
x \perp \mathbf{\overrightarrow{1}}}}
\frac{n\sum_{i,j}A_{ij}(x_i-x_j)^2}{2x^Tx}
\label{eqn:cheeger-optimization}
\end{eqnarray}

\paragraph{\textbf{Writing $\lambda_2(G)$ as the result of a maximization:}} A natural starting point is the following expression of top two eigen values:

\begin{lemma}[{\bf Courant-Fischer Formula for $\lambda_2(G)$}]
If $\lambda_2(G)$ is the second largest eigen value of the normalized adjacency matrix of a graph $G$: Then,
\begin{equation}
\lambda_2 = \max_{\substack{x \in \mathbb{R}\setminus\{0\}\\x\perp 1}} \frac{x^TAx}{x^Tx}
\label{eqn:cf2}
\end{equation}
\end{lemma}
\begin{proof}
In fact, we start by deriving a similar formula for $\lambda_1(G)$ first and then adapt it to $\lambda_2(G)$. We prove:
\begin{equation}
\lambda_1 = \max_{x \in \mathbb{R}\setminus\{0\}} \frac{x^TAx}{x^Tx}
\label{eqn:cf1}
\end{equation}
We will estimate the numerator and denominator separately. We start with an orthonormal basis $v_1, v_2, \ldots v_n$ of $\mathbb{R}^n$ such that $v_i$ is an eigen vector of $\lambda_i$. Any vector $x$ can be expressed as $x = \sum_i \alpha_i v_i$ where $\alpha_i = \langle x,v_i \rangle$.

$$ x^TAx = \left(\sum_{i=1}^n \alpha_iv_i\right)^T A \left(\sum_{i=1}^n \alpha_i v_i \right) = \left(\sum_{i=1}^n \alpha_iv_i\right)^T \left(\sum_{i=1}^n \alpha_i \lambda_iv_i \right) = \sum_{i=1}^n \lambda_i \alpha_i^2  \le \lambda_1 \sum_{i=1}^n  \alpha_i^2 $$

$$ x^Tx = \left(\sum_{i=1}^n \alpha_iv_i\right)^T \left(\sum_{i=1}^n \alpha_i v_i \right) = \sum_{i=1}^n \alpha_i^2$$

Dividing the two and taking max over all $x \in \mathbb{R}^n\setminus\{0\}$, we have proved Equation~\ref{eqn:cf1} (Equality follows from the fact that $x$ can also be $\textbf{1}$). The formula for $\lambda_2$ follows if we restrict ourselves to $x \perp \textbf{1}$, $\alpha_1 = 0$. Thus, $\forall x \in \mathbb{R}^n \setminus \{0\}$, $x^TAx \le \lambda_2 \sum_{i=1}^n  \alpha_i^2$ (and considering the case of $x$ as the eigen vector corresponding to $\lambda_2$ establishes equality and hence Equation~\ref{eqn:cf2}.
\end{proof}

Now we will just modify the Courant-Fischer formula to see it as the optimization of the same objective function as in Equation~\ref{eqn:cheeger-optimization}. We use the lemma, whose proof is left as an exercise.
\begin{exercise}
If $A$ is the normalized adjacency matrix of $G$ and $x \in \R^n\setminus\{0\}$:
$$\sum_{i,j} A_{ij}(x_i-x_j)^2 = 2x^Tx - 2x^TAx$$
\end{exercise}
Applying this, $x^TAx = x^Tx - \half \sum_{i,j} A_{ij}(x_i-x_j)^2$.
$$\lambda_2 = \max_{\substack{
x \in \mathbb{R}^n\setminus \{0^n,1^n\}\\
x \perp \mathbf{\overrightarrow{1}}}}
\frac{x^Tx - \half\sum_{i,j}A_{ij}(x_i-x_j)^2}{x^Tx} = 1-\min_{\substack{
x \in \mathbb{R}^n\setminus \{0^n,1^n\}\\
x \perp \mathbf{\overrightarrow{1}}}}
\frac{\sum_{i,j}A_{ij}(x_i-x_j)^2}{2x^Tx}$$
This matches with the objective function in Equation~\ref{eqn:cheeger-optimization}. Thus, $\Phi(G) \ge 1-\lambda_2(G)$.

\section{Proof of Cheeger's Inequality: From Combinatorial to Spectral}

\jsay{Write down this proof}

\begin{exercise-prob}[See Problem Set 3~(Problem~\ref{cheeger-apply})]
\begin{show-ps3}{cheeger-apply}
In this problem, we will apply Cheeger's inequality and also show that it is tight.
\begin{enumerate}[(a)]
\item Let $G$ be a complete graph on $n$ vertices. Show that $h(G) \approx \half$. Verify Cheeger's inequality.
\item Let $G$ be a cycle on $n$ vertices. Compute $\lambda_2$ and $h(G)$ asymptotically and compare.
\item Consider the graph $G$ with $2^n$ vertices labelled with strings in $\{0,1\}^n$. The edges of $G$ are as follows - two vertices to be adjacent if the corresponding strings differ by one bit flip. Show that $\lambda_2 = 1 - \frac{1}{n}$. Hence conclude that $h(G) \ge \frac{1}{2n}$. Derive an upper bound for $h(G)$ and compare.

\end{enumerate}

\end{show-ps3}
\end{exercise-prob}

\section{Expander Mixing Lemma}
We close this lecture by proving the expander mixing lemma, which is the reason expander graphs are thought of as having properties of random graphs yet being explicitly constructible. Recall the definition and interpretation of conductance of a graph that we saw in the last lecture, which measures, as a ration how much the cut of $E(S,\overline{S})$ is, compared to the expected number of edges across the sets if the graph (and hence the edges) were chosen at random. In this, we bound this in an additive fashion:

\begin{lemma}[{\bf Expander Mixing Lemma}]
\label{lem:expander-mixing-lemma} 
Let $G$ be a $d$-regular graph with $n$ vertices and let $\lambda_2<1$ be the second largest eigen value of the normalized adjacency matrix of $G$. Then, for any $S, T \subseteq [n]$:
$$\left||E(S,T)|-\frac{d|S||T|}{n}\right| \le d\lambda_2 \left(\sqrt{|S||T|}\right)$$
\end{lemma}

\begin{remark}
This lemma can be interpreted as follows. For any $S,T \subseteq V$, the expected number of edges between them if the edges are chosen at random is the second term. Indeed, the probability that a randomly chosen vertex is in the set $T$ is $(|T|/n)$. Since there $d|S|$ vertices being chosen (as the other end point of edges where one endpoint is in $S$). Hence the expected number of edges that cross from $S$ to $T$ is $\frac{d|S||T|}{n}$.
\end{remark}

\begin{proof}
We will again use the orthonormal basis of $\mathbb{R}^n$ : $v_1,v_2, \ldots v_n$ such that $Av_i = \lambda v_i$ for each $i$. Consider the characterestic vectors of the two sets $S$ and $T$ as $1_S$ and $1_T$. Express each of them in terms of the basis:
\[1_S = \sum_i \alpha_i v_i \textrm{ where } 
\alpha_i = \langle 1_S, v_i \rangle
\textrm{~~ and ~~} 1_T = \sum_i \beta_j v_j \textrm{ where } \beta_i = \langle 1_T, v_i \rangle
\]
Note that :  $v_1 = (\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}},\ldots,\frac{1}{\sqrt{n}})$ and hence 
$\alpha_1 = \frac{|S|}{\sqrt{n}}$ and 
$\beta_1 = \frac{|T|}{\sqrt{n}}$
\begin{eqnarray*}
|E(S,T)| = 1_S^tdA1_T & = & \left( \sum_i \alpha_i v_i \right) dA \left( \sum_i \beta_j v_j \right) = 
d\alpha_1\beta_1 + d\sum_{i=2}^n \lambda_i \alpha_i \beta_i \\
& \le &
d\alpha_1\beta_1 + d\lambda_2 \sum_{i=2}^n \alpha_i \beta_i \\ 
& \le &
\frac{d|S||T|}{n} + d\lambda_2 \left(\sum_{i=2}^n \alpha_i \beta_i \right)\\
\end{eqnarray*}
Hence : 
\begin{eqnarray*}
\left||E(S,T)| - \frac{d|S||T|}{n}\right| \le d\lambda_2 \left(\sum_{i=2}^n \alpha_i \beta_i \right) \le d\lambda_2 \left(\sum_{i=1}^n \alpha_i \beta_i \right) 
\le d\lambda_2\|\alpha\|\|\beta\|
\end{eqnarray*}
Note that $$||\alpha|| = \sqrt{\sum_i \alpha_i^2} = \sqrt{\sum_i \langle 1_S, v_i \rangle^2} = ||1_S||$$
Hence the above inequality gives:
\begin{eqnarray*}
\left||E(S,T)| - \frac{d|S||T|}{n}\right| \le d\lambda_2 \|1_S\|\times\|1_T\| \le d\lambda_2\sqrt{|S||T|}
\end{eqnarray*}
\end{proof}

\begin{curiousity}[{\bf Converse of Expander Mixing Lemma}]
Expander Mixing Lemma (Lemma~\ref{lem:expander-mixing-lemma}) says that the edges acorss $S$ and $T$ for any two subsets of matrices behaves like that of the random graphs. In fact, even the converse is also true. For any two subsets of vertices if the density of the subsets of matrices behaves like that of random graphs, then it must necessarily have a good spectral gap.

In fact, the converse of Expander Mixing Lemma (Lemma~\ref{lem:expander-mixing-lemma}) is also known~\cite{BL06}. The statement is as follows:
Let $G$ be a $d$-regular graph and suppose that
$$\left||E(S,T)|-\frac{d|S||T|}{n}\right| \le d\delta \left(\sqrt{|S||T|}\right)$$
then $\lambda_2(G)$ is $O\left(\delta+\delta\log\left(d/\delta\right)\right)$.
\end{curiousity}

%\begin{curiousity}[{\bf Generalization to Hypergrahps}] Can we define spectrum for hypergraphs? In particular, is adjacency matrix and its eigen value well-defined? See \cite{FW95}. They show:
%let $H(V,E)$￼ be a $k$￼-uniform hypergraph, for any choice of subsets $V_1,\ldots V_k \subseteq V$,
%$$\left| \card{E(V_1,V_2,\ldots,V_k)}-\frac{k!\card{E}}{n^k}\card{V_1}\card{V_2}\ldots\card{V_k}\right| \le \lambda_2(H) \sqrt{\card{V_1}\card{V_2}\ldots\card{V_k}}$$
%\end{curiousity}
%
\begin{curiousity}[{\bf Ramanujan Graphs}]
Ramanujan graph, is a regular graph whose spectral gap is almost as large as possible. Such graphs are excellent spectral expanders. The complete graph $K_{d+1}$  has spectrum $d,-1, -1 \ldots -1$ and thus $\lambda_2(K_{d+1}) = d$ and hence is a Ramanujan graph for every $d > 1$. The complete bipartite graph $K_{d,d}$ has spectrum $d,0,0, \ldots 0, -d$ and hence is a bipartite Ramanujan graph for every $d$. \cite{LRS88} showed\footnote{The proof uses what is called Ramanujan conjecture, which led to the name of Ramanujan graphs.} how to construct an infinite family of $(p+1)$-regular Ramanujan graphs, whenever $p$ is a prime number and $p \equiv 1 \mod 4$. This was extended for any prime power later.See \cite{Mur03} for a survey. 
It is still an open problem whether there are infinitely many $d$-regular (non-bipartite) Ramanujan graphs for any $d \geq 3$.
\end{curiousity}

\begin{exercise-prob}
[See Problem Set 3~(Problem~\ref{stronger-mixing-lemma})]
\begin{show-ps3}{stronger-mixing-lemma}
Using techniques similar to what we used for mixing lemma, prove the following stronger version of the mixing lemma. Let $G$ be a $d$-regular graph with $n$ vertices and let $\lambda_2<1$ be the second largest eigen value of the normalized adjacency matrix of $G$. Then, for any $S, T \subseteq [n]$:
$$\left||E(S,T)|-\frac{d|S||T|}{n}\right| \le d\lambda_2 \left(\sqrt{|S||T|\left(1-\frac{|S|}{n}\right)\left(1-\frac{|T|}{n})\right)}\right)$$
\end{show-ps3}
\end{exercise-prob}

%\begin{exercise-prob}
%[See Problem Set 3~(Problem~\ref{mixing-lemma-application})]
%\begin{show-ps3}{mixing-lemma-application}
%An independent set in a graph $G(V,E)$ is a subset $V' \subseteq V$ such that no pair of vertices in $V$ has an edge among them. Use expander mixing lemma to show that the size of the independent set in an $(n,d,\lambda)$ spectral expander is at most $\lambda_2 n$. Use this to show that the chromatic number (the minimum number of colors required to properly color the graph vertices of $G$) is at least $1/\lambda_2$.
%\end{show-ps3}
%\end{exercise-prob}