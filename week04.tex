\Week{4}{Proving the Existence : The Probabilistic Method}

The probabilistic method is a simple and powerful technique to show that some combinatorial object with certain properties exists. The idea is quite simple, design a random experiment to obtain the combinatorial object and then show that the probability that the properties does not get satisfied with probability strictly less than 1. Hence, by probability arguments, there must exist the combinatorial object having the properties in the underlying sample space.

\section{Hypergraph 2-coloring}

Now we show the first application of the probablistic method through the example of hypergraph 2-coloring. 
A hypergraph is a pair of two sets $(V,E)$ referred to as vertices and edges (or hyperedges).The edges are subsets of the vertices. If all the sets in $E$ have size $k$, then we call it the $k$-uniform hypergraph. Notice that a graph is a $2$-uniform hypergraph. A proper (vertex) coloring is an assignments of colors to the vertices of a hypergraph so that no edge is monochromatic. Note that this naturally generalizes graph (vertex) coloring where it is insisted that adjacent vertices (that is, the elements of the edges) get different colors.

We show the following theorem using probabilistic method.

\begin{theorem}[\bf Erd\"os~(1963)]
If $H(V,E)$ is a $k$-uniform hypergraph with less than $2^{k-1}$ hyperedges then there exists a proper 2-coloring of $H$.
\end{theorem}
\begin{proof}
Let $H(V,E)$ be a hypergraph such that $|E| < 2^{k-1}$.
The experiment we set up is to color each vertex in the graph with one of the two colors (say red and blue) uniformly at random (that is, with probability $\half$ the vertex will be colored red and with probability $\half$ it will be colored blue). Let $A_e$ be the event that all the $k$ vertices in the hyperedge $e \in E$ gets the same color. We calculate $Pr[A_e]$ first. Since the monochromatic color for $A_e$ can be chosen in two ways: 
$$Pr[A_e] = \frac{2}{2^k} = \frac{1}{2^{k-1}}$$

The coloring is not proper if the event $A_e$ happens for at least one of the hyperedge $e$. Hence, 
$$Pr[\textrm{ coloring is not proper }] \le Pr\left[\bigcup_{e \in E} A_e\right] \le \sum_{e \in E}Pr[A_e] \le \frac{|E|}{2^{k-1}} < 1$$

That is, if we choose a random 2-coloring, we will get a proper-coloring with probability greater than zero. This, in particular implies that there exists a proper $2$-coloring of the hypergraph.
\end{proof}

We remark that by simply restricting $|E| < 2^{k-2}$ we could have proved that if we choose a random 2-coloring, we will get a proper-coloring with probability greater than $\half$. However, this implies that there is even a randomized algorithm to construct the coloring, in this case. Indeed, if we can efficiently derandomize the algorithm, it makes the proof constructive.

The following exercise tells us that the above scheme can also be applied for higher number of colors too.

\begin{exercise}
Suppose $k > 2$ and let $H$ be a $k$-uniform hypergraph with $4^{k-1}$ edges. Show that there is a 4-colouring of $V(H)$ such that no edge is monochromatic.
\end{exercise}

\begin{curiousity}[\textbf{Property-B Conjecture}]
A hypergraph H has \textbf{Property B} (or 2-colorable) if there is a red-blue vertex-coloring with no monochromatic edge. A hypergraph with property B is also called bipartite, by analogy to the bipartite graphs. Erdos (1963) asked: What is the minimum number of edges $m_2(k)$ of a $k$-uniform
hypergraph not having property $B$? Indeed, the above discussion implies that $m_2(k) \ge 2^{k-1}$. Erdos proved an upper bound of $m_2(k) \le O(k^22^k)$. The best known bounds are :
$$\Omega\left(\sqrt{\frac{k}{\ln k}}2^k\right) \le m_2(k) \le O\left(k^22^k\right)$$
The upper bound is due to Erdos (1964) and the lower bound went through a series of improvements to reach the above bound Radhakrishnan and Srinivasan (2000).
$$
\left\{
\begin{array}{c}
m_2(k) \ge \left(\half\right) 2^k \\
\textrm{\cite{Erd63} }
\end{array}
\right\}
\rightarrow
\left\{
\begin{array}{c}
m_2(k) \ge \left(\frac{k}{k+4}\right)2^k \\
\textrm{\cite{Sch64} }
\end{array}
\right\}
\rightarrow
\left\{
\begin{array}{c}
m_2(k) \ge \left(\sqrt[3]{k}\right)2^k \\
\textrm{\cite{Bec78} }
\end{array}
\right\}
\rightarrow
\vspace{-3mm}
\left\{
\begin{array}{l}
m_2(k) \ge \left(\sqrt{\frac{k}{\ln k}}\right)2^k \\[-2mm]
\textrm{[{\color{blue}RS, 2000)}]\nocite{RS00}}
\end{array}
\right\}
$$
It is believed that $k2^k$ is the right asymptotic bound for $m_2(k)$. In fact, this is a conjecture due to Erdos and Lovasz: $m_2(k) \in \Theta(k2^k)$.
\end{curiousity}

\section{Diagonal Ramsey Number Bound}

We now show the original application of the probabilistic method, when it was introduced by Erd\"os. This is to prove a lower bound for certain ramsey numbers. We now quickly introduce Ramsey numbers in this lecture.

The standard starting point is the following brain teaser : any party with at least $6$ people will contain a group of three mutual friends or a group of three mutual non-friends. Indeed, the immediate combinatorial argument goes like this : call the people $1, 2, 3, 4, 5, 6$. Either $1$ has three friends or three non-friends. Without loss of generality, suppose that $2,3,4$ are all friends with $1$. Then if any pair of them are friends with each other, that pair plus $1$ forms a group of three mutual friends. If no two of them are friends, then they are a group of three mutual non-friends. 

Note that the above can also be done graph theoretically where we consider a 6 vertex graph with drawing an edge between two vertices $i,j \in \{1,2,\ldots 6\}$ if and only if they are friends with each other. Now the above argument can be tranlated to graph : \textit{any graph on $6$ vertices will contain either a clique on 3 vertices or an independent set on 3 vertices}.\\[-2mm]

\hspace{-6mm}\begin{minipage}{0.75\textwidth}
An alternate way to represent the above problem is by $2$-coloring the edges of the complete graph $K_6$, by red if the two vertices are friends with each other and with blue otherwise. Now the above statement becomes : {\em in any 2-coloring of $K_n$, there is a monochromatic triangle} - which indeed, is more concise statement.

Is there a peculiarity with $6$ and can the same be argued for $5$? It turns out that we cannot and there is the following counter example (which we represent by the $2$-coloring of $K_5$).
Thus $6$ is the minimum number such that for any $2$-coloring of the edges of $K_6$ there will exist a monochromatic triangle.
\end{minipage}
\begin{minipage}{0.15\textwidth}
\vspace{-3mm}
\includegraphics[scale=0.5]{ramsey.jpg}
\end{minipage}

\noindent The Ramsey theory asks a general extremal combinatorics question of this form : For any $s,t \in \mathbb{N}$, what the minimum number $n$ such that there is a guarantee of the form : any $2$-coloring of the edges of the graph $G$ has either a red $K_s$ or a blue $K_t$ in it. This number exists (as proved by Frank Ramsey) and is called the Ramsey Number $R(s,t)$.
In this language, the above argument says $R(3,3) = 6$.
In fact, the existance argument for Ramsey number actually gives the followng upper bound.
\begin{proposition}
$R(s,t) \le R(s,t-1)+R(s-1,t)$.
\end{proposition}

Computing other Ramsey numbers has attracted a lot of attention from combinatorialists. However, we know very little still. $R(s,2)=s$, $43 \le R(5,5) \le 49$ etc. The numbers where $s=t$ are the diagonal entries of the Ramsey matrix (which is natural to imagine given the above). By applyng the above theorem, we have that: $$R(s,s) \le 2^{2s}\sqrt{s}$$

In the rest of this section, we will concetrate on  lower bounds for the diagonal Ramsey numbers. Notice that to show lower bound $R(s,s) \ge n$, we need to show that there is a $2$-coloring of $K_n$ where there is no monochromatic $K_s$. A constructive lower bound of this kind was discovered by Nagy which shows :
$$R(s,s) \ge {s \choose 3}$$

We now apply probabilistic method in order to obtain a stronger lower bound. Erdos, in 1947, introduced
probabilistic methods in his paper {\em Some Remarks on the Theory of Graphs} for proving this lower bound.

\begin{theorem}
The diagonal Ramsey number $R(s,s)$ is at least $\lfloor 2^{\frac{s}{2}}\rfloor$.
Equivalently, when $n=2^{\frac{s}{2}}$, there exists a $2$-coloring of $K_n$ where there is no monochromatic $K_s$.
\end{theorem}
\begin{proof}
As usual, we need to show the existance of a 2-coloring to edges. We set up the following experiment. Color each edge uniformly at random with red or blue. The total number of possible colorings is $2^{n \choose 2}$. The probability of any particular color configuraton is exactly $\frac{1}{2^{n \choose 2}}$.

We need to prove an upper bound of the bad events. Let $S \subseteq n$ of size $s$. Our coloring is bad if $S$ is colored monochromatic under the above coloring.
$$Pr[\textrm{$S$ is monochromatic}] \le \frac{2 \times 2^{{n \choose 2}-{s \choose 2}}}{2^{n \choose 2}} \le 2^{1-{s \choose 2}}$$
$$Pr[\textrm{$\exists S : S$ is monochromatic}] \le
\sum_{S \subseteq [n]:|S|=s} Pr[\textrm{$S$ is monochromatic}] \le {n \choose s}2^{1-{s \choose 2}}$$

If we show that ${n \choose s}2^{1-{s \choose 2}}$ is less than $1$ for $n=2^{s/2}$, then we are done by probablistic argument.

$$
{n \choose s}2^{1-{s \choose 2}} 
\le \frac{n^s}{s!}2^{1-(s/2)+(s^2/s)} 
\le \frac{n^s}{2^{s^2/2}}\frac{2^{1+s/2}}{s!}<1
$$
Hence the proof.
\end{proof}

\begin{exercise-prob}[See Problem Set 2~(Problem~\ref{tournament})]
\begin{show-ps2}{tournament}
A tournament is a directed graph $G(V,E)$ on $n$ vertices where for every pair $(i,j)$, there is either an edge from $i$ to $j$ or from $j$ to $i$, but not both (it represents real tournaments, where we interpret $(i,j)$ directed edge as player $i$ beats player $j$. There is no draw and all pairs of players play a game with each other). 

A tournament T is said to have \textbf{$k$-championship property} if for any set of $k$ vertices in the tournament, there is some vertex in $V$ that has a directed edge to each of those $k$ vertices.

Can $k$-Championship property occur in small tournament graphs? For example, for $k=1$, a tournament will need at least $3$ vertices to have the $k$-Championship property. If $k=2$, a tournament will need at least 5 vertices to have $k$-Championship property.

Show that there are tournments of size $O(k^22^k)$ having $k$-Championship property. [Hint : Consider a random tournament. Fix a set $S$ of $k$ vertices and some vertex $v \notin S$. What is the probability that $v$ is the champion in $S$?]
\end{show-ps2}
\end{exercise-prob}

\begin{exercise-prob}[See Problem Set 2~(Problem~\ref{dominating-set}]
\begin{show-ps2}{dominating-set}
Let $G(V,E)$ be a graph. A set of vertices $D \subseteq V$ is called dominating
with respect to $G$ if every vertex in $V \setminus D$ is adjacent to a vertex in D. $\delta(G)$, the minimum degree amongst $G$â€™s vertices, is strictly positive. Then $G$ contains a dominating set of size less than or equal to:
$$ \frac{n(1+\log(1+\delta))}{1+\delta} $$
[Hint : Choose a subset $X\subseteq V$ at random (with each vertex in with probability $p$). Let $Y \subseteq V\setminus X$ having no neighbor in $X$. Estimate $X \cup Y$.]
\end{show-ps2}
\end{exercise-prob}

\section{Expectation Method}

We now apply the method of expectation with probablistic method. The basic idea is that when we
want to show the existence of an object for which a parameter (say graph with at least certain number of connected components) satisfy some lower/upper bounds. We first define the parameter as the random variable and show that the expected value satisfies the bounds which implies that there exists an object which satisfies the bounds. The following lemma makes this more precise.

\begin{lemma}[{\bf Expectation Method}]
\label{lem:expectation-method}
For any random variable $X$, 
$$\E[X] \ge t \implies Pr[X \ge t] > 0$$
\end{lemma}

\begin{proof}
Assume for the sake of contradiction that $\Pr[X \ge t] = 0$. 
$$\E[X] = \sum_{w \in \Omega} X(w)P(w)$$
If $\Pr[X \ge t]$ was zero, then by definition, $$\sum_{\substack{w \in \Omega \\ X(w) \ge t}} P(w) = 0$$ 
Since $P(w)$ is non-negative, we have that, for every $w \in \Omega$, with $X(w) \ge t$, $P(w) = 0$.
$$\E[X] = \sum_{w \in \Omega} X(w)P(w) = \sum_{\substack{w \in \Omega \\ X(w) < t}} X(w)P(w) < t \left( \sum_{\substack{w \in \Omega}} P(w) \right) = t$$
That will be a contradiction.

\end{proof}
\noindent Question to students : Does the same statement holds when $\ge$ is replaced by any of $\le,<$ or$ >$ ?

\subsection{Sum-free Sets}

To apply the expectation method, we need to make a suitable choice of the random variable $X$ so that it is not difficult to compute its expected value. We demonstrate thetechnique by using the example of sum-free sets. We start witht he definitions.

A subset $A$ of positive integers is called \textit{sum-free}, if $\not\exists x,y,z \in S$ such that $x+y = z$ (the number may repeat). Given a set of positive integers, say $B = \{b_1,b_2, \ldots b_n\}$ how large a sum-free subset is it guaranteed to contain? 
A simple example to try out is $B = \{1,2,\ldots n\}$. If we choose all odd numbers in the set, or even the elements more than $\frac{n}{2}$, they all are sumfree subsets of size at least $\frac{n}{2}$.
Erd\"os answered this question using a cute mathematical statement.

\begin{theorem}
For any set $B = \{b_1,b_2, \ldots b_n\}$ of positive integers, there must exist a sumfree subset $A \subseteq B$ such that $|A| > \frac{n}{3}$.
\end{theorem}
\begin{proof}
We will follow the above outlined strategy to define an appropriate random variable but in an indirect way. We choose a prime $p > 2b_n$ where $b_n$ is the largest number in $B$ and that $p=3k+2$. Now all addition operation between the elements in $B$ are faithfully  captured in the the set $\Z_p$ where addition is done modulo $p$. That is there is a natural mapping from $B$ to $\Z_p$ which preserves the sum operations. The mapping can be thought of as the identity function since $p \ge 2b_n$. Indeed, $\phi(b_i+b_j) = \phi_(b_i)+\phi(b_j)$ from the definitions itself.

\hspace{-5mm}\begin{minipage}{0.65\linewidth}
{\bf Sum-free sets in $\Z_p$:} Instead of showing a sum-free subset of $B$, we show a sum-free subset $C$ of $\Z_p$ first. We explicitly define $C$ first. Consider the middle third elements of $\Z_p$. That is, $C = \{k+1,k+2, \ldots 2k+1\}$. And $|C| > \frac{p-1}{3}$. We claim that $C$ is sum-free. To see this : $x,y,z \in C$, $x+y \ge 2k+2$ even for the smallest elements in $C$, and $x+y = 4k+2$ (which is at most $k$ modulo $3k+2$) even for the largest elements in $C$. Hence none of these sums will land back in $C$ as a result of the modulo operation. Hence $C$ is sum-free.
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.25\linewidth}
\vspace{-1cm}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\draw (2,0) ellipse (1cm and 2cm) node[midway,yshift=2cm, xshift=2cm]{$B$~~~~~};
\draw (2,0) ellipse (0.7cm and 1.1cm)
node{$A$};;
\draw (5,0) ellipse (1cm and 2.1cm) node[midway,yshift=2cm, xshift=5.1cm]{$\Z_p$~~~~~};
\draw (5,0) ellipse (0.5cm and 1cm)
node{$C$};;
\end{tikzpicture}
\end{minipage}
\vspace{4mm}

\noindent{\bf Pull-back to $B$ - A failed attempt:} Now how do we go back to get a subset of $B$ which is sumfree. A natural idea is to use the "pull back" of the function $\phi$. The pre-image of $C$, namely $\phi^{-1}(C)$ must be sumfree as well. To see this : if there exists $x, y, z \in \phi^{-1}(C)$  such that $x+y = z$, then $\phi(x)+\phi(y) = \phi(z)$. Since $\phi(x),\phi(y),\phi(z) \in C$ by definition, this implies that $C$ is not sumfree and that is a contradiction. But then, why should the size of the pre-image by at least $\frac{n}{3}$? It could even be that none of the elements in $B$ are mapped to $C$, so the pre-image can even be empty !.\\[-3mm]

\noindent{\bf Way forward - random scaling: } Since the above mapping may not be good, we consider variants of $\phi$ which also preserves the addition operation  in $B$. Consider any $\alpha \in \Z_p \setminus \{0\}$, the map $\phi_\alpha : B \to \Z_p$:
$$\phi_\alpha : b_i \mapsto \alpha b_i \mod p$$
Let us quickly check if this is a function which respects the sum operation: if $b_i+b_j = b_k$, $\phi_\alpha(b_i)+\phi_\alpha(b_j)=\alpha b_i \mod p +\alpha b_j \mod p = (\alpha b_i+\alpha b_j) \mod p = \phi_\alpha(b_i+b_j) = \phi_\alpha(b_k)$
As a consequence, the pre-image of $C$ under any $\alpha$, $\phi_\alpha^{-1}(C)$  will be sumfree.

\noindent Now comes the punch line argument. We will argue :
$$\textrm{\em $\exists \alpha \in \Z_p\setminus\{0\}$ such that $\phi_\alpha^{-1}(C)$ is a large subset of $B$.}$$
It is to prove this last statement that we apply the expectation method described above. To begin with, let us write the claim more precisely:
\begin{claim}
$\exists \alpha \in \Z_p\setminus\{0\}$ such that $|\phi_\alpha^{-1}(C)| > n/3$.
\end{claim}

Original statement that we wanted to prove said, $\exists A \subseteq B$, now it says $\exists \alpha \in \Z_p \setminus \{0\}$. Let us set up the experiment.
Choose $\alpha$ uniformly at random from $\Z_p \setminus \{0\}$. Let us defined the random variable $X$ as:
$$X = |\phi_\alpha^{-1}(C)| = |\{ i \in [n] \mid \phi_\alpha(b_i) \in C \}|$$

As per expectation method, we just need to prove that $\E[X] > \frac{n}{3}$. Let us breakdown this random variable into simpler ones in terms of the individual $b_i$s. Let us define the indicator random variable corresponding to the event $\phi_\alpha (b_i) \in C$.

$$X_i^{\alpha} = \left\{
\begin{array}{ll}
1 & \textrm{ if $\phi_\alpha (b_i) \in C$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
$$
Note that, $X = \sum_{i=1}^n X_i$. Hence, by linearity of expectation, $\E[X] = \sum_{i=1}^n \E[X_i^{\alpha}]$. Thus, we need to lower bound $\E[X_i^{\alpha}]$. Since it is an indicator random variable, $\E[X_i^{\alpha}] = \Pr_\alpha\left[\phi_\alpha (b_i) \in C\right]$ which we will compute now.

For a $b_i$ and a a target element on RHS (consider a non-zero element on RHS $y \in \Z_p \setminus \{0\}$) how many $\alpha$ can map $b_i$ to $y$? This is exactly one element $\alpha = b_iy^{-1} \mod p$. Thus, if we choose an $\alpha$ uniformly at random, probability that it maps $b_i$ to $y$ is $\frac{1}{p-1}$. If, in addition, this $y$ has to be in the set $C$, then there are exactly $|C|$ choices of $\alpha$ which sends $b_i$ to $C$. That is:
$$\E[X_i^{\alpha}] = \Pr_\alpha\left[\phi_\alpha (b_i) \in C\right] = \frac{|C|}{p-1} > \frac{1}{3}$$

Thus, as planned, we have shown that $\E[X] = \sum_{i=1}^n \E[X_i^{\alpha}]$ is greater than $\frac{n}{3}$. Hence as per Lemma~\ref{lem:expectation-method} (expectation method), $\Pr_{\alpha}[X \ge \frac{n}{3}] = \Pr_{\alpha}\left[|\phi_\alpha^{-1}(C)| > \frac{n}{3}\right] > 0$. Hence, by probabilistic method, we have that there exists an $\alpha$ such that $|\phi_\alpha^{-1}(C)| > \frac{n}{3}$. Hence there exists an $A$ (namely $\phi_\alpha^{-1}(C)$) which is a subset of $B$ and is sum-free. This completes the argument.
\end{proof}

