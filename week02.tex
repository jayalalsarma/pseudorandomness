\Week{2}{Method of Conditional Expectation, Pessimistic Estimators}


In this week, the plan is to learn the technique of conditional expection which is a derandomization technique that works for several algorithms. However, this is a technique which is specific to algorithms and not to problems. And there is no hard and fast rule by which we can say whether this technique is applicable for an algorithm. We demonstrate it with the {\sc MaxCut} problem.

\section{{\sc Maxcut} Problem and Randomized Approximation}

For an undirected graph $G(V,E)$, a cut is a partition of vertices into two sets $S,T \subseteq V$. The size of a cut is the number of edges that go across these partitions. That is, the size of the set :
$$cut(S,T) = \{ e=(u,v) \mid (u,v) \in E, u \in S, v \in T \}$$

Maximum cut is a cut whose size is at least the size of any other cut. That is $|cut(S,T)|$ is the largest possible. Given a graph, the problem of finding a maximum cut in a graph is known as the {\sc Maxcut} problem.

\paragraph*{The problem is hard:} The {\sc Maxcut} problem is known to be $\NP$-hard. This implies, in particular, that if we have an efficient algorithm for the {\sc MaxCut} problem, then some of the very hard problems will yield to having efficient algorithms solving them. This is believed to be unlikely. \\[-9mm]

\paragraph*{Approximation algorithms: }Hence it makes sense to talk about algorithms which may not output the exact maximum cut, but instead another cut. Indeed, this is useless unless there are guaranteed how large is the cut output by the algorithm. An example guarantee that we may want to target is, for the algorithm, no matter what the input graph $G$ is, the cut output by the algorithm will be, say, at least $\left(\frac{1}{10}\right)$-th of the size of the maximum cut. This is called a $0.1$-approximation algorithm\footnote{Exercise: if you have not seen it already, think about what would be a similar statement that you would like to target for a minimization problem, like vertex cover problem}. Even with this relaxed target for the algorithm, is not immediately clear how to design such an algorithm. It turns out that the best known algorithm for {\sc Maxcut} does much better than this and achieves an approximation guarantee of $0.875$, and for many reasons this is believed to be the best possible ratio that any polynomial time algorithm can achieve for {\sc Maxcut} problem. However, in this lecture, we will concentrate on much smaller rations. \\[-9mm]

\paragraph*{Randomized Approximation algorithms: }
We resort to randomized algorithms - which in this context will be called randomized approximation algorithms. Notice that unlike the previous examples, {\sc Maxcut} is not a decision problem. Hence, we need to be careful about designing and analysing randomized algorithms for it. For example, there is nothing like the algorithm being correct. Instead, we have only the notion of the approximation ratio - that is how close the output of the algorithm is, to the optimal value.

\subsection{A Simple Randomized Algorithm} We start with a simple randomized algorithm for {\sc MaxCut} problem.

\begin{algorithm}%\captionsetup{labelfont={sc,bf},labelsep=newline}
\label{alg:maxcut-rand}
\caption{: Randomized Approx. Algorithm for {\sc MaxCut} for graph $G(V,E)$, $|V| = n$}
\begin{algorithmic}[1]
\State $S = T = \phi$
\For{each $i \in [n]$}
\State Choose bit $b_i \in \{0,1\}$ uniformly at random.
\If{$b_i = 1$}
\State $S = S \cup \{i\}$
\Else
\State $T = T \cup \{i\}$.
\EndIf
\EndFor
\State Output $cut(S,T)$.
\end{algorithmic}
\end{algorithm}

Notice that the sets $S$ and $T$ will form a partition of $V$ at the end of the algorithm. Clearly, the algorithm will run in linear time. Indeed, the above description is also equivalent to - choose a random subset $S$ of vertices from $V$ and outputing $cut(S,\overline{S})$. Indeed, we need to give a guarantee about the size of the cut output by the algorithm. Roughly, the claim is that the average size of the cut (where average is taken over the $2^n$ different outcomes of the random choices). We recap some basics of random variables and expections now before stating the correctness claim.

\subsection{Recap of Probability Basics, Random Variables, Expectation}

Fix a set $\Omega$, which is called the {\em sample space}.
A probability distribution is defined over $\Omega$, is a function $\Pr: \Omega \to [0,1]$ satisfying the additional condition that, $\sum_{w \in \Omega} \Pr(w) = 1$. An event is a subset ${\cal{E}} \subseteq \Omega$. The probability of an even $\cal{E}$ is nothing buy the sum of the probability values by assigned by the distribution functon to the elements in the subset $\cal{E}$. That is, $\Pr({\cal{E}}) = \sum_{w \in E} \Pr(w)$.

We consider an example which are going to be relevant for us. This is the notion of random graphs. Consider $n$ vertices, and there are ${n \choose 2}$ possible edges. Imagine that we will choose (independently) each edge to be present in our graph with probability $p$ and to be absent in our graph with probability. The outcome of the experiment is an $n$-vertex simple graph and hence the sample space is the set of all $n$ vertex graphs.

As an example, we can consider, $n=3$. There are $3$ possible edges and hence $8$ possible graphs. The probability assigned the triangle graph (which is the complete graph on $3$ vertices) is $p^3$ since all the three edges have to be chosen for this particular outcome to happen. In a similar way, the following pictures denote the sample space in this case with the corresponding probability values.\\

\begin{center}
\begin{tikzpicture}
\path[draw] (2,2)node(){$\circ$} -- (3,3)node(){$\circ$} -- (4,2)node(a){$\circ$} -- cycle;
\path[draw] (5,2)node(){$\circ$} -- (6,3)node(){$\circ$}--(7,2)node(){$\circ$};
\path[draw] (8,2)node(){$\circ$} -- (10,2)node(){$\circ$}--(9,3)node(){$\circ$};  
\path[draw] (13,2)node(){$\circ$}--(12,3)node(){$\circ$} -- (11,2)node(){$\circ$};  
\path[draw] (2,0)node(){$\circ$} -- (3,1)node(){$\circ$}(4,0)node(){$\circ$};
\path[draw] (5,0)node(){$\circ$} -- (7,0)node(){$\circ$}(6,1)node(){$\circ$};
\path[draw] (10,0)node(){$\circ$} -- (9,1)node(){$\circ$}(8,0)node(){$\circ$};
\path[draw] (11,0)node(){$\circ$} (12,1)node(){$\circ$}(13,0)node(){$\circ$};
\end{tikzpicture}
\end{center}

\noindent The probabilities in that order are $p^3$, $p^2(1-p)$, $p^2(1-p)$, $p^2(1-p)$, $p(1-p)^2$, $p(1-p)^2$, $p(1-p)^2$, $(1-p)^3$.

How do we analyse the probability that we get a connected graph as the outcome of the experiment. This is where events are used. Recall that formally, an event $\cal{E}$ is a subset of $\Omega$.

$$Pr({\cal{E}}) = \sum_{w \in {\cal{E}}} Pr(w)$$

\noindent In the above example, if the event ${\cal{E}}$ represent the set of connected graphs.

$$Pr({\cal{E}}) = p^3+2p^2(1-p)$$

\noindent In the above example, if the event ${\cal{E}'}$ represent the set of bipartite graphs.

$$Pr({\cal{E}'}) = 1-p^3$$

\begin{proposition}[{\bf Subadditivity of Probability - a.k.a - Union theorem}]
Let ${\cal{E}}_1$,${\cal{E}}_2 \ldots {\cal{E}}_n$ be events, then :
$$Pr\left[\bigcup_{i} {\cal{E}}_i\right] \le \sum_{i=1}^n Pr[{\cal{E}}_i]$$
\end{proposition}

\begin{definition}[{\bf Conditional Probability}]
For two events ${\cal{E}}$ and ${\cal{E}'}$, we define, 
$$Pr({\cal{E}}|{\cal{E}'}) = \frac{Pr({\cal{E}}\cap{\cal{E}'})}{Pr({\cal{E}'})}$$
\end{definition}

\noindent The conditional probability captures the questions of the kind, what is the probability that we get a connected graph if we are given that the outcome is a bipartite graph? 

\begin{definition}[{\bf Independent events}]
Two events ${\cal{E}}$ and ${\cal{E}'}$ are said to be \textit{independent}, if 
$$Pr({\cal{E}}|{\cal{E}'}) = Pr({\cal{E}})$$
Equivalently,
$$Pr({\cal{E}} \cap {\cal{E}'}) = Pr({\cal{E}})Pr({\cal{E}}')$$
\end{definition}

For example, if we consider the events event ${\cal{E}}$ represent the set of connected graphs and event ${\cal{E}'}$ represent the set of bipartite graphs, then:
$$Pr({\cal{E}} \cap {\cal{E}'}) = 3p^2(1-p)$$
$$Pr({\cal{E}})Pr({\cal{E}'}) = \left[(1-p)^3+3p(1-p)^2+3p^2(1-p)\right](1-p^3)$$

Since they are not equal, we conclude that the two events are not indendent. That is, the event that the graph is bipartite has an "influence" on the event that the graph is connected. To make this clearer, we suggest the following exercise:

\begin{exercise}
Let $G \in G(n,p)$. For all $S \subseteq V$, let $A_S$ be the event that $S$ forms an independent set in $G$. Show that if $S$ and $T$ are two distinct subsets of $k$ vertices then $A_S$ and $A_T$ are independent if and only if
$|S \cap T| \le 1$.
\end{exercise}

\noindent Now, we will generalize the above notion of independence to more than two events. An event $\calE$ is independent of a set of events
$\{ \calE_j \mid j \in J\}$ if, for all subset $J' \subseteq J$, 
$Pr [ \calE | \cap_{j \in J} B_j ] = Pr(A)$.

\begin{exercise}
Prove that an event $\calE$ is independent of a set of events $\{ \calE_j \mid j \in J\}$ 
if and only if for all $J_1, J_2 \subseteq J$
such that $J_1 \cap J_2 = \phi$
$$\Pr [ \calE \cap \left( \cap_{j \in J_1} B_j \right) \cap \left( \cap_{j \in J_2} \overline{B_j} \right) ] = \Pr(\calE) \Pr[\left( \cap_{j \in J_1} B_j \right) \cap \left( \cap_{j \in J_2} \overline{B_j} \right) ]$$
\end{exercise}

Let $\{ \calE_i \mid i \in I \}$ be a (finite) set of events. They are \textit{pairwise independent} if for all $i \ne j$ the events $\calE_i$ and $\calE_j$ are independent. Events are \textit{mutually independent} if each of them is independent from the set of the others. It is important to note that events may be pairwise independent but not mutually independent. Following exercise demonstrates that.

\begin{exercise-prob}[See Problem Set 1(Problem~\ref{independence})]
\begin{show-ps1}{independence}
A random $k$-colouring fo a graph $G$ is an element of the probability space $(\Omega,Pr)$
where $\Omega$ is the set of all $k$-colourings (i.e. partition of $V$ into $k$ sets ($V_1,V_2,\ldots,V_k$), all this colourings being equally likely (so happening with probability $\frac{1}{k^n}$. For every edge $e$ of $G$, let $A_e$ be the event that the two endvertices of e receive the same colour. Show that:
\begin{enumerate}[(a)]
\item for any two edges e and f of G, the events $A_e$ and $A_f$ are independent.
\item if $e,f$ and $g$ are three edges of a triangle of $G$, the events $A_e$, $A_f$ and $A_g$ are dependents.
\end{enumerate}
\end{show-ps1}
\end{exercise-prob}

\paragraph*{Random Variables:}
We need the idea of random variables which we recap now. A random variable is another function $X : \Omega \to \mathbb{R}$. The expected value of the random variable is the "weighted average" value that it takes over the real numbers - weighted by the correspoding probability values. That is, 
$$ E[X] = \bigsum_{\alpha \in \mathbb{R}} \alpha \Pr[X = \alpha]$$
Indeed, $[X = \alpha]$ represents an event $\{ w \in \Omega \mid X(w) = \alpha \} \subseteq \Omega$. Hence, the expectation can also be written equivalently as follows:
$$E[X] = \bigsum_{\alpha \in \mathbb{R}} \alpha \left( \bigsum_{\substack{w \in \Omega \\ X(w) = \alpha}} \Pr(w) \right)  = \bigsum_{w \in \Omega} X(w)\Pr(w)$$

We need the following properties of expectation:
\begin{description}
\item{\bf Tool 1 : Boolean Random Variables } - 
Suppose $X$ is a random variable that takes only Boolean values. In this case, $E[X] = \Pr[X=1]$ which follows from the definitions.
\item{\bf Tool 2 : Linearity of Expectation }: Suppose $X_1$ and $X_2$ are random variables defined based on the same probability distribution, consider the new random variable defined as $X = c_1X_1 + c_2X_2$. This is also a random variable as it is a function from $\Omega \to \mathbb{R}$ defined as $X(w) = c_1X_1(w)+c_2X_2(w)$ for every $w \in \Omega$. It turns out there is a neat relationship between the expectation of the random variables $X, X_1$ and $X_2$. This is one of the most imporant relation that is extensively used in analysis of randomized algorithms.
\begin{eqnarray*}
E[X] & = & \sum_{w \in \Omega} X(w)\Pr(w) = \sum_{w \in \Omega} \left( c_1X_1(w)+c_2X_2(2) \right) \Pr(w) \\
& = & c_1 \left( \sum_{w \in \Omega} X_1(w)\Pr(w)  \right)  + c_2 \left( \sum_{w \in \Omega} X_2(w) \Pr(w) \right)
= c_1E[X_1]+c_2E[X_2]
\end{eqnarray*}

We suggest practicing the application of linearity of expectation using the following exercise.
\begin{exercise-prob}[See Problem Set 1(Problem~\ref{spanning-trees})]
\begin{show-ps1}{spanning-trees}
A graph $G=(V,E)$ is created at random by selecting each edge with probability $p$.  What is the expected number of spanning trees in the randomly sampled graph?
(\textit{Hint : Use Cayley’s Theorem that the number of distinct spanning trees on $n$ vertices is $n^{n-2}$. Order them, and define an indicator random variable.})

\end{show-ps1}
\end{exercise-prob}

\item{\bf Tool 3 : Averaging Principle } - 
Suppose $X$ is a random variable and $E[X] = \mu$, then the following statements follow:
$$\exists w \in \Omega : X(w) \ge \mu \hspace{1cm} \exists w \in \Omega : X(w) \le \mu$$
Both of them can be proved by contradiction. For sample, we spell out the first one, suppose that the first statement is false. That is, $\forall w \in \Omega, X(w) < \mu$, then:
$$E[X] = \bigsum_{w \in \Omega} X(w) \Pr(w) < \bigsum_{w \in \Omega} \left( \mu \times  \Pr(w) \right) = \mu \left( \bigsum_{w \in \Omega} \Pr(w) \right) = \mu$$
This implies, $E[X] < \mu$ which is a contrdiction. A similar proof holds for the other claim as well.
\item{\bf Tool 4 : Tail inequalities} - Suppose we have a random variable $X$ such that $E[X] = \mu$. What kind of probability guarantees can we write for $X$? For example, can we bound (in terms of the expectation) the probability that $X > \alpha$ for some $\alpha \in \mathbb{R}$? This is what tail bounds do. They help us write probability upper bounds based on expectations and other related parameters. 

As a first example, consider a random variable that takes only non-negative values. Then we can write :
$$\textrm{\bf Markov's Inequality : } \Pr[X \ge a] \le \frac{E[X]}{a}$$
The proof is also quite simple.
$$
E[X] = \bigsum_{\alpha \in \mathbb{R}} \alpha \Pr[X=\alpha] 
\ge \bigsum_{\alpha \ge a} \alpha \Pr[X=\alpha] \ge 
\bigsum_{\alpha \ge a} a \Pr[X=\alpha] \ge 
a \Pr[X \ge a]$$

For example, this helps us make statements of the form :

In particular, Markov’s inequality implies the following bound on the probability that any random variable $X$ is significantly larger than its expectation:

\begin{equation}
\Pr[X \ge (1 + \delta)\E[X]] \le \frac{1}{1+\delta}
\label{Markovs-inequality}
\end{equation}


For example, the probability that the random variable takes a value which is more than $4$ times the expected value is at most $0.25$. Unfortunately, this does not help us write down a probability bound for $X$ taking value less than say $\frac{\E[X]}{4}$. Indeed, another form is :

Indeed, Markov's inequality is also pretty weak - as demonstrated by the following example - consider tossing $n$ coins and $X$ be the number of heads. Clearly $E[X] = \frac{n}{2}$. By Markov's inequality, $\Pr[X \ge n] \le \frac{1}{2}$ but we know that it is much smaller than that, namely $\frac{1}{2^n}$.

What do we do if we want to bound the probability that the random variable takes a much lower value than the expectation? This is where we require more the next tail bound (without any assumption of positivity on the random variable).
$$\textrm{\bf Chebychev's Inequality : } \Pr[|X-\mu| \ge a] \le \frac{Var[X]}{a^2} \textrm{ where, $Var[X] = \E[X^2] - \E[X]^2$} $$
\begin{proof}[\textit{Proof of Chebychev's Inequality in the sum of random variables case:}]
More often, we require the Chebychev's inequality when $X$ is a random variable that is expressible as a sum of several independent random variables. We will handle that case. In particular, we will prove that:
\begin{equation}
\Pr\left[ (X-\mu)^2 \ge a \right] \le \frac{E[X]}{a}
\label{chebychev:sum}
\end{equation}

$X = \sum_{i=1}^k X_i$. Let $\E[X_i] = \mu_i$ and $\E[X] = \sum_i \mu_i = \mu$ (say). 

Define $Y_i = X_i - \mu_i$. By linearity of expectation, $\E[Y_i] = 0$. And let $Y = \sum_i Y_i = X - \mu$. The idea is to apply Markov's inequality to a postive random variable that can be formed out of $Y$. Indeed $Y$ can take negative values, hence we can consider $Y^2$. We care about $\E[Y^2]$ in order to apply the Markov's inequality. 
$$\E[Y^2] = \E\left[\left( \sum_i Y_i \right) \left( \sum_i Y_i \right)\right] =  \E \left[\sum_{i,j} Y_i Y_j \right] = \sum_{i,j} \E[Y_iY_j] = \sum_i \E[Y_i^2] + \sum_{i \neq j} \E[Y_iY_j] $$
Now we will apply independence of the random variables, and conclude that $\E[Y_iY_j] = \E[Y_i]\E[Y_j]$ but then in our case $\E[Y_i]=0$. Using this:
$$\E[Y^2] = \sum_i \E[Y_i^2] = \sum_i \left[ \mu_i(1-\mu_i)^2 + (1-\mu_i)\mu_i^2 \right] = \sum_i \mu_i(1-\mu_i) < \mu$$
Hence, the theorem (equation~\ref{chebychev:sum}) follows from Markov's inequality.
The general form also has a similar proof. And note that $Var[Y] = \E[Y^2]$ since $\E[Y] = 0$.
\end{proof}

Chebychev's inequality gives significantly better bounds compared to Markov's inequality. For example, it implies (compare with Equation~\ref{Markovs-inequality}), by substituting $a = \delta^2 \mu^2$ in Equation~\ref{chebychev:sum}.
\begin{equation}
\Pr[X \ge (1 + \delta)\mu] \le \Pr[(X-\mu)^2 \ge \delta^2\mu^2] \le \frac{1}{\delta^2 \mu}
\label{Chebychev-inequality}
\end{equation}
The advantage of working with $(X - \mu)^2$ is that, we can use it to bound the probability of the "lower tail" also.
\begin{equation}
\Pr[X \ge (1 - \delta)\mu] \le \Pr[(X-\mu)^2 \ge \delta^2\mu^2] \le \frac{1}{\delta^2 \mu}
\label{Chebychev-inequality-lowertale}
\end{equation}


\begin{remark}
Note that we did not use independence fully - we only needed that for all $i \ne j$, $\E[Y_iY_j] = \E[Y_i]\E_[Y_j]$. This is a strictly weaker condition than saying all random variables $Y_1, Y_2, \ldots Y_k$ are independent. Such random variables are called "pairwise independent". We will come across them later in the course.
\end{remark}

We now present one of the stronger tools to estimate probability tails in the case of sum of fully independent random variables. In this case, the set up itself is about a random variable $X = \sum_{i=1}^k X_i$. Let $\E[X_i] = \mu_i$ and $\E[X] = \sum_i \mu_i = \mu$ (say). We have the following:
$$\textrm{{\bf Chernoff Bound :} For any $\alpha > \mu$, } \Pr[X \ge \alpha] \le e^{\alpha-\mu}\left(\frac{\mu}{\alpha}\right)^\alpha$$
$$\textrm{In particular, it implies: } \Pr[X \ge (1+\delta)\mu] \le \left( \frac{e^{\delta}}{\left(1+\delta\right)^{(1+\delta)}} \right)^\mu$$

\begin{proof}[Proof of Chernoff Bound:]
Again the idea is to use Markov's inequality for an appropriately defined positive random variable. In this case, we will just use $Y = \left(\frac{\alpha}{\mu}\right)^X$ and it is related to the original probability that we wanted to estimate. More precisely, 
$$\Pr[X \ge \alpha] = \Pr\left[\left(\frac{\alpha}{\mu}\right)^X \ge \left(\frac{\alpha}{\mu}\right)^\alpha\right] = \Pr\left[Y > \left(\frac{\alpha}{\mu}\right)^\alpha\right] \le \frac{E[Y]}{\left(\frac{\alpha}{\mu}\right)^\alpha}$$

Since $\E[Y]$ is $\E\left[\left(\frac{\alpha}{\mu}\right)^X\right]$, we have the motivation to estimate $\E\left[a^X\right]$ in terms of $\E[X]$. Here we use the fact that $X = \sum_i[X_i]$ and are fully independent. Note that $\E\left[a^X\right] =  \prod_i \E\left[a^{X_i}\right]$. Hence, we need to estimate $\E\left[a^{X_i}\right]$.
\begin{eqnarray*}
\E\left[a^{X_i}\right] & = & a^0\Pr[X_i=0]+a^1\Pr[X_i=0] \\
& = & 1.(1-\E[X_i])+a.\E[X_i] = a\mu_i+(1-\mu_i) = \mu_i(a-1)+1 < e^{(a-1)\mu_i}
\end{eqnarray*}
In our context $a = \frac{\alpha}{\mu}$ and $Y = \left(\frac{\alpha}{\mu}\right)^X$. This gives, 
$$\Pr[X \ge \alpha] 
\le \frac{E[Y]}{\left(\frac{\alpha}{\mu}\right)^\alpha}
\le \frac{E\left[\left(\frac{\alpha}{\mu}\right)^X\right]}{\left(\frac{\alpha}{\mu}\right)^\alpha}
\le \frac{\prod_i E\left[\left(\frac{\alpha}{\mu}\right)^{X_i}\right]}{\left(\frac{\alpha}{\mu}\right)^\alpha}
\le \frac{\prod_i e^{(a-1)\mu_i}}{\left(\frac{\alpha}{\mu}\right)^\alpha}
\le \frac{e^{(\frac{\alpha}{\mu}-1)\mu}}{\left(\frac{\alpha}{\mu}\right)^\alpha}
\le e^{(\alpha-\mu)}\left(\frac{\mu}{\alpha}\right)^\alpha
$$
\end{proof}
\end{description}

\subsection{Analysis of the Algorithm for {\sc Maxcut}}
Recall Algorithm~\ref{alg:maxcut-rand}. We want to guarantee that th expected size of the cut is at most half of the optimal cut size. In fact, we prove something stronger.
\begin{claim}
Let $X$ be the size of the cut output by the algorithm~\ref{alg:maxcut-rand}. Then, $E[X] \ge \frac{m}{2}$ where $m$ is the number of edges in the graph $G$.
\end{claim}
\begin{proof}
For each edge $e \in E$ define a random variable $X_e$ as the following indicator variable:
$$ X_e = \begin{cases} 1 & \textrm{ if $e \in cut(S,T)$ } \\
0 & \textrm{ otherwise } \end{cases} $$
By definition, $X = \sum_{e \in E} X_e$. Hence, by linearity of expectation:
$$E[X] = \sum_{e \in E} E[X_e] = m E[X_e]$$
We just need to notice that $E[X_e] = \Pr[X_e = 1] = \Pr[e \in cut(S,T)]$ because of tool 1. Notice that an edge $e$ is in the cut if the two end points get into different sets among $S$ and $T$. That is, out of the four possible outcomes of the random coin tosses correspnoding to the endpoint vertices of $e$, two of them leads to $e$ being in $cut(S,T)$. Hence this is exactly $\half$. This gives:
$E[X] \ge \frac{m}{2}$. Hence the proof.
\end{proof}
Notice that the claim is stronger. Indeed, since the optimum cut can only cut at most $m$ edges, the above also implies $E[X] \ge \frac{m}{2} \ge \frac{\textrm{\sc OptCut}}{2}$.

\section{Method of Conditional Expectation}

We now describe the main technical idea to be learned this week. Mainly an algorithm specific technique of derandomization of randomized algorithms. This presentation is from Salil Vadhan's book on Pseudorandomness.

There are two kinds of randomized algorithms that we have seen so far - essentially to solve two kinds of problems. One is for the decision problems where the probability over different paths of the  computation tree, the algorithm being correct is at least $\frac{2}{3}$. The other is for optimization problems where the expected size of the output has guarantees. The method of derandomization that we are going to discuss can in principle be applied for both, if the randomized algorithm in question uses the random bits in a peculiar way that certain measures can be computed efficiently about the output for particular settings of the randomness. \\[-8mm]

\paragraph*{Main Idea:}(as we described in the lecture) - we discuss the first kind of algorithms first and then adapt it to the second type. In the decision problem case, we know that $\frac{2}{3}$-rd paths in the computation tree are going to make the algorithm answer correctly. A vague idea would be - walk down the path of the tree, \textit{making a choice deterministically and efficiently at each node (without trying both choices which leads to exponential time) maintaining the invariant that within the subtree that have restricted ourselves to, a $\frac{2}{3}$ fraction of paths within the subtree still make the algorithm go correct}. If we make choices like this and set the random bits based on that choices, it is intuitive that we will reach a leaf that makes the algroithm answer correctly and then we can just run the algorithm on that leaf (that choice of random bits) and output the answer. The process is deterministic and effcient and hence gives a derandomization of the original algorithm.

Of course, this is easier said than done. An important question remains - \textit{at an intermediate node, in the above walk-down, how do we deterministically and efficiently decided whether to take the edge labelled $0$ or $1$ to move to the child}? Formalizing this will require us to fix the type of problem as decision vs search/optimization problem.

\subsection{Framework for Algorithms for Decision Problems}

Recall the following notational set up where $\calA$ is the randomized algorithm solving a decision problem.

\vspace{5mm}
\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^m$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.5\linewidth}
%\vspace{-7mm}
%\vspace{-3mm}
$$\forall x \in \{0,1\}^n,~\Pr_{y \in \{0,1\}^m} [A(x,y) \textrm{ is correct.}] \ge \frac{2}{3} $$
\end{minipage}
\vspace{3mm}

Since we have to keep track of the fraction of paths for a particular partial setting of the random bits (while analysing the walk-down of the tree at an intermediate stage) - we define the following notation:

For every $i \in [n]$, bits $r_1, r_2, \ldots r_i \in \{0,1\}$, define:
$$p(r_1, r_2, \ldots, r_i) = \Pr_{y \in \{0,1\}^m} \left\{ A(x,y) \textrm{ is correct } \mid  (y_1 = r_1) \land (y_2 = r_2) \land \ldots \land (y_i = r_i) \right\}$$

Indeed, if we are at a particular node represented by a partial assignments $r_1, r_2, \ldots r_i$, the value of $p(r_1, r_2, \ldots , r_i)$ is the average of the value at the two children of that node since the bit is chosen uniformly at random. In terms of expectation, this is equivalent to :
$$p(r_1, r_2, \ldots r_i) = E_{y_{i+1} \in \{0,1\}} (r_1, r_2, \ldots r_i, y_{i+1})$$

To understand this definition clearly, let us ask, a first question, what is the value of $p(r_1, r_2, \ldots r_m)$ for a setting $r_1, r_2, \ldots r_m \in \zo$. (Class answered $0$ or $1$ depending on whether the setting represents a path which makes $\cal{A}$ correct or not). How about $p(\phi)$, which represents the value of the function when no bit is set. Clearly, by definition, this represents the top of the computation tree, and hence the fraction of correct paths under the node is exactly the success probability of the algorithm. That is, $p(\phi) \ge \frac{2}{3}$.

Indeed, if we call $p(r_1, r_2, \ldots r_i) = \mu$, we have that: $E_{y_{i+1} \in \{0,1\}} p(r_1, r_2, \ldots r_i, y_{i+1}) = \mu$. Hence we know that there must exist a setting of $y_{i+1}$ such that the value of the random variable - which in this case is $p(r_1, r_2, \ldots r_i, y_{i+1})$ - is at least the expected value. That is,
$$\exists r_{i+1} \in \{0,1\} : p(r_1, r_2, \ldots r_{i+1}) \ge p(r_1, r_2, \ldots r_i) $$
Applying this repeatedly, we have that $\exists r_1, r_2, \ldots r_m \in \zo$:
$$ p(r_1, r_2, \ldots r_m) \ge p(r_1, r_2, \ldots r_{m-1}) \ge \ldots \ge p(r_1, r_2) \ge p(r_1) \ge p(\phi) \ge \frac{2}{3}$$
Notice that, by our observation, the left-end term is Boolean, and hence it must be that there exists $r_1, r_2, \ldots r_m \in \zo$ such that $p(r_1, r_2, \ldots r_m) = 1$. But then, this is not a big deal in the end, we knew about existance of such $r_i$'s anyway. So in the end it does not look very useful.

But the above framework has an interesting feature. It also shows how to construct $r_i$'s bit-by-bit, if we have an efficient algorithm to compute $p(r_1, r_2, \ldots r_i)$ for any $i$. Suppose we have computed $r_1, \ldots r_i$ already, we can compute $r_{i+1}$ as follows: compute $p(r_1, r_2, \ldots, r_{i}, 0)$ and $p(r_1, r_2, \ldots, r_{i}, 1)$ using the above algorithm and set $r_{i+1}$ to be whichever bit in $\zo$ which achieves the maximum. 

However, we still have the problem of computing $p(r_1, r_2, \ldots r_i)$ for any $i \in [m]$ efficiently. This is where the algorithm-specifics come in. The algorithm $\cal{A}$ should be using the random bits in a peculiar way such that the value of $p(r_1, r_2, \ldots, r_i)$ can be computed efficiently for that algorithm $\cal{A}$. Indeed, the trivial method of computing $p(r_1, r_2, \ldots r_i)$ ends up taking exponential time in the worst case. Hence one has to use the algorithm-specific attributes to design this algorithm. We will demonstrate this in the next context.

\subsection{Framework for Algorithms for Optimization Problems}

We now the adapt the above framework for search and optimization problems. The guarantee for algorithms solving such problems is as follows - the expected size of the output is at least as "good" as this, where "good" means at most or at least in minimization and maximization problems respectively. For demonstrative purposes, we restrict ourselves to the randomized algorithm for {\sc Maxcut} that we presented earlier. Notice that the algorithm uses exactly $n$ random bits where $|V|=n$. Note that $S$ and $T$ are the two subsets output by the algorithm.
For any $i  \in [n]$, define:
$$V(r_1, r_2, \ldots r_i) = {\mathbb{E}}_{y \in \zo^n} \left[ |cut(S,T)| : (y_1=r_1) \land (y_2=r_2) \land \ldots \land (y_i = r_i) \right] $$

Similar to the previous setting, note that $V(\phi) \ge \frac{|E|}{2}$ since the expected size of the cut when no random bit is conditioned is similar to the original analysis of the algorithm. We apply the averaging principle and argue in a similar way that there must exist a choice of the random bits $r_1, r_2, \ldots, r_n \in \zo$ such that $cut(S,T)$ output by the algorithm has at least $\frac{|E|}{2}$ many edges.

Indeed, we start with $V(\phi) \ge \frac{|E|}{2}$. By averaging principle, there must exist $r_1 \in \zo$ such that $V(r_1) \ge V(\phi) \ge \frac{|E|}{2}$. Continuing in a similar way there must exist $r_1, r_2, \ldots, r_n \in \zo$ such that : 
$$ V(r_1, r_2, \ldots r_{n-2},r_{n-1},r_n) \ge V(r_1, r_2, \ldots r_{n-2},r_{n-1}) \ge V(r_1, r_2, \ldots r_{n-2}) \ldots \ge V(r_1) \ge V(\phi) \ge \frac{|E|}{2} $$
Again, this is not new information. We can always derive by a globally applying averaging principle that there must exist such a choice of random bits. But the advantage here is that if there is an efficient algorithm for computing $V(r_1, r_2, \ldots r_i)$ for any $i$, then we can find out the explicit choice of values of the bits as well. As in the previous case, if $r_1, r_2, \ldots r_i$ is already fixed, then we compute $r_{i+1}$ as : compute $V(r_1, r_2, \ldots r_i,0)$ and $V(r_1, r_2, \ldots r_i,1)$ and set $r_{i+1}$ to be that value in $\zo$ which results in the maximum among the two.

\paragraph*{Computing $V(r_1, r_2, \ldots r_i)$ for the algorithm for {\sc Maxcut} :} To apply the above framework, all we need is an efficient algorithm to compute the value of $V(r_1, r_2, \ldots r_i)$ for any choice of $i$ and $r_1, r_2, \ldots r_i \in \zo$. Note that this is a speciality of the algorithm - more importantly the way the bits $y_1, y_2, \ldots y_n$ are used by the algorithm.

At any intermediate point of computation, there are vertices for which the decision (of whether they should be in the set $S$ or not) is already made by then and there are vertices which are decided later. To keep track of this, we define:
\begin{eqnarray*}
S_i & = & \{ j \in [n] \mid j \le i, b_j = 1 \} \\
T_i & = & \{ j \in [n] \mid j \le i, b_j = 0 \} \\
U_i & = & \{ j \in [n] \mid j > i \}
\end{eqnarray*}
The algorithm will grow $S_i$ to $S$ and $T_i$ to $T$, by randomly choosing the remaining vertices $(U_i)$ to be in $S$ or $T$. We need to compute the expected size of the cut conditioned on the fact that the sets $S_i$ and $T_i$ are already fixed by the algorithm. An immediate observation is that  the edges that go across $S_i$ and $T_i$ will necessarily a part of the cut, since their endpoints are already at $S$ and $T$ respectively by definition. The edges which are fully within $S_i$ or fully within $T_i$ are not going to be a part of the cut finally. But there may be more number of edges which forms a part of the final cut. Considering this, we can write:
$$V(r_1, r_2, \ldots r_i) = |cut(S_i, T_i)| + \frac{|(cut(S_i, U_i)| + |cut(U_i,T_i)| + |cut(U_i,U_i)|}{2}$$
We need to explain the second term in the RHS. Consider edges $e=(u,v) \in E$ that has one endpoint $u \in S_i$ and the other endpoint in $v \in U_i$. Note that $u \in S$ finally, and hence $(u,v)$ edge will be counted in the cut, if $v$ falls into $T$. Since this is decided by choosing a random bit, the probability that the edge appears in the cut finally is $\half$. Hence, the expected number of edges in $cut(S_i,U_i)$ which appear in the final cut is $\frac{|cut(S_i,U_i)|}{2}$. Similar argument explains the term $\frac{|cut(T_i,U_i)|}{2}$. To see the $\frac{|cut(U_i,U_i)|}{2}$ term, consider edges which are having both end points in $U_i$. They are both going to be put in $S$ or $T$ uniformly at random - hence out of the four possible outcomes (for these two vertices), two of them puts them in the final cut and two of them puts them outside the final cut output by the algorithm. Hence for any edge in $cut(U_i, U_i)$, with probability $\half$ it will form a part of the cut. That is, expected number of edges that gets contributed to the final cut is $\frac{|cut(U_i,U_i)|}{2}$. Hence the expression for $V(r_1, r_2, \ldots r_{i})$ is correct.


\begin{exercise-prob}[See Problem Set 1(Problem~\ref{maxcut-greedy})]
\begin{show-ps1}{maxcut-greedy}
In the derandomization of {\sc Maxcut} algorithm that we described, we derived an expression for $V(r_1, r_2, \ldots r_i)$ for any $i$ and $r_1, r_2, \ldots r_i \in \zo$. We used this to determine, the value of $r_{i+1}$ by computing $V(r_1, r_2, \dots r_i, 0)$ and $V(r_1, r_2, \dots r_i, 1)$ and then choosing the value of $r_{i+1}$ to be the one which produces the largest among the two.
Prove that the choice of $r_{i+1}$ will be $1$ if vertex $i+1$ has more neighbors in $T_i$ than in $S_i$ and vice versa. Hence, write down the derandomized $0.5$-approximation deterministic polynomial time algorithm for {\sc Maxcut} as a simple greedy algorithm in terms of the above rule.
\end{show-ps1}
\end{exercise-prob}


\begin{exercise-prob}[See Problem Set 1(Problem~\ref{maxsat})]
\begin{show-ps1}{maxsat}
Let $x_1, x_2, \ldots x_n \in \zo$ be Boolean variables and let $f$ be a Boolean formula in CNF form. That is, $f = C_1 \land C_2 \land \ldots \land C_m$ where each $C_i$ (called a {\em clause}) is a disjunction of literals in the set $\{x_1, \overline{x_1}, x_2, \overline{x_2}, \ldots, x_n, \overline{x_n} \}$. We want to find an assignment of the Boolean variables that satisfies as many clauses in the formula as possible.
\begin{enumerate}[(a)]
\item Write down a randomized algorithm that outputs an assignment with the guarantee that the expected number of clauses satisfied is at least $\frac{m}{2}$.
\item Derandomize this algorithm using the method of conditional probabilities discussed in class to get a deterministic algorithm that satisfies at least $\frac{m}{2}$ number of clauses.
\item Suppose $k$ is the minimum number of literals in any clause, how will you modify the parameters in part(a) and (b) 
\end{enumerate}
\end{show-ps1}
\end{exercise-prob}

\section{Method of Pessimistic Estimators}

We now see a more sophisticated adaptation of the method of conditional expectation. For this we need another randomized algorithm, as it is again going to be algorithm specific\footnote{That is, it can be applied for a class of algorithms for the problem which has the property.}.

\subsection{Congestion Minimization Problem}

The problem that we are going to use as the example is called \textsc{Congestion Minimization} problem. We are given a directed graph $G$ with $k$ pairs of vertices $(s_1,t_1),(s_2,t_2), \ldots (s_k,t_k)$ which are sources and destinations respectively. We want to route packets from sources to destination through edge disjoint paths in the graphs so that there is no edge that is used for two paths and hence no change of a congestion. However, even testing whether there are edge-disjoint paths between such pairs of vertices is $\NP$-hard for directed graphs even for $k=2$. Hence, we have to allow congestion, but ideally we would like to minimise congestion. Formally, a collection of paths $P_1, P_2, \ldots P_k$ (which need not be vertex disjoint) in the above problem is a solution with congestion $C$, if every edge takes part in at most $C$ paths in the collection. Indeed, we would like to find out a collection of paths with least congestion. The case when $C=1$ is exactly the edge disjoint path problem which indicates that the optimization problem is hard to solve.

Next step is to look for approximation algorithms. We would like to design algorithm which outputs a set of paths with a guarantess that the congestion is at most $\alpha C$ where $C$ is the optimal congestion possible for the given graph and the $\{(s_i,t_i)\}_{1 \le i \le t}$ pairs. Indeed, the problem admits a randomized approximation algorithm with a reasonable approximation ratio.

\begin{theorem}
{\sc Congestion Minimization} admits a randomized algorithm where the solution is guaranteed to be at most $\left(\frac{\log n}{\log \log n}\right) \times OPT$ where $OPT$ is the optimal congestion possible for the input instance and $n$ is the size of the graph.
\end{theorem}

We need some details about this randomized algorithm and the analysis since we have to deal with specifics of the analysis in the method itself. 

\subsection{Randomized Approximation Algorithm}

The algorithm uses a standard and quite effective technique of designing randomized algorithms which is {\em linear programming relaxation and randomized rounding}. Firsly the problem can be written as a linear program as follows: \\[-10mm]

\paragraph*{LP Formulation:} For any $i \in [k]$, let ${\cal P}_i$ denote the set of paths in the graph $G$ which are from vertices $s_i$ to $t_i$. In the solution, exactly one of these paths needs to be chosen. Let $P$ be the notation for one such path. Let us introduce a Boolean variable $x_i^P$ to indicate whether the path $P \in {\cal P}_i$ is chosen for the solution or not. The following $k$ constraints say, that from each ${\cal P}_i$ \textit{exactly one} path must be chosen for the solution : 
\begin{equation}
\forall i \in [k], \bigsum_{P \in {\cal P}_i} x_I^P = 1
\label{lp-const1}
\end{equation}

And, we need to minimise the congestion bound $C$ subject to above constraints. Writing down this objective function mathematically:
\begin{equation}
C = \bigsum_{e \in E} \bigsum_{\substack{P \in {\cal P}_i \\ e \in P}}x_i^P
\end{equation}

Unfortunately, solving such "integer linear programs" is $\NP$-hard. But a technique is that of linearprogramming relaxation, where we do not insist anymore that the variables must take Boolean values. But instead, we allow them to be real numbers under the constraint -  $\forall i, P, 0 \le x_i^P \le 1$ which are again linear constraints. There are standard techniques by which this can be solved now in polynomial in the number of variables. However, note that the number of variables in our setting is exponential in $n$. But we will ignore this fact\footnote{The trick to address this issue is to formulate a linear program with the granularity of edges - that is introducing variables at the edge-level than path-level as we have done.} for now as the exposition of the technique is easier with the above set up.

\paragraph*{Randomized Rounding:} The solution to the above linear program gives us values for $x_i^P$ (call them $\alpha_i^P$ between $0$ and $1$) for $i \in [k]$ and $P \in {\cal P}_i$ such that the congestion expression is at most $C^*$ (and is optimal). But this does not point to choice of any path $P \in \ P_i$ as they are not Booelan variables. So we need to make that choice (and hence turn th e values to Boolean) and this is the place where randomness is used\footnote{A simple deterministic idea is worth thinking about - namely, for every $i$, choose the $P$ for which $\alpha_i^P$ is maximum to be the path - this amounts to assigning $x_i^P$ to one for that path $P$ but $0$ for the other paths. But this can increase the congestion (constraint~\ref{}) without any reasonable bound.}.

The idea is as follows. For each $i$, we will choose a path $P \in {\cal P}_i$ at random with probability $x_i^P$. Notice that this is a probability distribution because of contstraint~\ref{lp-const1}. This can also be seen as, for each $i$, we will choose one of the $x_i^P$ at random with probability $\alpha_i^P$ and then assign that variable to $1$ and make the rest of the variables for that $i$ to be $0$. This is equivalent to choosing one path $P_i \in {\cal P}_i$ to be in the solution. For example, say we have three paths
with values $0.2, 0.7$, and $0.1$. Get a random number between $0$ and $1$. If the number is between $0$
and $0.2$, pick the first path. If the number is between $0.2$ and $0.9$, pick the second path, and if the
number is between $0.9$ and $1$, pick the third path.

\paragraph{Bounding the Approximation:} We need to analyse how badly the objective function (which was evaluating to optimal value $C$ when $\alpha_i^P$ were the assignments for $x_i^P$s) be affected by this {\em rounding} step. 
For every edge $e \in E$, define a random variable $Y_i^e$ as follows:
$$Y_i^e = 
\begin{cases}
1 & \textrm{ if $e \in P_i$ }\\
0 & \textrm{ otherwise }
\end{cases}
\textrm{\hspace{6mm} and by definition, }
\E[Y_i^e] \le \Pr[e \in P_i] \le \bigsum_{\substack{P \in {\cal P}_i \\ e \in P}} \alpha_i^P
$$

\noindent Define $Y_e = \sum_{i} Y_i^e$. By linearity of expectation, $\E[Y_e] = \sum_i \E[Y_i^e] = C^*$ (because it is exactly the objective function). Now we are in a perfect situation for a tail bound. What is the probability that the random variable (which in this case is a sum of independent random variables). 

\begin{proposition}[Chernoff Bound]
Let $X = \sum X_i$ be a random variable expressed as a sum of $X_i$'s which are independent random variables. Let $\E[X] \le \mu$. Then for any $\alpha > 1$, 
$$\Pr\left[ X \ge \alpha \mu \right] \le e^{-\mu \times [\alpha \ln \alpha - \alpha + 1]}$$
\end{proposition}
We just need to apply this with a $\alpha = \frac{\log n}{\log \log n}$ is the approximation ratio that we are looking for. Working out the math, this gives $\Pr[Y_e > \alpha C^*] \le \frac{1}{n^3}$. By union bound, probability that there exists an edge $e \in E$ with $Y_e > \alpha C^*$ is at most $\sum_e \Pr[ Y_e > \alpha C^*] \le \frac{1}{n}$. In other words, with probability at least $1-\frac{1}{n}$ the above rounding gives a solution (equivalently a Boolean assignment for the variables) which satisfies all the constraints but at the same time does not make the objective function value worse that a factor of $\alpha$. Hence it is a randomized alpha approximation algorithm.

\subsection{Pessimistic Estimator}

We abstract out a scenario from the above analysis as follows. 
Let $X = X_1 + X_2 + \ldots + X_k$ be a random variable that is expressed as a sum of independent random variables in $[0,1]$.
Chernoff Bound is typically applied when we want to estimate $\Pr[\sum_i X_i > \alpha]$. To see the event as a Boolean value, define the indicator function:
$$I(x_1, x_2, \ldots x_n) = 
\begin{cases}
1 & \textrm{ if $\sum_i x_i \ge \alpha$ } \\
0 & \textrm{ otherwise }
\end{cases}
$$
So one way to descend down the tree to derandomize is to apply the method of conditional expectation on $I(x_1, x_2, \ldots x_n)$. Similar to evaluating $e(r_1, r_2, \ldots r_i)$ in the {\sc MaxCut} algorithm, we should be able to efficiently evaluate:
$$\E[I(x_1, x_2, \ldots, x_i, X_{i+1}, \ldots X_n)] = \Pr\left[ ~\sum_{j=1}^k X_j \ge \alpha \mid X_1 = x_1, X_2 = x_2, \ldots X_i = x_i~\right]$$

But unfortunately, the RHS expression is not easy to compute since it depends on the distribution of the $X_j$ variables. However, we can still work with the framework, with the idea of replacing it with an upper bound function instead. Since we are bounding a bad event of the sum being large, a function which gives a larger value  would be more "pessimistic" and hence the term {\em pessimistic estimator}.

We can use an exponentiation idea coming from Chernoff Bound proof: For any $t > 0$:
$$\Pr\left[ ~\sum_{j=1}^k X_j \ge \alpha \right] \le \E \left[ \frac{\prod_{j=1}^k e^{tX_j}}{e^{t\alpha}} \right] $$
This upper bound is easy to compute when you have substitutions for some of the $X_i$s. That is,
$$\Pr\left[ ~\sum_{j=1}^k X_i \ge \alpha \mid X_1 = x_1, X_2 = x_2, \ldots X_i = x_i~\right] \le \left( \frac{\prod_{j=1}^i e^{tx_i}}{e^{t\alpha}} \right) \E \left[  \prod_{j={i+1}}^k e^{tX_i} \right]$$
Since we know the distribution of each $X_j$, we can compute the $\E \left[  \prod_{j={i+1}}^k e^{tX_i} \right]$ in the right hand side as we do in the proof of Chernoff Bound proof itself.

\paragraph{Applying the Method to the Algorithm for Congestion Minimization:}

Recall the random variables involved.
For every edge $e \in E$, we used random variable 
$Y_i^e = 
\begin{cases}
1 & \textrm{ if $e \in P_i$ }\\
0 & \textrm{ otherwise }
\end{cases}
$
\noindent Define $Y_e = \sum_{i} Y_i^e$. By linearity of expectation, $\E[Y_e] = \sum_i \E[Y_i^e] = C^*$.
In the analysis of the algorithm, we proved, by using Chernoff bound that, for $\alpha = \frac{\log n}{\log \log n}$:
$$\Pr \left[ \sum_i Y_i^e > \alpha C^* \right] \le \frac{1}{n}$$

As per our plan, rather than working with the indicator $I(x_1, x_2, \ldots x_i)$ we will work with the pessimistic esimator, for edge $e \in E$:
$\left( \frac{\prod_{j=1}^i e^{tr_i}}{e^{t\alpha}} \right) \E \left[  \prod_{j={i+1}}^k e^{tY_i^e} \right]$. We will also incorportate union bound into our estimate to use the pessiminstic estimator for the whole event as :

$$ I(r_1, r_2, \ldots r_i) \le f(r_1, r_2, \ldots r_i) = \left( \frac{\prod_{j=1}^i e^{tr_i}}{e^{t\alpha}} \right) \bigsum_{e \in E} \left( \E \left[  \prod_{j={i+1}}^k e^{tY_i^e} \right] \right) $$
By notation, we know that $f(r_1, r_2, \ldots r_n) \le \frac{1}{n}$.

\jsay{A few more lines to be completed here about the precise application of the estimators.}