\Week{07}{Efficient Error Reduction using Expander Graphs}

We provide two approaches to error reduction of algorithms using expander graphs. We fix some notations first. A randomized algorithm ${\cal{A}}$ on input $x$ runs in time $t(n)$ (where $n=|x|$) and let $y \in \{0,1\}^{m(n)}$ be the concatenation of the unbiased coin toss experiement that the algorithm does during its execution. Notice that $m(n) \le t(n)$ (we drop the $n$ when it is not required explicitly). If the algorithm runs in polynomial time $t(n) \le n^c$ for a constant $c$ independent of $n$. \\

\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^m$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.5\linewidth}
%The guarantee we have is there is an $\epsilon \in (0,\half]$.
\vspace{-5mm}
$$\forall x \in \{0,1\}^n,~\Pr_{y \in \{0,1\}^m} [\mathcal{A}(x,y) \textrm{ is correct.}] \ge 1-\epsilon $$
For an input $x \in {0,1}^n$, let $B$ be the set of random bits on which $\mathcal{A}$ makes an error.
$$B = \{ y \in \{0,1\}^m \mid \mathcal{A}(x,y) \textrm{ errs}\} \textrm{, and $|B| \le \epsilon2^m$}$$
\end{minipage}
\vspace{3mm}

\noindent We collect the different approaches that we have seen and will be seeing.
\begin{description}
\item{\bf Repetition with Independent Random Bits:}
Repeat the experiment $k$ times with independent random bits and accept the majority as the answer to be reported. This makes the new algorithm use $km$ random bits, runs in time $kt(n)$ and achieve error bound of $\frac{1}{2^{k}}$.
\item{\bf Repetition with Expander Neighbors:}
We will achieve an error bound of $\frac{1}{\sqrt{k}}$ without spending any additional random bits and time $kt(n)$.
\item{\bf Repetition with Expander Walk:}
We will achieve an error bound of $\frac{1}{2^k}$ by using $m+O(k)$ random bits and time $kt(n)$.
\item{\bf Repetition with Pairwise Independence:} Although not using expanders, we also use this context to introduce how basic notions of dependence can still be used in the context of amplification. We will show that if we use {\em pairwise independent} random variables, then it can still achieve error bounds of the form $\frac{1}{k}$ with only $O(\log n)$ additional random bits.
\end{description}

\section{Approach 1 : Using Expander Mixing Lemma}

The algorithm is as follows: let $G$ be an $(2^m,d,\lambda)$ spectral expander. Let $N = 2^m$.
\begin{algorithm}
\label{alg:hittingset-algo}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State Choose $y \in \{0,1\}^m$ uniformly at random.
\State Compute $N(y) = \{y_1, y_2, \ldots y_d\}$.
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.

\State {\sc Reject}.
\end{algorithmic}
\end{algorithm}

Note that the algorithm runs in time $O(dm)$ time and it does not use any extra random bits other than what the original algorithm does !. Now we show that it can still do some reasonable amplification.

\begin{lemma}
$\Pr\left[\calA' \textrm{ errs }\right] \le \frac{2 \epsilon (\lambda_2)^2 }{1-2\epsilon}$
\end{lemma}
\begin{proof}
Let $B_x$ (we will drop subscript $x$ from now on) denote the set of strings that makes the algorithm $\calA$ err on input $x$. Thus we have: $\card{B} \le \epsilon 2^m$.
 To analyse the algorithm $\calA'$, we need to bound the probability that at least $\frac{d}{2}$ of the neighbors of the algorithm is in the set $B$. Let us define:
$$B' = \left\{ y \in \zo^m ~:~ \card{N(y) \cap B} \ge \frac{d}{2} \right\}$$
We need to show an upper bound for $\frac{\card{B'}}{2^m}$. This follows from the expander mixing lemma (Lemma~\ref{lem:expander-mixing-lemma}) with $S = B'$ and $T = B$. This gives, 
$$\left||E(B,B')|-\frac{d|B||B'|}{2^m}\right| \le d\lambda_2 \left(\sqrt{|B||B'|}\right)$$

\noindent Note that $\card{E(B,B')} \ge \frac{d}{2}|B'|$. And, $\frac{d|B||B'|}{2^m} \le \epsilon d |B'|$. Since $\epsilon \le \half$, the first term in the LHS is larger.
\begin{eqnarray*}
\frac{d}{2}|B'| - d\epsilon|B'| & \le & d \lambda_2 \left(\sqrt{\epsilon2^m|B'|}\right) \\
\frac{|B'|}{2^m} & \le & \frac{2 \epsilon (\lambda_2)^2 }{1-2\epsilon}
\end{eqnarray*}
As an example, if the original error probability is $\frac{1}{3}$, this gives a success probability of $2\lambda^2$. If we want the error bound to be less than $\frac{1}{m}$, we should choose a polynomial degree spectral expander with vertices as $\zo^m$ with $\lambda_2 \le \frac{1}{2\sqrt{m}}$.
\end{proof}

\begin{remark}
There are two curious aspects about the above proof. First of all, it looks like the error probability bound is independent of $d$ -  number of times the algorithm is repeated. Since $d$ does not matter, can we take $d$ as a constant? (This saves running time for us !).
But then we need a spectral expander on $n$ vertices with $\lambda_2 \le \frac{1}{2\sqrt{\log |V|}}$. Can we have graphs with arbitrarily small second largest eigen value? Unfortunately, if we have degree to be small, then there is a constant fraction lower bound for $\lambda_2$. Hence our $d$ has to grow with the number of vertices. We can afford to take $d = \poly(\log |V|)$ in this case. So, in a general expander graph notation, we need $n$-vertex expander graphs of $\poly(\log n)$ degree and $\lambda_2 \le \frac{1}{\log n}$.
\end{remark}

%\begin{exercise-prob}[See Problem Set 4~(Problem~\ref{spectral-exp-lowerbound})]
%\begin{show-ps4}{spectral-exp-lowerbound}
%Let $G$ be a $d$-regular $n$-vertex undirected graph and $T_d$ be the infinite $d$-regular tree. For a graph $H$ and $\ell \in \N$, let $p^\ell(H)$ denote the probability that if we choose a random vertex $v$ in $H$ and do a random walk of length $2\ell$, we end back at vertex $v$.
%\begin{enumerate}[(a)]
%\item Show that $$p_\ell(G) \ge p_\ell(T_d) \ge C_\ell \frac{(d-1)^\ell}{d^{2\ell}}$$
%where $C_\ell$ is the $\ell^{th}$ Catalan number.
%\item Show that $$n p_\ell(G) \le 1+(n-1)\lambda^{2\ell}$$
%\item Use the fact that $C_\ell = \frac{1}{(\ell+1)}{2\ell \choose \ell}$ to prove that:
%$$\lambda \ge \frac{2\sqrt{d-1}}{d} - o(1)$$
%where the $o(1)$ term vanishes as $n \to \infty$.
%\end{enumerate}
%\end{show-ps4}
%\end{exercise-prob}

\section{Approach 2 : Using Random Walk Mixing}

We have seen the following amplification approach in Section~\ref{sec:intro-expanders}. In this section, we show the formal treatment of the idea.

\begin{algorithm}
\label{alg:expwalk-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State {\em count} $\gets 0$. 
\State Let $G(V,E)$ be an expander graph on $2^{m}$ vertices.
\State Choose a vertex $y_1 \in V$ uniformly at random. \Comment{Uses $m$ random bits}
\State Starting at $v$ perform a random walk for $k$ steps in $G$. Let $y_1, y_1, y_2, \ldots y_k$ each in $\{0,1\}^m$ be the vertices representing the walk. \Comment{Uses $O(k \log d)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.
\end{algorithmic}
\end{algorithm}

As stated along with the algorithms description, the number of random bits used here is $m+O(k \log d)$, and since we can use a constant degree expander graph family, the number of random bits used is $m+O(k)$. The time taken by the algorithm is $k$ times the original running time. We now bound the error probability of the algorithm.

\begin{lemma}
\label{lem:expwalk-amplification-bound}
$\Pr\left[\calA' \textrm{ errs }\right] \le 2^{-k}$
\end{lemma}

\begin{proof}
Again $B$ will denote the set of strings that makes the algorithm $\calA$ err on input $x$. Thus we have: $\card{B} \le \epsilon 2^m$. To analyse the algorithm $\calA'$, we need to bound the probability that at least $\frac{d}{2}$ of the neighbors of the algorithm is in the set $B$. Let $I = \{1,2,\ldots k\}$ be the index set we use for denoting the repetition iteration. We need to estimate the probability:
$$\Pr\left[ \begin{array}{c}
\exists I \subseteq [k], |I|=k/2 \\
\left[ \forall i \in I,~y_i \in B \right]
\end{array}
\right]
$$
where the probability is over the $m+O(k\log d)$ random bits used in the algorithm. As we have done before, the strategy is to remove the existential quantifier first (and then apply union bound later). Fix an $I \subseteq [k]$ of size $k/2$. We need to estimate the probability of the event $\left[ \forall i \in I,~y_i \in B \right]$. We will first do a simpler task - to estimate the probability when $I=[k]$.

Let us denote $\pi_1 \in \mathbb{R}^{2^m}$ to be the distribution corresponding to the first variable $y_1 \in \zo^m$. We first understand the case when $I = \{1\}$. We know that $\Pr[y_1 \in B] = \frac{|B|}{2^m}$. To express it in a form which we can use to iterate, define the operator $\Gamma : \mathbb{R}^{2^m} \to \mathbb{R}^{2^m}$ which for any $\phi \in \mathbb{R}^{2^m}$ projects to co-ordinates in $B$. More formally,
\[
\Gamma(\pi)_i = \left\{
\begin{array}{ll}
\pi_i & \textrm{ if $i \in B$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]

For shorthand notation, we denote $\Gamma(\pi)$ as $\Gamma \pi$. Now $\Pr[y_1 \in B]$ can be written as $|\Gamma \pi|_1$. We need to understand when $I = \{1,2\}$, recall that the steps in the random walk is represented by the linear transformation corresponding to the normalized adjacency matrix $A$.
\begin{eqnarray*}
\Pr[y_1 \in B \land y_2 \in B] & = & \Pr[y_2 \in B \mid y_1 \in B] \Pr[y_1 \in B] \le |\Gamma A \Gamma \pi_0|_1 \\
\Pr[y_1 \in B \land y_2 \in B \ldots y_k \in B] & \le & |(\Gamma A)^{k-1} \Gamma \pi_0|_1
\end{eqnarray*}
Thus, we need to upper bound $|(\Gamma A)^{k-1} \Gamma \pi_0|_1$. Note that $\Gamma \circ \Gamma = \Gamma$. Hence, $(\Gamma A)^{k-1} \Gamma \equiv (\Gamma A \Gamma)^{k-1} \Gamma$. We will replace $k-1$ with $k$ in the expression from now on, for easy representation, but this is only going make the upper bound bigger, so it does not affect us.

Now here comes the advantage of moving to vectors. Instead of bounding the $\ell_1$ norm, we will upper bound the $\ell_2$ norm\footnote{This is enough since, for any $v \in R^n$, $\frac{|v|_1}{\sqrt{n}} \le \norm{v} \le |v|_1$. To see first inequality which we are using, $|v|_1 = \sum_i |v_i| = \sum_i |v_i|.1 \le \sqrt{\sum_i v_i^2 \sum_i 1^2} = \|v\|_2 \sqrt{n} $ where the second last inequality follows from Cauchy-Schwartz inequality.}.

To do this, we use the following claim which indicates that this $\ell_2$ norm actually gets closer and closer to $\epsilon\norm{\pi_1}$ if $k$ is large enough.
\begin{claim}
\label{claim:pigamma}
For all $k$, and for all $\pi$:
$$\norm{(\Gamma A \Gamma)^k \pi} \le (\epsilon+\lambda)^k \norm{\pi}$$
$$\norm{(\Gamma A^k \Gamma) \pi} \le (\epsilon+\lambda^k) \norm{\pi}$$
\end{claim}
\begin{proof}
We prove the second inequality (which is needed in the next step too). Applying the second inequality for $k=1$ repeatedly will yield the first inequality. 

We need to prove : 
\[ \norm{\Gamma A^k (\Gamma \pi)} \le (\epsilon+\lambda) \norm{\pi} \]
The natural attack, given the above is to write $\Gamma \pi$ vector in terms of the orthonormal basis $v_1, v_2 \ldots v_{2^n}$ of the eigen spaces of the eigen values of the matrix $A$. Notice that since $\norm{v_1} = 1$, we have that the vector : 
$v_1 = \left(\frac{1}{\sqrt{2^m}},\frac{1}{\sqrt{2^m}}, \ldots,\frac{1}{\sqrt{2^m}} \right)$.

Let $\Gamma\pi$ be written as $\Gamma\pi = \alpha_1 v_1 + w$ where $\alpha_1 = \langle \Gamma \pi, v_1 \rangle$ and $w \perp v_1$. 
Hence, the term that we wanted to bound, $\norm{\Gamma A^k (\Gamma \pi)} \le \alpha_1 \norm{\Gamma A^k v_1} + \norm{\Gamma A^k w}$. We bound the two terms in the summation separately.
\begin{description}
\item{\textsc{Sub-Claim 1} : ${\alpha_1 \norm{\Gamma A^k v_1} \le \epsilon\norm{\pi}}$:}
$$ \alpha_1 \norm{\Gamma A^k v_1} = \alpha_1 \norm{\Gamma v_1} = \alpha_1 \sqrt{\sum_{i=1}^{|B|} \left( \frac{1}{\sqrt{2^{m}}} \right)^2 } = \alpha_1 \sqrt{\frac{|B|}{2^m}} \le \alpha_1 \sqrt{\epsilon}$$
Moreover, 
$$
\alpha_1 \le \langle \Gamma \pi, v_1 \rangle \le \norm{\Gamma \pi}\norm{v_1} \le \norm{\Gamma \pi} \le \sqrt{\sum_{i \in B} \pi_i^2} \le  \sqrt{\sum_{i \in B} \pi_i.\pi_i} \le \sqrt{\sum_{i \in B} 1.\pi_i}  \le  \sqrt{\epsilon} \norm{\pi}
$$

\jsay{the last inequality is clear when $\pi$ is a uniform distribution, but that is not enough for us since we are applying it for arbitrary $\pi$ as well. So this needs a fix.}
\item{\textsc{Sub-Claim 2} : $\norm{\Gamma A^k w} \le \lambda^k\norm{\pi}$}

Since $\Gamma$ only inhibits certain co-ordinates, it can only diminish norms and this give $\norm{\Gamma A^k w} \le \norm{A^kw} \le (\lambda)^k\norm{w}$. The last inequality follows because  $\lambda = \max_{w \perp v_1} \frac{\norm{Aw}}{\norm{w}}$. Now we just need to use the fact that $\norm{w} \le \norm{\Gamma \pi} \le \norm{\pi} $ and this gives $\norm{\Gamma A^k w} \le \lambda^k\norm{\pi}$ as needed.
\end{description} 

\noindent The claimed bound in Claim~\ref{claim:pigamma} follows from the two subclaims. 
\end{proof}
\noindent Using Claim~\ref{claim:pigamma} first part, we derive the probability bound that we wanted to estimate.

\begin{eqnarray*}
\Pr[y_0 \in B \land y_1 \in B \ldots y_k \in B] & \le & |(\Gamma A\Gamma)^k \pi_0|_1 \le \sqrt{2^m}\norm{(\Gamma A\Gamma)^k \pi_0}\\
& \le & \sqrt{2^m} (\epsilon+\lambda)^k \norm{\pi_0} \le \sqrt{2^m} (\epsilon+\lambda)^k \frac{1}{\sqrt{2^m}} \le (\epsilon+\lambda)^k
\end{eqnarray*}

\noindent Now we understand how to estimate the membership in $B$ for the whole of the index set, we take the next step to estimate it for a subset of $[k]$ of size $\frac{k}{2}$. Let $I \subseteq [k]$, $|I|=\frac{k}{2}$.
\begin{eqnarray*}
\Pr\left[\bigwedge_{i \in I} y_i \in B\right] & \le & |(\Gamma A) A (\Gamma A) (A) (A) (\Gamma A) \Gamma \pi_0|_1
\end{eqnarray*}
where, in the expression, we should see exactly $\frac{k}{2}$ $(\Gamma A)$s. We collect all of the $A$s together and write:
\begin{eqnarray*}
\Pr\left[\bigwedge_{i \in I} y_i \in B\right] & \le & |(\Gamma A^{k_1} \Gamma) (\Gamma A^{k_2} \Gamma) \ldots (\Gamma A^{k_t} \Gamma) \pi_0|_1 \\
& \le & \sqrt{2^m}\norm{(\Gamma A^{k_1} \Gamma) (\Gamma A^{k_2} \Gamma) \ldots (\Gamma A^{k_t} \Gamma) \pi_0} \\
& \le & \sqrt{2^m}(\epsilon+\lambda^{k_1}) (\epsilon+\lambda^{k_2}) \ldots (\epsilon+\lambda^{k_t}) \frac{1}{\sqrt{2^m}} \\
& \le & (\epsilon+\lambda)^{(k/2)} \le \frac{1}{4^k} 
\textrm{\hspace{1cm} $\because$ choose $\lambda$ such that $\epsilon+\lambda \le \frac{1}{16}$.}
\end{eqnarray*}
\noindent Now we apply the union bound as per our plan.

\begin{eqnarray*}
\Pr\left[ \begin{array}{c}
\exists I \subseteq [k], |I|=k/2 \\
\left[ \forall i \in I,~y_i \in B \right]
\end{array}
\right] & \le & \left(\textrm{ \# of Is of size $\frac{k}{2}$}\right) \times  \Pr\left[\bigwedge_{i \in I} y_i \in B\right] \\
& \le & 2^k \times (\epsilon+\lambda)^{(k/2)} \le \frac{1}{2^k}
\end{eqnarray*}

Hence the proof.

\end{proof}

We conclude this part by providing a table of the various amplification methods that we have seen so far.

\renewcommand{\arraystretch}{2}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline 
Approach & Error bound & Random bits used  \\ 
\hline 
Trivial & $\half(1-4\epsilon^2)^{k/2}$ & $km$ \\ 
\hline 
Expander Neighborhood & $\frac{\displaystyle 2\epsilon\lambda^2}{\displaystyle (1-2\epsilon)}$ & $m$ \\ 
\hline 
Expander Walk & $2^k(\epsilon+\lambda)^{k/2}$ & $m+k\log d$ \\ 
\hline 
Pairwise Independence & $\frac{\displaystyle 4}{\displaystyle k(1-2\epsilon)^2}$ & $2m$ \\ 
\hline
$t$-wise Independence & $\frac{\displaystyle 1}{\displaystyle k^{t/2}(1-2\epsilon)^t}$ & $tm$ \\ 
\hline
\end{tabular} 
\end{center}
\renewcommand{\arraystretch}{1}
