\Week{07}{Efficient Error Reduction using Expander Graphs}

We provide two approaches to error reduction of algorithms using expander graphs. We fix some notations first. A randomized algorithm ${\cal{A}}$ on input $x$ runs in time $t(n)$ (where $n=|x|$) and let $y \in \{0,1\}^{m(n)}$ be the concatenation of the unbiased coin toss experiement that the algorithm does during its execution. Notice that $m(n) \le t(n)$ (we drop the $n$ when it is not required explicitly). If the algorithm runs in polynomial time $t(n) \le n^c$ for a constant $c$ independent of $n$. \\

\begin{minipage}{0.4\linewidth}
\begin{tikzpicture}[shorten >=0.5pt,node distance=0.2cm,on grid,auto]
\node[state,rectangle,minimum width=1.5cm,minimum height=1.5cm,align=center](q_r)[]{${\cal A}(x,y)$}; 
\node[coordinate](q_0)[right=of q_r,xshift=3cm]{};
\node[coordinate](q_1)[below=of q_r,xshift=0cm,yshift=-1cm]{};
\node[coordinate](q_2)[left=of q_r,xshift=-3cm,yshift=0mm]{};
\path[->](q_r) edge [midway] node {\sc Yes/No} (q_0);   
\draw[->] ([yshift=1mm]q_2) -- ([yshift=1mm]q_r.west)node[midway] {$x \in \{0,1\}^n$};
\draw[->] ([yshift=-2mm]q_1) -- ([yshift=0mm]q_r.south)node[midway,swap] {$y \in \{0,1\}^m$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.05\linewidth}
~
\end{minipage}
\begin{minipage}{0.5\linewidth}
%The guarantee we have is there is an $\epsilon \in (0,\half]$.
\vspace{-5mm}
$$\forall x \in \{0,1\}^n,~\Pr_{y \in \{0,1\}^m} [\mathcal{A}(x,y) \textrm{ is correct.}] \ge 1-\epsilon $$
For an input $x \in {0,1}^n$, let $B$ be the set of random bits on which $\mathcal{A}$ makes an error.
$$B = \{ y \in \{0,1\}^m \mid \mathcal{A}(x,y) \textrm{ errs}\} \textrm{, and $|B| \le \epsilon2^m$}$$
\end{minipage}
\vspace{3mm}

\noindent We collect the different approaches that we have seen and will be seeing.
\begin{description}
\item{\bf Repetition with Independent Random Bits:}
Repeat the experiment $k$ times with independent random bits and accept the majority as the answer to be reported. This makes the new algorithm use $km$ random bits, runs in time $kt(n)$ and achieve error bound of $\frac{1}{2^{k}}$.
\item{\bf Repetition with Expander Neighbors:}
We will achieve an error bound of $\frac{1}{\sqrt{k}}$ without spending any additional random bits and time $kt(n)$.
\item{\bf Repetition with Expander Walk:}
We will achieve an error bound of $\frac{1}{2^k}$ by using $m+O(k)$ random bits and time $kt(n)$.
\item{\bf Repetition with Pairwise Independence:} Although not using expanders, we also use this context to introduce how basic notions of dependence can still be used in the context of amplification. We will show that if we use {\em pairwise independent} random variables, then it can still achieve error bounds of the form $\frac{1}{k}$ with only $O(\log n)$ additional random bits.
\end{description}

\section{Approach 1 : Using Expander Mixing Lemma}

The algorithm is as follows: let $G$ be an $(2^m,d,\lambda)$ spectral expander. Let $N = 2^m$.
\begin{algorithm}
\label{alg:hittingset-algo}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State Choose $y \in \{0,1\}^m$ uniformly at random.
\State Compute $N(y) = \{y_1, y_2, \ldots y_d\}$.
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.

\State {\sc Reject}.
\end{algorithmic}
\end{algorithm}

Note that the algorithm runs in time $O(dm)$ time and it does not use any extra random bits other than what the original algorithm does !. Now we show that it can still do some reasonable amplification.

\begin{lemma}
$\Pr\left[\calA' \textrm{ errs }\right] \le \frac{2 \epsilon (\lambda_2)^2 }{1-2\epsilon}$
\end{lemma}
\begin{proof}
Let $B_x$ (we will drop subscript $x$ from now on) denote the set of strings that makes the algorithm $\calA$ err on input $x$. Thus we have: $\card{B} \le \epsilon 2^m$.
 To analyse the algorithm $\calA'$, we need to bound the probability that at least $\frac{d}{2}$ of the neighbors of the algorithm is in the set $B$. Let us define:
$$B' = \left\{ y \in \zo^m ~:~ \card{N(y) \cap B} \ge \frac{d}{2} \right\}$$
We need to show an upper bound for $\frac{\card{B'}}{2^m}$. This follows from the expander mixing lemma (Lemma~\ref{lem:expander-mixing-lemma}) with $S = B'$ and $T = B$. This gives, 
$$\left||E(B,B')|-\frac{d|B||B'|}{2^m}\right| \le d\lambda_2 \left(\sqrt{|B||B'|}\right)$$

\noindent Note that $\card{E(B,B')} \ge \frac{d}{2}|B'|$. And, $\frac{d|B||B'|}{2^m} \le \epsilon d |B'|$. Since $\epsilon \le \half$, the first term in the LHS is larger.
\begin{eqnarray*}
\frac{d}{2}|B'| - d\epsilon|B'| & \le & d \lambda_2 \left(\sqrt{\epsilon2^m|B'|}\right) \\
\frac{|B'|}{2^m} & \le & \frac{2 \epsilon (\lambda_2)^2 }{1-2\epsilon}
\end{eqnarray*}
As an example, if the original error probability is $\frac{1}{3}$, this gives a success probability of $2\lambda^2$. If we want the error bound to be less than $\frac{1}{m}$, we should choose a polynomial degree spectral expander with vertices as $\zo^m$ with $\lambda_2 \le \frac{1}{2\sqrt{m}}$.
\end{proof}

\begin{remark}
There are two curious aspects about the above proof. First of all, it looks like the error probability bound is independent of $d$ -  number of times the algorithm is repeated. Since $d$ does not matter, can we take $d$ as a constant? (This saves running time for us !).
But then we need a spectral expander on $n$ vertices with $\lambda_2 \le \frac{1}{2\sqrt{\log |V|}}$. Can we have graphs with arbitrarily small second largest eigen value? Unfortunately, if we have degree to be small, then there is a constant fraction lower bound for $\lambda_2$. Hence our $d$ has to grow with the number of vertices. We can afford to take $d = \poly(\log |V|)$ in this case. So, in a general expander graph notation, we need $n$-vertex expander graphs of $\poly(\log n)$ degree and $\lambda_2 \le \frac{1}{\log n}$.
\end{remark}

%\begin{exercise-prob}[See Problem Set 4~(Problem~\ref{spectral-exp-lowerbound})]
%\begin{show-ps4}{spectral-exp-lowerbound}
%Let $G$ be a $d$-regular $n$-vertex undirected graph and $T_d$ be the infinite $d$-regular tree. For a graph $H$ and $\ell \in \N$, let $p^\ell(H)$ denote the probability that if we choose a random vertex $v$ in $H$ and do a random walk of length $2\ell$, we end back at vertex $v$.
%\begin{enumerate}[(a)]
%\item Show that $$p_\ell(G) \ge p_\ell(T_d) \ge C_\ell \frac{(d-1)^\ell}{d^{2\ell}}$$
%where $C_\ell$ is the $\ell^{th}$ Catalan number.
%\item Show that $$n p_\ell(G) \le 1+(n-1)\lambda^{2\ell}$$
%\item Use the fact that $C_\ell = \frac{1}{(\ell+1)}{2\ell \choose \ell}$ to prove that:
%$$\lambda \ge \frac{2\sqrt{d-1}}{d} - o(1)$$
%where the $o(1)$ term vanishes as $n \to \infty$.
%\end{enumerate}
%\end{show-ps4}
%\end{exercise-prob}

\section{Approach 2 : Using Random Walk Mixing}

We have seen the following amplification approach in Section~\ref{sec:intro-expanders}. In this section, we show the formal treatment of the idea.

\begin{algorithm}
\label{alg:expwalk-amplification}
\caption{(${\cal{A}'}$) : input $x \in \{0,1\}^n$} 
\begin{algorithmic}[1]
\State {\em count} $\gets 0$. 
\State Let $G(V,E)$ be an expander graph on $2^{m}$ vertices.
\State Choose a vertex $y_0 \in V$ uniformly at random. \Comment{Uses $m$ random bits}
\State Starting at $v$ perform a random walk for $k$ steps in $G$. Let $y_0, y_1, y_2, \ldots y_k$ each in $\{0,1\}^m$ be the vertices representing the walk. \Comment{Uses $O(k \log d)$ random bits.}
\For{\texttt{each $i \in [k]$}}
	\State If ${\cal{A}}(x,y_i)$ accepts, if so increment {\em count}
\EndFor
\State If ~[{\em count} $> \frac{k}{2}$]~ then output {\sc Yes} else output {\sc No}.
\end{algorithmic}
\end{algorithm}

As stated along with the algorithms description, the number of random bits used here is $m+O(k \log d)$, and since we can use a constant degree expander graph family, the number of random bits used is $m+O(k)$. The time taken by the algorithm is $k$ times the original running time. We now bound the error probability of the algorithm.

\begin{lemma}
\label{lem:expwalk-amplification-bound}
$\Pr\left[\calA' \textrm{ errs }\right] \le 2^{-m}$
\end{lemma}
\begin{proof}
Again $B$ will denote the set of strings that makes the algorithm $\calA$ err on input $x$. Thus we have: $\card{B} \le \epsilon 2^m$. To analyse the algorithm $\calA'$, we need to bound the probability that at least $\frac{d}{2}$ of the neighbors of the algorithm is in the set $B$. Let $I = \{1,2,\ldots k\}$ be the index set we use for denoting the repetition iteration. We need to estimate the probability:
$$\Pr\left[ \begin{array}{c}
\exists I \subseteq [k], |I|=k/2 \\
\left[ \forall i \in I,~y_i \in B \right]
\end{array}
\right]
$$
where the probability is over the $m+O(k\log d)$ random bits used in the algorithm. As we have done before, the strategy is to remove the existential quantifier first (and then apply union bound later). Fix an $I \subseteq [k]$ of size $k/2$. We need to estimate the probability of the event $\left[ \forall i \in I,~y_i \in B \right]$. We will first do a simpler task - to estimate the probability when $I=[k]$.

Let us denote $\pi_0 \in \mathbb{R}^{2^m}$ to be the distribution corresponding to the first variable $y_0 \in \zo^m$. We first understand the case when $I = \{1\}$. We know that $\Pr[y_0 \in B] = \frac{|B|}{2^m}$. To express it in a form which we can use to iterate, define the operator $\Gamma : \mathbb{R}^{2^m} \to \mathbb{R}^{2^m}$ which for any $\phi \in \mathbb{R}^{2^m}$ projects to co-ordinates in $B$. More formally,
\[
\Gamma(\pi)_i = \left\{
\begin{array}{ll}
\pi_i & \textrm{ if $i \in B$} \\
0 & \textrm{ otherwise}
\end{array}
\right.
\]

\jsay{to be continued ....}

\end{proof}